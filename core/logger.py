# core/logger.py
"""
–ú–æ–¥—É–ª—å –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è –æ—Ç–≤–µ—Ç–æ–≤ –±–æ—Ç–∞.
–°—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –ª–æ–≥–∏ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ –∫–∞—á–µ—Å—Ç–≤–∞ –∏ –æ—Ç–ª–∞–¥–∫–∏.
"""

import json
from datetime import datetime
from pathlib import Path
from typing import Dict, Any, List, Optional
from config.feature_flags import feature_flags


def log_bot_response(
    user_query: str,
    response_data: Dict[str, Any],
    theme_hint: Optional[Dict[str, Any]] = None,
    candidates: Optional[List[Dict[str, Any]]] = None,
    scores: Optional[Dict[str, float]] = None,
    relevance_score: Optional[float] = None,
    guard_threshold: Optional[float] = None,
    low_relevance: bool = False,
    shown_cta: bool = False,
    followups_count: int = 0,
    opener_used: bool = False,
    closer_used: bool = False
) -> None:
    """
    –õ–æ–≥–∏—Ä—É–µ—Ç —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –æ—Ç–≤–µ—Ç –±–æ—Ç–∞.
    
    Args:
        user_query: –ó–∞–ø—Ä–æ—Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
        response_data: –î–∞–Ω–Ω—ã–µ –æ—Ç–≤–µ—Ç–∞ (JSON –∏–ª–∏ legacy)
        theme_hint: –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ —Ç–µ–º–µ –∑–∞–ø—Ä–æ—Å–∞
        candidates: –°–ø–∏—Å–æ–∫ –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤ —Å –¥–µ—Ç–∞–ª—è–º–∏
        scores: –°–∫–æ—Ä—Ä—ã —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏
        relevance_score: –û–±—â–∏–π —Å–∫–æ—Ä—Ä —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏
        guard_threshold: –ü–æ—Ä–æ–≥ guard —Å–∏—Å—Ç–µ–º—ã
        low_relevance: –§–ª–∞–≥ –Ω–∏–∑–∫–æ–π —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏
        shown_cta: –ü–æ–∫–∞–∑–∞–Ω–∞ –ª–∏ CTA
        followups_count: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ followups
        opener_used: –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω –ª–∏ opener —ç–º–ø–∞—Ç–∏–∏
        closer_used: –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω –ª–∏ closer —ç–º–ø–∞—Ç–∏–∏
    """
    
    # –°–æ–∑–¥–∞–µ–º —Å—Ç—Ä—É–∫—Ç—É—Ä—É –ª–æ–≥–∞
    meta = response_data.get("meta", {})
    followups = response_data.get("followups", [])
    
    log_entry = {
        "ts": datetime.now().isoformat() + "Z",
        "user_query": user_query,
        "theme_hint": theme_hint or {},
        "primary_doc_type": meta.get("doc_type", "unknown"),
        "meta": {
            "doc_type": meta.get("doc_type", "unknown"),
            "topic": meta.get("topic", ""),
            "low_relevance": meta.get("low_relevance", False),
            "has_ui_cta": meta.get("has_ui_cta", False)
        },
        "candidates": candidates or [],
        "evidence": _extract_evidence(response_data),
        "scores": scores or {},
        "relevance_score": relevance_score,
        "guard_threshold": guard_threshold,
        "low_relevance": low_relevance,
        "shown_cta": shown_cta,
        "opener_used": opener_used,
        "closer_used": closer_used,
        "followups_count": followups_count,
        "followups_labels": [f.get("label", "") for f in followups if isinstance(f, dict)],
        "response_mode": feature_flags.get("RESPONSE_MODE")
    }
    
    # –ó–∞–ø–∏—Å—ã–≤–∞–µ–º –≤ —Ñ–∞–π–ª
    try:
        logs_dir = Path("logs")
        logs_dir.mkdir(exist_ok=True)
        
        log_file = logs_dir / "bot_responses.jsonl"
        with open(log_file, "a", encoding="utf-8") as f:
            f.write(json.dumps(log_entry, ensure_ascii=False) + "\n")
        
        print(f"üìù –õ–æ–≥ –∑–∞–ø–∏—Å–∞–Ω: {log_file}")
        
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è: {e}")


def _extract_evidence(response_data: Dict[str, Any]) -> Dict[str, Any]:
    """
    –ò–∑–≤–ª–µ–∫–∞–µ—Ç –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ–± –∏—Å—Ç–æ—á–Ω–∏–∫–µ –æ—Ç–≤–µ—Ç–∞.
    
    Args:
        response_data: –î–∞–Ω–Ω—ã–µ –æ—Ç–≤–µ—Ç–∞
    
    Returns:
        –°–ª–æ–≤–∞—Ä—å —Å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–µ–π –æ–± –∏—Å—Ç–æ—á–Ω–∏–∫–µ
    """
    evidence = {}
    
    # –î–ª—è JSON —Ñ–æ—Ä–º–∞—Ç–∞
    if "answer" in response_data:
        used_chunks = response_data.get("used_chunks", [])
        if used_chunks:
            # –ë–µ—Ä–µ–º –ø–µ—Ä–≤—ã–π —á–∞–Ω–∫ –∫–∞–∫ –æ—Å–Ω–æ–≤–Ω–æ–π –∏—Å—Ç–æ—á–Ω–∏–∫
            first_chunk = used_chunks[0]
            if isinstance(first_chunk, str) and "#" in first_chunk:
                file_part, section_part = first_chunk.split("#", 1)
                evidence = {
                    "file": file_part,
                    "h2": section_part
                }
    
    # –î–ª—è legacy —Ñ–æ—Ä–º–∞—Ç–∞
    elif "used_chunks" in response_data:
        used_chunks = response_data["used_chunks"]
        if used_chunks:
            first_chunk = used_chunks[0]
            if isinstance(first_chunk, str) and "#" in first_chunk:
                file_part, section_part = first_chunk.split("#", 1)
                evidence = {
                    "file": file_part,
                    "h2": section_part
                }
    
    return evidence


def format_candidates_for_log(candidates: List[tuple]) -> List[Dict[str, Any]]:
    """
    –§–æ—Ä–º–∞—Ç–∏—Ä—É–µ—Ç –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤ –¥–ª—è –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è.
    
    Args:
        candidates: –°–ø–∏—Å–æ–∫ (chunk, score) –∫–æ—Ä—Ç–µ–∂–µ–π
    
    Returns:
        –°–ø–∏—Å–æ–∫ —Å–ª–æ–≤–∞—Ä–µ–π —Å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–µ–π –æ –∫–∞–Ω–¥–∏–¥–∞—Ç–∞—Ö
    """
    formatted = []
    
    for chunk, score in candidates:
        if hasattr(chunk, 'file_name') and hasattr(chunk, 'text'):
            # –ò–∑–≤–ª–µ–∫–∞–µ–º –∑–∞–≥–æ–ª–æ–≤–æ–∫ –∏–∑ —Ç–µ–∫—Å—Ç–∞
            h2_match = None
            if hasattr(chunk, 'text'):
                import re
                h2_match = re.search(r'(?m)^##\s+(.+?)\s*$', chunk.text)
            
            formatted.append({
                "file": chunk.file_name,
                "h2": h2_match.group(1) if h2_match else "unknown",
                "source": "chunk",
                "score": score
            })
    
    return formatted
