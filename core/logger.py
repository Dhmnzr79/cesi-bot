# core/logger.py
"""
ĞœĞ¾Ğ´ÑƒĞ»ÑŒ Ğ»Ğ¾Ğ³Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ² Ğ±Ğ¾Ñ‚Ğ°.
Ğ¡Ñ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ»Ğ¾Ğ³Ğ¸ Ğ´Ğ»Ñ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ° ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ¸ Ğ¾Ñ‚Ğ»Ğ°Ğ´ĞºĞ¸.
"""

import json
import time
import logging
from datetime import datetime
from pathlib import Path
from typing import Dict, Any, List, Optional
from config.feature_flags import feature_flags

# Ğ˜Ğ¼ĞµĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ»Ğ¾Ğ³Ğ³ĞµÑ€Ñ‹ Ğ´Ğ»Ñ Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… Ñ‚Ğ¸Ğ¿Ğ¾Ğ² Ğ»Ğ¾Ğ³Ğ¾Ğ²
log_q = logging.getLogger("cesi.queries")
log_r = logging.getLogger("cesi.bot_responses")
log_m = logging.getLogger("cesi.minimal_logs")


def log_bot_response(
    user_query: str,
    response_data: Dict[str, Any],
    theme_hint: Optional[Dict[str, Any]] = None,
    candidates: Optional[List[Dict[str, Any]]] = None,
    scores: Optional[Dict[str, float]] = None,
    relevance_score: Optional[float] = None,
    guard_threshold: Optional[float] = None,
    low_relevance: bool = False,
    shown_cta: bool = False,
    followups_count: int = 0,
    opener_used: bool = False,
    closer_used: bool = False
) -> None:
    """
    Ğ›Ğ¾Ğ³Ğ¸Ñ€ÑƒĞµÑ‚ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ¾Ñ‚Ğ²ĞµÑ‚ Ğ±Ğ¾Ñ‚Ğ°.
    
    Args:
        user_query: Ğ—Ğ°Ğ¿Ñ€Ğ¾Ñ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ñ
        response_data: Ğ”Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ° (JSON Ğ¸Ğ»Ğ¸ legacy)
        theme_hint: Ğ˜Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ñ Ğ¾ Ñ‚ĞµĞ¼Ğµ Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ°
        candidates: Ğ¡Ğ¿Ğ¸ÑĞ¾Ğº ĞºĞ°Ğ½Ğ´Ğ¸Ğ´Ğ°Ñ‚Ğ¾Ğ² Ñ Ğ´ĞµÑ‚Ğ°Ğ»ÑĞ¼Ğ¸
        scores: Ğ¡ĞºĞ¾Ñ€Ñ€Ñ‹ Ñ€ĞµĞ»ĞµĞ²Ğ°Ğ½Ñ‚Ğ½Ğ¾ÑÑ‚Ğ¸
        relevance_score: ĞĞ±Ñ‰Ğ¸Ğ¹ ÑĞºĞ¾Ñ€Ñ€ Ñ€ĞµĞ»ĞµĞ²Ğ°Ğ½Ñ‚Ğ½Ğ¾ÑÑ‚Ğ¸
        guard_threshold: ĞŸĞ¾Ñ€Ğ¾Ğ³ guard ÑĞ¸ÑÑ‚ĞµĞ¼Ñ‹
        low_relevance: Ğ¤Ğ»Ğ°Ğ³ Ğ½Ğ¸Ğ·ĞºĞ¾Ğ¹ Ñ€ĞµĞ»ĞµĞ²Ğ°Ğ½Ñ‚Ğ½Ğ¾ÑÑ‚Ğ¸
        shown_cta: ĞŸĞ¾ĞºĞ°Ğ·Ğ°Ğ½Ğ° Ğ»Ğ¸ CTA
        followups_count: ĞšĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ¾ followups
        opener_used: Ğ˜ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½ Ğ»Ğ¸ opener ÑĞ¼Ğ¿Ğ°Ñ‚Ğ¸Ğ¸
        closer_used: Ğ˜ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½ Ğ»Ğ¸ closer ÑĞ¼Ğ¿Ğ°Ñ‚Ğ¸Ğ¸
    """
    
    # Ğ¡Ğ¾Ğ·Ğ´Ğ°ĞµĞ¼ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ñƒ Ğ»Ğ¾Ğ³Ğ°
    meta = response_data.get("meta", {})
    followups = response_data.get("followups", [])
    
    log_entry = {
        "ts": datetime.now().isoformat() + "Z",
        "user_query": user_query,
        "theme_hint": theme_hint or {},
        "primary_doc_type": meta.get("doc_type", "unknown"),
        "meta": {
            "doc_type": meta.get("doc_type", "unknown"),
            "topic": meta.get("topic", ""),
            "low_relevance": meta.get("low_relevance", False),
            "has_ui_cta": meta.get("has_ui_cta", False)
        },
        "candidates": candidates or [],
        "evidence": _extract_evidence(response_data),
        "scores": scores or {},
        "relevance_score": relevance_score,
        "guard_threshold": guard_threshold,
        "low_relevance": low_relevance,
        "shown_cta": shown_cta,
        "opener_used": opener_used,
        "closer_used": closer_used,
        "followups_count": followups_count,
        "followups_labels": [f.get("label", "") for f in followups if isinstance(f, dict)],
        "response_mode": feature_flags.get("RESPONSE_MODE")
    }
    
    # Ğ—Ğ°Ğ¿Ğ¸ÑÑ‹Ğ²Ğ°ĞµĞ¼ Ğ² Ñ„Ğ°Ğ¹Ğ»
    try:
        logs_dir = Path("logs")
        logs_dir.mkdir(exist_ok=True)
        
        log_file = logs_dir / "bot_responses.jsonl"
        with open(log_file, "a", encoding="utf-8") as f:
            f.write(json.dumps(log_entry, ensure_ascii=False) + "\n")
        
        print(f"ğŸ“ Ğ›Ğ¾Ğ³ Ğ·Ğ°Ğ¿Ğ¸ÑĞ°Ğ½: {log_file}")
        
    except Exception as e:
        print(f"âŒ ĞÑˆĞ¸Ğ±ĞºĞ° Ğ»Ğ¾Ğ³Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ: {e}")


def _extract_evidence(response_data: Dict[str, Any]) -> Dict[str, Any]:
    """
    Ğ˜Ğ·Ğ²Ğ»ĞµĞºĞ°ĞµÑ‚ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ñ Ğ¾Ğ± Ğ¸ÑÑ‚Ğ¾Ñ‡Ğ½Ğ¸ĞºĞµ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ°.
    
    Args:
        response_data: Ğ”Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ°
    
    Returns:
        Ğ¡Ğ»Ğ¾Ğ²Ğ°Ñ€ÑŒ Ñ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸ĞµĞ¹ Ğ¾Ğ± Ğ¸ÑÑ‚Ğ¾Ñ‡Ğ½Ğ¸ĞºĞµ
    """
    evidence = {}
    
    # Ğ”Ğ»Ñ JSON Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ‚Ğ°
    if "answer" in response_data:
        used_chunks = response_data.get("used_chunks", [])
        if used_chunks:
            # Ğ‘ĞµÑ€ĞµĞ¼ Ğ¿ĞµÑ€Ğ²Ñ‹Ğ¹ Ñ‡Ğ°Ğ½Ğº ĞºĞ°Ğº Ğ¾ÑĞ½Ğ¾Ğ²Ğ½Ğ¾Ğ¹ Ğ¸ÑÑ‚Ğ¾Ñ‡Ğ½Ğ¸Ğº
            first_chunk = used_chunks[0]
            if isinstance(first_chunk, str) and "#" in first_chunk:
                file_part, section_part = first_chunk.split("#", 1)
                evidence = {
                    "file": file_part,
                    "h2": section_part
                }
    
    # Ğ”Ğ»Ñ legacy Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ‚Ğ°
    elif "used_chunks" in response_data:
        used_chunks = response_data["used_chunks"]
        if used_chunks:
            first_chunk = used_chunks[0]
            if isinstance(first_chunk, str) and "#" in first_chunk:
                file_part, section_part = first_chunk.split("#", 1)
                evidence = {
                    "file": file_part,
                    "h2": section_part
                }
    
    return evidence


def log_query(q: str, session_id: str = None) -> None:
    """Ğ›Ğ¾Ğ³Ğ¸Ñ€ÑƒĞµÑ‚ Ğ·Ğ°Ğ¿Ñ€Ğ¾Ñ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ñ"""
    log_q.info(json.dumps({"q": q, "session": session_id or "unknown"}, ensure_ascii=False))


def log_response(q: str, candidates: List[Dict[str, Any]] = None, answer_len: int = 0) -> None:
    """Ğ›Ğ¾Ğ³Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¾Ñ‚Ğ²ĞµÑ‚ Ğ±Ğ¾Ñ‚Ğ°"""
    log_r.info(json.dumps({
        "q": q, 
        "candidates": candidates or [], 
        "answer_len": answer_len
    }, ensure_ascii=False))


def log_minimal(q: str, best_score: float, threshold: float, cta: bool, low_rel: bool = False) -> None:
    """Ğ›Ğ¾Ğ³Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¼Ğ¸Ğ½Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½ÑƒÑ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ñ Ğ¾ Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞµ"""
    log_m.info(json.dumps({
        "q": q, 
        "best": best_score, 
        "thr": threshold, 
        "cta": cta, 
        "low_rel": low_rel
    }, ensure_ascii=False))


def format_candidates_for_log(candidates: list[Any], top: int | None = None):
    """
    Ğ£Ğ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ğ°Ñ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ñ Ğ´Ğ»Ñ Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ ĞºĞ°Ğ½Ğ´Ğ¸Ğ´Ğ°Ñ‚Ğ¾Ğ² Ğ² Ğ»Ğ¾Ğ³Ğ°Ñ….
    
    Args:
        candidates: Ğ¡Ğ¿Ğ¸ÑĞ¾Ğº ĞºĞ°Ğ½Ğ´Ğ¸Ğ´Ğ°Ñ‚Ğ¾Ğ² (ĞºĞ¾Ñ€Ñ‚ĞµĞ¶Ğ¸, Ğ¾Ğ±ÑŠĞµĞºÑ‚Ñ‹, ÑĞ»Ğ¾Ğ²Ğ°Ñ€Ğ¸)
        top: ĞœĞ°ĞºÑĞ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ğ¾Ğµ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ¾ ĞºĞ°Ğ½Ğ´Ğ¸Ğ´Ğ°Ñ‚Ğ¾Ğ² Ğ´Ğ»Ñ Ğ»Ğ¾Ğ³Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ
    
    Returns:
        Ğ¡Ğ¿Ğ¸ÑĞ¾Ğº ÑĞ»Ğ¾Ğ²Ğ°Ñ€ĞµĞ¹ Ñ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸ĞµĞ¹ Ğ¾ ĞºĞ°Ğ½Ğ´Ğ¸Ğ´Ğ°Ñ‚Ğ°Ñ…
    """
    out = []
    def to_item(cand, rank):
        # Ğ²Ğ°Ñ€Ğ¸Ğ°Ğ½Ñ‚ 1: ĞºĞ¾Ñ€Ñ‚ĞµĞ¶/ÑĞ¿Ğ¸ÑĞ¾Ğº (chunk, score)
        if isinstance(cand, (tuple, list)) and len(cand) >= 2:
            chunk, score = cand[0], cand[1]
        else:
            chunk, score = cand, getattr(cand, "score", getattr(cand, "hybrid", getattr(cand, "rerank", None)))
        h2  = getattr(chunk, "h2", getattr(chunk, "h2_id", None))
        doc = getattr(chunk, "doc", getattr(chunk, "file", getattr(chunk, "doc_id", None)))
        try:
            s = float(score) if score is not None else None
        except Exception:
            s = None
        return {"rank": rank, "score": s, "h2": h2, "doc": doc}
    for i, c in enumerate(candidates, 1):
        out.append(to_item(c, i))
        if top and i >= top:
            break
    return out
