# rag_engine.py
import os
import numpy as np
from openai import OpenAI
import faiss
import yaml
import re
import json
import difflib
import random
from pathlib import Path
from dotenv import load_dotenv
from typing import Dict, List, Optional, Any, Set, Tuple
from textwrap import dedent
from rank_bm25 import BM25Okapi
from rapidfuzz import fuzz
from core.empathy import detect_emotion, build_answer

# ==== –ö–û–ù–°–¢–ê–ù–¢–´ ====
CONFIG_DIR = Path("config")
MD_DIR = Path("md")
THEMES_PATH = CONFIG_DIR / "themes.json"

# ==== –ê–ù–¢–ò-–í–û–î–ê –†–ï–ì–ï–ö–°–´ ====
ANTI_FLUFF = [
    r"^–º—ã\s+—Ä–∞–¥—ã\s+—Å–æ–æ–±—â–∏—Ç—å.*",
    r"^—Å\s+—Ä–∞–¥–æ—Å—Ç—å—é\s+—Å–æ–æ–±—â–∞–µ–º.*",
    r"^–º—ã\s+–≥–æ—Ä–¥–∏–º—Å—è.*",
    r"^–≤\s+–Ω–∞—à–µ–π\s+–∫–ª–∏–Ω–∏–∫–µ\s+–º—ã.*",
    r"^—ç—Ç–æ\s+–∑–∞–º–µ—á–∞—Ç–µ–ª—å–Ω[–æ—ã].*",
    r"–ø–æ—Å—Ç–∞—Ä–∞–µ–º—Å—è\s+–º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ\s+–ø–æ–Ω—è—Ç—å.*",
    r"^—Å–æ–≤—Ä–µ–º–µ–Ω–Ω[–∞-—è]+\s+—Ç–µ—Ö–Ω–æ–ª–æ–≥–∏.*",
    r"^–æ—Ç–∫—Ä—ã–≤–∞–µ—Ç\s+–≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏.*",
]

# ==== –ì–õ–û–ë–ê–õ–¨–ù–´–ï –°–¢–†–£–ö–¢–£–†–´ –î–õ–Ø –ö–ê–¢–ê–õ–û–ì–ê –°–£–©–ù–û–°–¢–ï–ô ====
ENTITY_INDEX = {}  # alias_norm -> {"topic": str, "entity": str, "doc_id": str, "section": str}
ENTITY_CHUNKS = {}  # (topic, entity) -> RetrievedChunk
ALIAS_MAP = {}  # normalize(alias) -> {"file": path, "primary_h2_id": ...}

# ==== –ù–û–í–ê–Ø –ê–†–•–ò–¢–ï–ö–¢–£–†–ê –ò–ù–î–ï–ö–°–û–í ====
# === 1) –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è —Ç–µ–º ===
CANON = {"doctors","consultation","prices","warranty","contacts","implants","safety","clinic"}

def norm_topic(x: str) -> str:
    return (x or "").strip().lower()

def norm_text(x: str) -> str:
    import unicodedata
    x = unicodedata.normalize("NFKD", (x or "").lower().strip())
    x = re.sub(r"\s+", " ", x)
    return x

# === 2) –ö–æ–Ω—Ç–µ–π–Ω–µ—Ä—ã –∏–Ω–¥–µ–∫—Å–æ–≤ ===
ALIAS_MAP_GLOBAL = {}   # norm(alias) -> {"topic":..., "file":...}
H2_INDEX = {}           # norm(h2_text/h2_id/local_alias) -> {"topic":..., "file":..., "h2_id":...}
FILE_META = {}          # file -> {"topic":..., "aliases": [...], "mini_links":[...]}
ALL_CHUNKS = []         # –≤–∞—à–∏ —á–∞–Ω–∫-–æ–±—ä–µ–∫—Ç—ã (–∫–∞–∫ –∏ —Ä–∞–Ω—å—à–µ)

# ==== BM25 –ò–ù–î–ï–ö–° ====
bm25_index = None
bm25_corpus = []

def _normalize(text: str) -> str:
    # NBSP -> –æ–±—ã—á–Ω—ã–π –ø—Ä–æ–±–µ–ª; CRLF -> LF
    return text.replace('\u00A0', ' ').replace('\r\n', '\n').replace('\r', '\n')

def _norm(s: str) -> str:
    """–ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è —Å—Ç—Ä–æ–∫–∏ –¥–ª—è –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏"""
    return re.sub(r'\s+', ' ', s.lower().replace('\u00a0', ' ')).strip()

def _slugify_implant_kind(name: str) -> str:
    """–°–æ–∑–¥–∞–µ—Ç slug –¥–ª—è –≤–∏–¥–∞ –∏–º–ø–ª–∞–Ω—Ç–∞—Ü–∏–∏"""
    n = _norm(name)
    
    # –°–ø–µ—Ü–∏—Ñ–∏—á–Ω—ã–µ –º–∞–ø–ø–∏–Ω–≥–∏
    if re.search(r'all\s*[- ]?on\s*[- ]?4', n):
        return "all-on-4"
    if re.search(r'all\s*[- ]?on\s*[- ]?6', n):
        return "all-on-6"
    if n.startswith("–æ–¥–Ω–æ"):
        return "single-stage"
    if n.startswith("–∫–ª–∞—Å—Å"):
        return "classic"
    
    # –û–±—â–∏–π slug
    return re.sub(r'[^a-z0-9\-]+', '-', n)  # unidecode(n) - –æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ

load_dotenv()

# –ó–∞–≥—Ä—É–∑–∫–∞ –∫–æ–Ω—Ñ–∏–≥–æ–≤ —ç–º–ø–∞—Ç–∏–∏
with open(os.path.join("config", "empathy_config.yaml"), "r", encoding="utf-8") as f:
    EMPATHY_CFG = yaml.safe_load(f)
with open(os.path.join("config","empathy.yaml"), "r", encoding="utf-8") as f:
    EMPATHY_BANK = yaml.safe_load(f)
with open(os.path.join("config", "empathy_triggers.yaml"), "r", encoding="utf-8") as f:
    EMPATHY_TRIGGERS = yaml.safe_load(f)
_RNG = random.Random()

# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è OpenAI –∫–ª–∏–µ–Ω—Ç–∞ v1
try:
    openai_client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"), timeout=30.0)
    print("‚úÖ OpenAI –∫–ª–∏–µ–Ω—Ç v1 —É—Å–ø–µ—à–Ω–æ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω")
except Exception as e:
    print(f"‚ùå –û—à–∏–±–∫–∞ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ OpenAI –∫–ª–∏–µ–Ω—Ç–∞: {e}")
    openai_client = None

# –¢–∏–ø—ã –¥–∞–Ω–Ω—ã—Ö
class Frontmatter:
    def __init__(self, data: Dict[str, Any]):
        self.id = data.get('id', '')
        self.slug = data.get('slug', '')
        self.title = data.get('title', '')
        self.description = data.get('description', '')
        self.doc_type = data.get('doc_type', 'info')
        self.topic = data.get('topic', '')
        self.tags = data.get('tags', [])
        self.aliases = data.get('aliases', [])  # ‚úÖ –î–æ–±–∞–≤–ª—è–µ–º aliases
        self.audience = data.get('audience', '')
        self.updated = data.get('updated', '')
        self.locale = data.get('locale', 'ru-RU')
        self.tone = data.get('tone', 'friendly')
        self.emotion = data.get('emotion', '')
        self.criticality = data.get('criticality', 'medium')
        self.verbatim = data.get('verbatim', False)
        self.policy_ref = data.get('policy_ref', [])
        self.source = data.get('source', [])
        self.preferred_format = data.get('preferred_format', ['short', 'bullets', 'cta'])
        self.canonical_url = data.get('canonical_url', '')
        self.noindex = data.get('noindex', False)
        self.cta_action = data.get('cta_action', '')
        self.cta_text = data.get('cta_text', '')
        self.cta_link = data.get('cta_link', '')

class RetrievedChunk:
    def __init__(self, id: str, text: str, metadata: Frontmatter, file_name: str):
        self.id = id
        self.text = text
        self.metadata = metadata
        self.file_name = file_name
        # –ë–µ–∑–æ–ø–∞—Å–Ω–æ–µ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ –∞—Ç—Ä–∏–±—É—Ç–æ–≤ —Å fallback –∑–Ω–∞—á–µ–Ω–∏—è–º–∏
        self.updated = metadata.updated if metadata else ""
        self.criticality = metadata.criticality if metadata else "medium"
        self.doc_type = metadata.doc_type if metadata else "info"
        self.tags = metadata.tags if metadata else []

class SynthJSON:
    def __init__(self, short: str, bullets: List[str], cta: str, used_chunks: List[str], tone: str, warnings: Optional[List[str]] = None):
        self.short = short
        self.bullets = bullets
        self.cta = cta
        self.used_chunks = used_chunks
        self.tone = tone
        self.warnings = warnings or []

# === 3) –ü–∞—Ä—Å MD ===
RX_H2 = re.compile(r"(?m)^##\s+([^\n{]+?)(?:\s*\{#([^\}]+)\})?\s*$")
RX_LOC = re.compile(r"<!--\s*aliases:\s*\[(.*?)\]\s*-->", re.S|re.I)

def parse_frontmatter(text: str):
    if not text.startswith("---"): return {}, text
    m = re.match(r"^---\s*\n(.*?)\n---\s*\n(.*)$", text, re.S)
    if not m: return {}, text
    fm = yaml.safe_load(m.group(1)) or {}
    body = m.group(2)
    return fm, body

def parse_h2_sections(body: str):
    sections = []
    for m in RX_H2.finditer(body):
        title = m.group(1).strip()
        h2_id = m.group(2) or slugify(title)
        tail = body[m.end(): m.end()+400]
        loc = RX_LOC.search(tail)
        local_aliases = []
        if loc:
            raw = loc.group(1)
            local_aliases = [a.strip().strip("'\"") for a in re.split(r",\s*", raw) if a.strip()]
        sections.append({"title": title, "h2_id": h2_id, "local_aliases": local_aliases})
    return sections

def slugify(s: str) -> str:
    import unicodedata
    s = unicodedata.normalize("NFKD", s.lower().strip())
    s = re.sub(r"\s+", "-", s)
    s = re.sub(r"[^a-z0-9\-–∞-—è—ë]", "", s)
    return s

# === 4) –†–µ–≥–∏—Å—Ç—Ä–∞—Ü–∏—è –∏–Ω–¥–µ–∫—Å–æ–≤ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ —Ñ–∞–π–ª–∞ ===
def register_file(file_name: str, md_text: str):
    fm, body = parse_frontmatter(md_text)
    doc_type = norm_topic(fm.get("doc_type"))
    topic = norm_topic(fm.get("topic") or doc_type)
    if topic not in CANON:
        topic = "clinic"  # fallback –Ω–∞ clinic

    aliases = fm.get("aliases") or []
    if isinstance(aliases, str): aliases = [aliases]
    mini_links = fm.get("mini_links") or []

    FILE_META[file_name] = {"topic": topic, "aliases": aliases, "mini_links": mini_links}

    # –≥–ª–æ–±–∞–ª—å–Ω—ã–µ –∞–ª–∏–∞—Å—ã ‚Üí —Ñ–∞–π–ª/—Ç–µ–º–∞
    for a in aliases:
        ALIAS_MAP_GLOBAL[norm_text(a)] = {"topic": topic, "file": file_name}

    # H2 –∏ –ª–æ–∫–∞–ª—å–Ω—ã–µ –∞–ª–∏–∞—Å—ã ‚Üí —Ç–æ—á–Ω—ã–π –∏–Ω–¥–µ–∫—Å
    for s in parse_h2_sections(body):
        # –∏–Ω–¥–µ–∫—Å–∏—Ä—É–µ–º –∑–∞–≥–æ–ª–æ–≤–æ–∫ –∏ h2_id
        for key in [s["title"], s["h2_id"], *s["local_aliases"]]:
            H2_INDEX[norm_text(key)] = {"topic": topic, "file": file_name, "h2_id": s["h2_id"]}

# === 5) DEFAULT_H2 (—Å—Ç—Ä–∞—Ö–æ–≤–∫–∞) ===
DEFAULT_H2 = {
  "consultation": ("consultation-free.md",         "–æ–±–∑–æ—Ä"),
  "prices":       ("prices-clinic.md",             None),
  "warranty":     ("warranty.md",                  "–æ–±–∑–æ—Ä"),
  "contacts":     ("clinic-contacts.md",           None),
  "implants":     ("implants-overview.md",         "–æ–±–∑–æ—Ä"),
  "safety":       ("implants-contraindications.md","–æ–±–∑–æ—Ä"),
  "doctors":      ("doctors.md",                   "–æ–±–∑–æ—Ä"),
  "clinic":       ("advantages-general.md",        "–æ–±–∑–æ—Ä")
}

def _find_chunk(file_name: str, h2_id: str|None):
    for ch in ALL_CHUNKS:
        if ch.file_name == file_name:
            if h2_id is None or getattr(ch.metadata, "h2_id", None) == h2_id:
                return ch
    return None

def get_default_chunk_for_topic(topic: str):
    file_name, h2_id = DEFAULT_H2.get(topic, (None, None))
    if file_name:
        ch = _find_chunk(file_name, h2_id)
        if ch: return ch, {"source":"default","topic":topic,"exact_h2_match":bool(h2_id)}
    # –∑–∞–ø–∞—Å–Ω–æ–π –ø—É—Ç—å ‚Äî –ø–µ—Ä–≤—ã–π —á–∞–Ω–∫ —ç—Ç–æ–π —Ç–µ–º—ã
    for ch in ALL_CHUNKS:
        if getattr(ch.metadata, "topic", None) == topic:
            return ch, {"source":"default-any","topic":topic,"exact_h2_match":False}
    return None, {}

def parse_yaml_front_matter(text: str):
    """–ü–∞—Ä—Å–∏—Ç YAML front matter –∏–∑ markdown —Ñ–∞–π–ª–∞"""
    pattern = r'^---\s*\n(.*?)\n---\s*\n'
    m = re.search(pattern, text, re.DOTALL | re.MULTILINE)
    
    if m:
        yaml_content = m.group(1)
        try:
            metadata = yaml.safe_load(yaml_content) or {}
        except yaml.YAMLError:
            metadata = {}
        # –ö–õ–Æ–ß–ï–í–ê–Ø –ü–†–ê–í–ö–ê: –≤—ã—Ä–µ–∑–∞–µ–º —Ç–æ–ª—å–∫–æ –ø–µ—Ä–≤—ã–π front-matter
        content = text[m.end():]
        return Frontmatter(metadata), content.strip()
    else:
        return Frontmatter({}), text

def chunk_text_by_sections(content: str, file_name: str) -> List[RetrievedChunk]:
    """–†–∞–∑–±–∏–≤–∞–µ—Ç —Ç–µ–∫—Å—Ç –Ω–∞ —á–∞–Ω–∫–∏ –ø–æ —Å–µ–∫—Ü–∏—è–º (## –∏ ###)"""
    chunks = []
    
    # –†–∞–∑—Ä–µ–∑–∞–µ–º –ù2 (## ...)
    h2_blocks = re.split(r'(?m)^\s*##\s+', content)
    
    for block in h2_blocks:
        block = block.strip()
        if not block:
            continue
        
        lines = block.splitlines()
        h2_title = lines[0]
        h2_body = '\n'.join(lines[1:])
        
        # –ï—Å–ª–∏ –≤–Ω—É—Ç—Ä–∏ –µ—Å—Ç—å –ù3 —Ä–µ–∂–µ–º –ø–æ –Ω–∏–º (### ...)
        h3_blocks = re.split(r'(?m)^\s*###\s+', block)
        if len(h3_blocks) > 1:
            # parts[0] ‚Äî —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ –¥–æ –ø–µ—Ä–≤–æ–≥–æ ### <- –≠–¢–û –ù–£–ñ–ù–û –°–û–•–†–ê–ù–ò–¢–¨!
            if h3_blocks[0].strip():
                preamble_text = f"## {h2_title}\n{h3_blocks[0].strip()}"
                chunk_id = f"{file_name}#{h2_title}_preamble"
                temp_metadata = Frontmatter({})
                chunks.append(RetrievedChunk(chunk_id, preamble_text.strip(), temp_metadata, file_name))
            
            # –¥–∞–ª—å—à–µ —Å–æ–∑–¥–∞—Ç—å —á–∞–Ω–∫–∏ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ ### –∫–∞–∫ —Å–µ–π—á–∞—Å
            for h3 in h3_blocks[1:]:
                h3 = h3.strip()
                if not h3:
                    continue
                
                lines = h3.splitlines()
                h3_title = lines[0]
                h3_body = '\n'.join(lines[1:])
                
                text = f"## {h2_title}\n### {h3_title}\n{h3_body}"
                chunk_id = f"{file_name}#{h2_title}_{h3_title}"
                temp_metadata = Frontmatter({})
                chunks.append(RetrievedChunk(chunk_id, text.strip(), temp_metadata, file_name))
        else:
            # –ù3 –Ω–µ—Ç - —Å–æ—Ö—Ä–∞–Ω—è–µ–º –ù2 –∫–∞–∫ –µ–¥–∏–Ω—ã–π —á–∞–Ω–∫
            text = f"## {h2_title}\n{h2_body}"
            chunk_id = f"{file_name}#{h2_title}"
            temp_metadata = Frontmatter({})
            chunks.append(RetrievedChunk(chunk_id, text.strip(), temp_metadata, file_name))
    
    return chunks

def extract_aliases_from_chunk(chunk_text: str) -> List[str]:
    """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –∞–ª–∏–∞—Å—ã –∏–∑ HTML-–∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–µ–≤ –≤ —á–∞–Ω–∫–µ"""
    m = re.search(r'<!--\s*aliases:\s*\[(.*?)\]\s*-->', chunk_text, re.S)
    if not m:
        return []
    raw = m.group(1)
    pairs = re.findall(r'"([^"]+)"|\'([^\']+)\'', raw)
    aliases = [a or b for a, b in pairs]
    return [a.strip() for a in aliases if a and a.strip()]

def update_entity_index(chunk: RetrievedChunk, topic: str, entity_key: str):
    """–û–±–Ω–æ–≤–ª—è–µ—Ç ENTITY_INDEX —Å –∞–ª–∏–∞—Å–∞–º–∏ –∏–∑ —á–∞–Ω–∫–∞"""
    # –ò–∑–≤–ª–µ–∫–∞–µ–º –∑–∞–≥–æ–ª–æ–≤–æ–∫ ##
    header_match = re.search(r'(?m)^##\s+(.+?)\s*$', chunk.text)
    title = header_match.group(1).strip() if header_match else ""
    
    # –ê–ª–∏–∞—Å—ã: HTML-–∫–æ–º–º–µ–Ω—Ç –≤ —Ç–µ–∫—Å—Ç–µ + —Ñ—Ä–æ–Ω—Ç–º–∞—Ç—Ç–µ—Ä
    aliases = extract_aliases_from_chunk(chunk.text)
    if hasattr(chunk, "metadata") and getattr(chunk.metadata, "aliases", None):
        aliases.extend(chunk.metadata.aliases)
    if title:
        aliases.append(title)
    
    # –ò–Ω–¥–µ–∫—Å–∏—Ä—É–µ–º –≤—Å–µ –∞–ª–∏–∞—Å—ã
    for alias in aliases:
        alias_norm = _norm(alias)
        if alias_norm:
            ENTITY_INDEX[alias_norm] = {
                "topic": topic,
                "entity": entity_key,
                "doc_id": chunk.file_name,
                "section": title
            }

# –ó–∞–≥—Ä—É–∂–∞–µ–º –∏ —Ä–∞–∑–±–∏–≤–∞–µ–º –≤—Å–µ markdown-—Ñ–∞–π–ª—ã
folder_path = Path("md/")
all_chunks: List[RetrievedChunk] = []

# –ì–ª–æ–±–∞–ª—å–Ω—ã–µ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –¥–ª—è –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–æ–≥–æ –ø–æ–∏—Å–∫–∞ –≤—Ä–∞—á–µ–π
DOCTOR_NAME_TOKENS: set[str] = set()
# –ü—Ä—è–º–∞—è –∞–¥—Ä–µ—Å–∞—Ü–∏—è –≤—Ä–∞—á–µ–π: –∏–º—è -> —á–∞–Ω–∫
DOCTOR_NAME_TO_CHUNK: dict[str, RetrievedChunk] = {}
DOCTOR_REGEX = None
DOCTOR_NAME_REGEX = None  # —Ç–æ–ª—å–∫–æ –∏–º–µ–Ω–∞
DOCTOR_QUERY_REGEX = None  # –∏–º–µ–Ω–∞ + —Å–ª–æ–≤–∞ "–≤—Ä–∞—á/–¥–æ–∫—Ç–æ—Ä/..."

# ==== THEMES (—É—Å–∏–ª–µ–Ω–∏–µ –ø–æ —Ç–µ–º–∞–º) ====
try:
    from pathlib import Path
    import json, re
    
    BASE_DIR = Path(__file__).resolve().parent
    THEMES_PATH = BASE_DIR / "config" / "themes.json"
    
    with open(THEMES_PATH, "r", encoding="utf-8") as f:
        THEME_MAP = json.load(f)
    
    # –∫–æ–º–ø–∏–ª—è—Ü–∏—è —Ä–µ–≥–µ–∫—Å–æ–≤ –∏ –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è
    for k, cfg in THEME_MAP.items():
        cfg["query_regex_compiled"] = re.compile(cfg["query_regex"], re.IGNORECASE)
        cfg["tag_aliases"] = [str(t).lower() for t in cfg.get("tag_aliases", [])]
        cfg["weight"] = float(cfg.get("weight", 0.2))
    print(f"OK: themes.json loaded: {len(THEME_MAP)} themes from {THEMES_PATH}")
    for theme, cfg in THEME_MAP.items():
        print(f"  Theme: {theme}: weight={cfg.get('weight', 'N/A')}, regex={cfg.get('query_regex', 'N/A')[:30]}...")
except Exception as e:
    print(f"WARNING: –ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å themes.json: {e}")
    THEME_MAP = {}

def _extract_doctor_names_from_text(text: str) -> list[str]:
    """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –∏–º–µ–Ω–∞ –≤—Ä–∞—á–µ–π –∏–∑ —Ç–µ–∫—Å—Ç–∞"""
    names = set()
    
    for line in text.split('\n'):
        line = line.strip()
        if not line:
            continue
            
        # —Å—Ä–µ–∑–∞—Ç—å –ø—Ä–µ—Ñ–∏–∫—Å ### / ##
        if line.startswith('### '):
            line = line[4:]
        elif line.startswith('## '):
            line = line[3:]
            
        # –ò–º—è –§–∞–º–∏–ª–∏—è [–û—Ç—á–µ—Å—Ç–≤–æ]
        match = re.match(r'^[–ê-–Ø–Å][–∞-—è—ë]+(?:\s+[–ê-–Ø–Å][–∞-—è—ë]+){1,2}$', line)
        if match:
            full = match.group(0)
            parts = full.split()
            if len(parts) >= 2:
                names.add(full)  # –ø–æ–ª–Ω–æ–µ –§–ò–û
                names.add(parts[0])  # —Ñ–∞–º–∏–ª–∏—è
                names.add(f"{parts[0]} {parts[1]}")  # –§–∞–º–∏–ª–∏—è –ò–º—è
                names.add(f"{parts[1]} {parts[0]}")  # –ò–º—è –§–∞–º–∏–ª–∏—è
                print(f"    üè• –ò–∑–≤–ª–µ—á–µ–Ω–æ –∏–º—è: '{full}' ‚Üí {parts[0]}, {parts[1]} {parts[0]}")
    
    return sorted(list(names))

def _rebuild_doctor_regex():
    """–ü–µ—Ä–µ—Å–æ–±–∏—Ä–∞–µ—Ç regex –¥–ª—è –ø–æ–∏—Å–∫–∞ –≤—Ä–∞—á–µ–π"""
    global DOCTOR_REGEX, DOCTOR_NAME_REGEX, DOCTOR_QUERY_REGEX
    
    base = ['–≤—Ä–∞—á', '–¥–æ–∫—Ç–æ—Ä', '—Ö–∏—Ä—É—Ä–≥', '–∏–º–ø–ª–∞–Ω—Ç–æ–ª–æ–≥', '—Å–ø–µ—Ü–∏–∞–ª–∏—Å—Ç']
    name_tokens = sorted([re.escape(t) for t in DOCTOR_NAME_TOKENS], key=len, reverse=True)
    base_tokens = [re.escape(t) for t in base]

    if name_tokens:
        DOCTOR_NAME_REGEX = re.compile(r'(' + '|'.join(name_tokens) + r')', re.IGNORECASE)
        DOCTOR_QUERY_REGEX = re.compile(r'(' + '|'.join(name_tokens + base_tokens) + r')', re.IGNORECASE)
    else:
        DOCTOR_NAME_REGEX = None
        DOCTOR_QUERY_REGEX = re.compile(r'(' + '|'.join(base_tokens) + r')', re.IGNORECASE)
    
    # –û—Å—Ç–∞–≤–ª—è–µ–º —Å—Ç–∞—Ä—ã–π DOCTOR_REGEX –¥–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏
    DOCTOR_REGEX = DOCTOR_QUERY_REGEX

def build_empathy_prompt(tone: str = "friendly", emotion: str = "empathy", allow_emoji: bool = True, cta_text: str | None = None, cta_link: str | None = None) -> str:
    """–°–æ–±–∏—Ä–∞–µ—Ç –∫–æ—Ä–æ—Ç–∫–∏–π –ø—Ä–æ–º–ø—Ç –¥–ª—è ¬´–æ–∂–∏–≤–ª–µ–Ω–∏—è¬ª –æ—Ç–≤–µ—Ç–∞ –±–µ–∑ –∏—Å–∫–∞–∂–µ–Ω–∏—è —Ñ–∞–∫—Ç–æ–≤."""
    
    emoji_rule = "–ò–Ω–æ–≥–¥–∞ (–Ω–µ —á–∞—â–µ –æ–¥–Ω–æ–≥–æ-–¥–≤—É—Ö –Ω–∞ –≤–µ—Å—å –æ—Ç–≤–µ—Ç) –¥–æ–±–∞–≤–ª—è–π —É–º–µ—Å—Ç–Ω—ã–µ —ç–º–æ–¥–∑–∏ ü¶∑üòåüìÖüí¨. –ï—Å–ª–∏ –Ω–µ—É–º–µ—Å—Ç–Ω–æ ‚Äî –Ω–µ –¥–æ–±–∞–≤–ª—è–π." if allow_emoji else "–ù–µ –∏—Å–ø–æ–ª—å–∑—É–π —ç–º–æ–¥–∑–∏."
    
    cta_rule = ""
    if cta_text:
        cta_rule = f"–ó–∞–≤–µ—Ä—à–∏ –º—è–≥–∫–æ–π —Ñ—Ä–∞–∑–æ–π-–ø—Ä–∏–≥–ª–∞—à–µ–Ω–∏–µ–º: ¬´{cta_text}¬ª"
        if cta_link:
            cta_rule += f" (—Å—Å—ã–ª–∫–∞: {cta_link})."
    
    prompt = dedent(f"""
    –¢—ã - –≤–Ω–∏–º–∞—Ç–µ–ª—å–Ω—ã–π –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç —Å—Ç–æ–º–∞—Ç–æ–ª–æ–≥–∏—á–µ—Å–∫–æ–π –∫–ª–∏–Ω–∏–∫–∏.
    –¢–æ–Ω: {tone}, —ç–º–æ—Ü–∏—è: {emotion}. –ü–∏—à–∏ –ø—Ä–æ—Å—Ç–æ –∏ –ø–æ-—á–µ–ª–æ–≤–µ—á–µ—Å–∫–∏.
    
    –ó–∞–¥–∞—á–∞:
    1. –í–æ–∑—å–º–∏ —Ç–µ–∫—Å—Ç –∏ –ø–µ—Ä–µ—Ñ–æ—Ä–º—É–ª–∏—Ä—É–π —Ç–µ–ø–ª–µ–µ, –Ω–æ –°–û–•–†–ê–ù–ò –í–°–ï –ß–ò–°–õ–ê –ò –ü–†–û–¶–ï–ù–¢–´ –ë–ï–ó –ò–ó–ú–ï–ù–ï–ù–ò–ô (–Ω–∞–ø—Ä–∏–º–µ—Ä: 99,8%, 0,2%, 97-98%).
    2. –ù–∏—á–µ–≥–æ –Ω–µ –≤—ã–¥—É–º—ã–≤–∞–π ‚Äî —Ç–æ–ª—å–∫–æ —Ñ–∞–∫—Ç—ã –∏–∑ —Ç–µ–∫—Å—Ç–∞.
    3. –°—Ç—Ä—É–∫—Ç—É—Ä–∞: –∫–æ—Ä–æ—Ç–∫–æ–µ –≤—Å—Ç—É–ø–ª–µ–Ω–∏–µ -> –ø–æ–ª–µ–∑–Ω—ã–µ –¥–µ—Ç–∞–ª–∏ (–∞–±–∑–∞—Ü—ã –∏–ª–∏ —Å–ø–∏—Å–æ–∫) -> –º—è–≥–∫–æ–µ –∑–∞–≤–µ—Ä—à–µ–Ω–∏–µ.
    4. –ë–µ–∑ —Å–ª—É–∂–µ–±–Ω—ã—Ö –ø–æ–º–µ—Ç–æ–∫ –∏ –∑–∞–≥–æ–ª–æ–≤–∫–æ–≤ ##, –±–µ–∑ HTML. –†–∞–∑–¥–µ–ª—è–π –∞–±–∑–∞—Ü—ã –ø—É—Å—Ç–æ–π —Å—Ç—Ä–æ–∫–æ–π; —Å–ø–∏—Å–∫–∏ ‚Äî –¥–µ—Ñ–∏—Å
    5. {emoji_rule}
    6. {cta_rule}
    
    –û—Ç–≤–µ—á–∞–π —Ç–æ–ª—å–∫–æ —Ç–µ–∫—Å—Ç–æ–º.
    """).strip()
    
    return prompt

def postprocess_answer_with_empathy(base_text: str, tone: str = "friendly", emotion: str = "empathy", cta_text: str | None = None, cta_link: str | None = None) -> str:
    """–î–µ–ª–∞–µ—Ç —Ñ–∏–Ω–∞–ª—å–Ω—É—é ¬´–æ–∂–∏–≤–ª—è—é—â—É—é¬ª –ø–µ—Ä–µ—Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–∫—É –æ—Ç–≤–µ—Ç–∞. –ï—Å–ª–∏ LLM –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∞/–æ—à–∏–±–∫–∞ ‚Äî –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç –∏—Å—Ö–æ–¥–Ω—ã–π base_text."""
    
    try:
        allow_emoji = True  # –≤–º–µ—Å—Ç–æ random.random() < 0.33
        
        system_prompt = build_empathy_prompt(tone, emotion, allow_emoji, cta_text, cta_link)
        
        completion = openai_client.chat.completions.create(
            model="gpt-4o-mini",
            temperature=0.2,  # –µ—â–µ –º–µ–Ω—å—à–µ –∫—Ä–µ–∞—Ç–∏–≤–Ω–æ—Å—Ç–∏
            top_p=0.8,       # –æ–≥—Ä–∞–Ω–∏—á–∏—Ç—å –≤—ã–±–æ—Ä —Ç–æ–∫–µ–Ω–æ–≤
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": base_text}
            ]
        )
        
        text = completion.choices[0].message.content.strip()
        
        # –ü–æ—Å—Ç–æ–±—Ä–∞–±–æ—Ç–∫–∞
        text = re.sub(r'^\s*#{1,6}\s*', '', text, flags=re.MULTILINE)  # —É–±–∏—Ä–∞–µ–º –∑–∞–≥–æ–ª–æ–≤–∫–∏
        text = re.sub(r'\n{3,}', '\n\n', text)  # –Ω–æ—Ä–º–∞–ª–∏–∑—É–µ–º –ø–µ—Ä–µ–Ω–æ—Å—ã
        
        return text
        
    except Exception as e:
        print(f"–ü–æ—Å—Ç–ø—Ä–æ—Ü–µ—Å—Å–æ—Ä –Ω–µ —Å—Ä–∞–±–æ—Ç–∞–ª: {e}")
        return base_text

def theme_boost(score: float, theme_key: str, cfg: dict, chunk) -> float:
    # –±—É—Å—Ç–∏–º, –µ—Å–ª–∏ –≤ —Ç–µ–≥–∞—Ö –∏–ª–∏ —Ç–µ–∫—Å—Ç–µ –µ—Å—Ç—å —Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏–µ –∞–ª–∏–∞—Å—ã
    text_l = chunk.text.lower()
    tags_l = getattr(chunk.metadata, 'tags_lower', [])
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ç–µ–≥–∏
    tag_match = any(alias in tags_l for alias in cfg["tag_aliases"])
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ç–µ–∫—Å—Ç
    text_match = any(alias in text_l for alias in cfg["tag_aliases"])
    
    if tag_match or text_match:
        boosted_score = score + cfg["weight"]
        print(f"      üéØ –ë—É—Å—Ç {theme_key}: —Ç–µ–≥–∏={tags_l}, —Ç–µ–∫—Å—Ç —Å–æ–¥–µ—Ä–∂–∏—Ç={[alias for alias in cfg['tag_aliases'] if alias in text_l]}")
        return boosted_score
    return score

# –£—Ç–∏–ª–∏—Ç—ã –¥–ª—è –ø–æ–∏—Å–∫–∞ —Ü–∏—Ñ—Ä –∏ –ø—Ä–æ—Ü–µ–Ω—Ç–æ–≤
PCT_RE = re.compile(r'\b\d{1,3}(?:[.,]\d{1,2})?\s?%')
NUM_RE = re.compile(r'\b\d{1,5}\b')

def has_numeric_facts(text: str) -> bool:
    """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç –Ω–∞–ª–∏—á–∏–µ —Ü–∏—Ñ—Ä –∏–ª–∏ –ø—Ä–æ—Ü–µ–Ω—Ç–æ–≤ –≤ —Ç–µ–∫—Å—Ç–µ"""
    text_l = text.lower()
    return bool(PCT_RE.search(text_l) or NUM_RE.search(text_l))

def _find_doctor_direct_or_fuzzy(query: str):
    q = (query or "").lower().replace("\u00a0", "")
    # 1) –ø—Ä—è–º–æ–µ –ø–æ–¥—Å—Ç—Ä–æ—á–Ω–æ–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ –ø–æ –∫–ª—é—á–∞–º
    for k, ch in DOCTOR_NAME_TO_CHUNK.items():
        if k in q:
            return ch
    # 2) –º–æ—Ä—Ñ–æ–ª–æ–≥–∏—á–µ—Å–∫–∏ ¬´–º—è–≥–∫–æ¬ª –ø–æ —Ñ–∞–º–∏–ª–∏–∏: –ú–æ–∏—Å–µ–µ–≤/–ú–æ–∏—Å–µ–µ–≤–∞/–ú–æ–∏—Å–µ–µ–≤—É...
    for k, ch in DOCTOR_NAME_TO_CHUNK.items():
        if " " not in k: # —Ç–æ–ª—å–∫–æ –æ–¥–Ω–æ—Å–ª–æ–≤–Ω—ã–µ –∫–ª—é—á–∏ = —Ñ–∞–º–∏–ª–∏–∏
            if re.search(rf"\b{k}\w*\b", q, flags=re.IGNORECASE):
                return ch
    # 3) –æ–ø–µ—á–∞—Ç–∫–∏
    tokens = re.findall(r"[–∞-—è—ë]{4,}", q)
    keys = list(DOCTOR_NAME_TO_CHUNK.keys())
    for t in tokens:
        best = difflib.get_close_matches(t, keys, n=1, cutoff=0.84)
        if best:
            return DOCTOR_NAME_TO_CHUNK[best[0]]
    return None

def fallback_theme_chunks(theme_key: str, limit: int = 3):
    """–ï—Å–ª–∏ —Å–µ–º–∞–Ω—Ç–∏–∫–∞ –ø—Ä–æ–º–∞—Ö–Ω—É–ª–∞—Å—å - –≤–µ—Ä–Ω—É—Ç—å –Ω–µ—Å–∫–æ–ª—å–∫–æ —è–≤–Ω—ã—Ö —Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏—Ö —á–∞–Ω–∫–æ–≤."""
    if not theme_key:
        return []
    
    cfg = THEME_MAP[theme_key]
    out = []
    
    print(f"üîç Fallback –¥–ª—è —Ç–µ–º—ã '{theme_key}': –∏—â–µ–º —Ç–µ–≥–∏ {cfg['tag_aliases']}")
    
    for ch in all_chunks:
        tags_l = getattr(ch.metadata, 'tags_lower', [])
        text_l = ch.text.lower()
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ç–µ–≥–∏
        tag_match = any(alias in (tags_l or []) for alias in cfg["tag_aliases"])
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ç–µ–∫—Å—Ç
        text_match = any(alias in text_l for alias in cfg["tag_aliases"])
        
        if tag_match or text_match:
            out.append(ch)
            print(f"  ‚úÖ –ù–∞–π–¥–µ–Ω —á–∞–Ω–∫: {ch.file_name} (—Ç–µ–≥–∏: {tags_l}, —Ç–µ–∫—Å—Ç —Å–æ–¥–µ—Ä–∂–∏—Ç: {[alias for alias in cfg['tag_aliases'] if alias in text_l]})")
            if len(out) >= limit:
                break
    
    print(f"üîç Fallback –≤–µ—Ä–Ω—É–ª {len(out)} —á–∞–Ω–∫–æ–≤")
    return out

def route_topics(query: str) -> Set[str]:
    """–†–æ—É—Ç–µ—Ä –ø–æ —Ç–µ–º–∞–º - –æ–ø—Ä–µ–¥–µ–ª—è–µ—Ç —Ç–µ–º—ã –∑–∞–ø—Ä–æ—Å–∞"""
    if not hasattr(route_topics, 'theme_map'):
        try:
            with open(THEMES_PATH, "r", encoding="utf-8") as f:
                route_topics.theme_map = json.load(f)
        except Exception as e:
            print(f"‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å themes.json: {e}")
            route_topics.theme_map = {}
    
    detected_topics = set()
    query_lower = query.lower()
    
    for topic, config in route_topics.theme_map.items():
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º regex
        if "query_regex" in config:
            try:
                pattern = re.compile(config["query_regex"], re.IGNORECASE)
                if pattern.search(query):
                    detected_topics.add(topic)
                    continue
            except Exception:
                pass
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º tag_aliases
        if "tag_aliases" in config:
            for alias in config["tag_aliases"]:
                if alias.lower() in query_lower:
                    detected_topics.add(topic)
                    break
    
    return detected_topics

def strip_fluff_start(text: str) -> str:
    """–£–¥–∞–ª—è–µ—Ç –≤—Å—Ç—É–ø–∏—Ç–µ–ª—å–Ω—É—é '–≤–æ–¥—É-–∞–±–∑–∞—Ü' –∏–∑ —Ç–µ–∫—Å—Ç–∞"""
    lines = text.split('\n')
    start_removed = False
    
    for i, line in enumerate(lines):
        line_stripped = line.strip()
        if not line_stripped:
            continue
            
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –≤—Å–µ –∞–Ω—Ç–∏-—Ñ–ª—É—Ñ—Ñ –ø–∞—Ç—Ç–µ—Ä–Ω—ã
        for pattern in ANTI_FLUFF:
            if re.match(pattern, line_stripped, re.IGNORECASE):
                # –£–¥–∞–ª—è–µ–º —ç—Ç—É —Å—Ç—Ä–æ–∫—É –∏ –≤—Å–µ –ø—É—Å—Ç—ã–µ —Å—Ç—Ä–æ–∫–∏ –ø–æ—Å–ª–µ –Ω–µ—ë
                lines = lines[i+1:]
                start_removed = True
                break
        
        if start_removed:
            break
    
    # –£–±–∏—Ä–∞–µ–º –ø—É—Å—Ç—ã–µ —Å—Ç—Ä–æ–∫–∏ –≤ –Ω–∞—á–∞–ª–µ
    while lines and not lines[0].strip():
        lines = lines[1:]
    
    return '\n'.join(lines)

try:
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ –ø–∞–ø–∫–∏ md
    if not folder_path.exists():
        print(f"ERROR: –ü–∞–ø–∫–∞ {folder_path} –ù–ï –Ω–∞–π–¥–µ–Ω–∞!")
        raise Exception(f"–ü–∞–ø–∫–∞ {folder_path} –Ω–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç")
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ doctors.md
    doctors_file = Path("md/doctors.md")
    if doctors_file.exists():
        print(f"OK: –§–∞–π–ª doctors.md –Ω–∞–π–¥–µ–Ω")
    else:
        print(f"ERROR: –§–∞–π–ª doctors.md –ù–ï –Ω–∞–π–¥–µ–Ω!")
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ themes.json
    themes_file = Path("config/themes.json")
    if themes_file.exists():
        print(f"OK: –§–∞–π–ª config/themes.json –Ω–∞–π–¥–µ–Ω")
    else:
        print(f"ERROR: –§–∞–π–ª config/themes.json –ù–ï –Ω–∞–π–¥–µ–Ω!")
    
    for file in folder_path.rglob("*.md"):
        try:
            print(f"üìÑ –û–±—Ä–∞–±–∞—Ç—ã–≤–∞—é —Ñ–∞–π–ª: {file}")
            text = file.read_text(encoding="utf-8")
            print(f"  üìñ –ü—Ä–æ—á–∏—Ç–∞–Ω —Ñ–∞–π–ª: {file.name} ({len(text)} —Å–∏–º–≤–æ–ª–æ–≤)")
            
            # –†–µ–≥–∏—Å—Ç—Ä–∏—Ä—É–µ–º —Ñ–∞–π–ª –≤ –Ω–æ–≤—ã—Ö –∏–Ω–¥–µ–∫—Å–∞—Ö
            register_file(file.name, text)
            
            # –ü–∞—Ä—Å–∏–º YAML front matter
            metadata, content = parse_yaml_front_matter(text)
            content = _normalize(content) # –Ω–æ—Ä–º–∞–ª–∏–∑—É–µ–º –ø—Ä–æ–±–µ–ª—ã
            print(f"  ‚úÖ YAML –ø–∞—Ä—Å–∏–Ω–≥: {metadata.id if metadata.id else '–±–µ–∑ ID'}")
            
            # –†–µ–≥–∏—Å—Ç—Ä–∏—Ä—É–µ–º –∞–ª–∏–∞—Å—ã –¥–ª—è fallback –ø–æ–∏—Å–∫–∞
            try:
                from core.md_loader import register_aliases
                # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º metadata –≤ dict –¥–ª—è register_aliases
                frontmatter_dict = {
                    "aliases": getattr(metadata, 'aliases', []),
                    "primary_h2_id": getattr(metadata, 'primary_h2_id', None)
                }
                register_aliases(frontmatter_dict, str(file))
            except Exception as e:
                print(f"  ‚ö†Ô∏è –û—à–∏–±–∫–∞ —Ä–µ–≥–∏—Å—Ç—Ä–∞—Ü–∏–∏ –∞–ª–∏–∞—Å–æ–≤: {e}")
            
            # –õ–æ–∫–∞–ª—å–Ω–∞—è —Ä–µ–≥–∏—Å—Ç—Ä–∞—Ü–∏—è –∞–ª–∏–∞—Å–æ–≤
            for a in (getattr(metadata, 'aliases', ()) or []):
                ALIAS_MAP[_norm(a)] = {"file": str(file), "primary_h2_id": getattr(metadata, 'primary_h2_id', None)}
            
            # –µ—Å–ª–∏ —ç—Ç–æ —Ñ–∞–π–ª —Å –≤—Ä–∞—á–∞–º–∏ - —Å–æ–±—Ä–∞—Ç—å –∏–º–µ–Ω–∞
            if getattr(metadata, 'doc_type', '') in ('doctor', 'doctors') or file.name == "doctors.md":
                found = _extract_doctor_names_from_text(content)
                if found:
                    DOCTOR_NAME_TOKENS.update(found)
                    print(f"  üè• –ù–∞–π–¥–µ–Ω—ã –≤—Ä–∞—á–∏ –≤ {file.name}: {found}")
            
            # –†–∞–∑–±–∏–≤–∞–µ–º –Ω–∞ —á–∞–Ω–∫–∏ –ø–æ —Å–µ–∫—Ü–∏—è–º
            file_chunks = chunk_text_by_sections(content, file.name)
            print(f"  üìù –°–æ–∑–¥–∞–Ω–æ —á–∞–Ω–∫–æ–≤: {len(file_chunks)}")
            
            # ==== –ò–ù–î–ï–ö–°–ê–¶–ò–Ø –ö–ê–¢–ê–õ–û–ì–ê –ò–ú–ü–õ–ê–ù–¢–û–í ====
            if metadata.doc_type == "catalog" and metadata.topic == "implants":
                print(f"  üè∑Ô∏è –ò–Ω–¥–µ–∫—Å–∏—Ä—É–µ–º –∫–∞—Ç–∞–ª–æ–≥ –∏–º–ø–ª–∞–Ω—Ç–æ–≤ –∏–∑ {file.name}")
                for ch in file_chunks:
                    # –ò—â–µ–º —Å–µ–∫—Ü–∏–∏ –ø–æ ### –ó–∞–≥–æ–ª–æ–≤–æ–∫
                    m = re.search(r'(?m)^###\s+(.+?)\s*$', ch.text)
                    if not m:
                        continue
                    
                    name = m.group(1).strip()
                    
                    # –ü–∞—Ä—Å–∏–º –∞–ª–∏–∞—Å—ã –∏–∑ HTML-–∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–µ–≤
                    alias_m = re.search(r'<!--\s*aliases:\s*\[(.*?)\]\s*-->', ch.text)
                    aliases = [name]
                    if alias_m:
                        aliases.extend([a.strip() for a in alias_m.group(1).split(',')])
                    
                    # –°–æ–∑–¥–∞–µ–º entity_key
                    entity_key = _slugify_implant_kind(name)
                    print(f"    üìã –°–µ–∫—Ü–∏—è: '{name}' ‚Üí '{entity_key}' (–∞–ª–∏–∞—Å—ã: {aliases})")
                    
                    # –°–æ—Ö—Ä–∞–Ω—è–µ–º —á–∞–Ω–∫
                    ENTITY_CHUNKS[("implants", entity_key)] = ch
                    
                    # –ò–Ω–¥–µ–∫—Å–∏—Ä—É–µ–º –∞–ª–∏–∞—Å—ã
                    for a in aliases:
                        ENTITY_INDEX[_norm(a)] = {
                            "topic": "implants", 
                            "entity": entity_key, 
                            "doc_id": metadata.id or file.name, 
                            "section": name
                        }
            
            # –ü—Ä–∏–≤—è–∑—ã–≤–∞–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –∫ –∫–∞–∂–¥–æ–º—É —á–∞–Ω–∫—É
            for chunk in file_chunks:
                chunk.metadata = metadata
                
                # –û–±–Ω–æ–≤–ª—è–µ–º ENTITY_INDEX –¥–ª—è –≤—Å–µ—Ö —á–∞–Ω–∫–æ–≤
                if metadata.topic:
                    update_entity_index(chunk, metadata.topic, chunk.id)
                
                # –ø–µ—Ä–µ–¥ append(chunk) - –Ω–æ—Ä–º–∞–ª–∏–∑—É–π —Ç–µ–≥–∏ –≤ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö
                try:
                    if hasattr(chunk.metadata, "tags") and isinstance(chunk.metadata.tags, list):
                        chunk.metadata.tags_lower = [str(t).strip().lower() for t in chunk.metadata.tags]
                    else:
                        chunk.metadata.tags_lower = []
                except Exception:
                    chunk.metadata.tags_lower = []
                
                all_chunks.append(chunk)
                
                # –†–µ–≥–∏—Å—Ç—Ä–∏—Ä—É–µ–º –≤—Å–µ —á–∞–Ω–∫–∏ –≤ ENTITY_CHUNKS (–Ω–µ —Ç–æ–ª—å–∫–æ –∏–º–ø–ª–∞–Ω—Ç—ã)
                ENTITY_CHUNKS[(chunk.metadata.topic or "general", chunk.id)] = chunk
                
                # –ï—Å–ª–∏ —ç—Ç–æ –∫–∞—Ä—Ç–æ—á–∫–∞ –≤—Ä–∞—á–∞ (–ø–æ–¥–∑–∞–≥–æ–ª–æ–≤–æ–∫ ## –∏–ª–∏ ### –ò–º—è –§–∞–º–∏–ª–∏—è [–û—Ç—á–µ—Å—Ç–≤–æ]) ‚Äî –∑–∞–ø–æ–º–∏–Ω–∞–µ–º –ø—Ä—è–º—É—é —Å—Å—ã–ª–∫—É
                hdr = re.search(r'(?m)^#{2,3}\s+([–ê-–Ø–Å][–∞-—è—ë]+(?:\s+[–ê-–Ø–Å][–∞-—è—ë]+){1,2})\s*$', chunk.text)
                if hdr:
                    full = hdr.group(1).strip()
                    parts = full.split()
                    last = parts[0] # –§–∞–º–∏–ª–∏—è
                    first = parts[1] if len(parts) > 1 else ""
                    patr = parts[2] if len(parts) > 2 else ""
                    
                    # –ö–ª—é—á–∏, –ø–æ –∫–æ—Ç–æ—Ä—ã–º —Ä–µ–∞–ª—å–Ω–æ —Å–ø—Ä–∞—à–∏–≤–∞—é—Ç
                    keys = {
                        full,                               # "–ú–æ–∏—Å–µ–µ–≤ –ö–∏—Ä–∏–ª–ª –ù–∏–∫–æ–ª–∞–µ–≤–∏—á"
                        last,                               # "–ú–æ–∏—Å–µ–µ–≤"
                        (f"{last} {first}").strip(),        # "–ú–æ–∏—Å–µ–µ–≤ –ö–∏—Ä–∏–ª–ª"
                        (f"{first} {last}").strip(),        # "–ö–∏—Ä–∏–ª–ª –ú–æ–∏—Å–µ–µ–≤"
                    }
                    
                    for k in keys:
                        DOCTOR_NAME_TO_CHUNK[k.lower()] = chunk
                    
                    # –î–ª—è —Ä–µ–≥—ç–∫—Å–ø–∞/–ø–æ–¥—Å–≤–µ—Ç–∫–∏ - –±–µ–∑ –æ—Ç—á–µ—Å—Ç–≤, —á—Ç–æ–±—ã –Ω–µ –∑–∞—Å–æ—Ä—è—Ç—å
                    DOCTOR_NAME_TOKENS.update({full, last, f"{first} {last}".strip(), f"{last} {first}".strip()})
                    print(f"üîó –ö–∞—Ä—Ç–æ—á–∫–∞ –≤—Ä–∞—á–∞: {full} ‚Üí {getattr(chunk, 'section', chunk.file_name)}; –∫–ª—é—á–∏: {sorted(keys)}")
            
            print(f"  ‚úÖ –§–∞–π–ª –æ–±—Ä–∞–±–æ—Ç–∞–Ω —É—Å–ø–µ—à–Ω–æ")
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ —Ñ–∞–π–ª–∞ {file}: {e}")
            import traceback
            traceback.print_exc()
            continue
    
    print(f"\u23f3 –ù–∞–π–¥–µ–Ω–æ {len(all_chunks)} —á–∞–Ω–∫–æ–≤")
    
    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º ALL_CHUNKS –¥–ª—è –Ω–æ–≤—ã—Ö –∏–Ω–¥–µ–∫—Å–æ–≤
    ALL_CHUNKS.extend(all_chunks)
    print(f"‚úÖ ALL_CHUNKS –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω: {len(ALL_CHUNKS)} —á–∞–Ω–∫–æ–≤")
    print(f"‚úÖ ALIAS_MAP_GLOBAL: {len(ALIAS_MAP_GLOBAL)} –∞–ª–∏–∞—Å–æ–≤")
    print(f"‚úÖ H2_INDEX: {len(H2_INDEX)} –∑–∞–≥–æ–ª–æ–≤–∫–æ–≤")
    print(f"‚úÖ FILE_META: {len(FILE_META)} —Ñ–∞–π–ª–æ–≤")
    
    # –°–æ–∑–¥–∞–µ–º BM25 –∏–Ω–¥–µ–∫—Å
    if all_chunks:
        print(f"üîç –°–æ–∑–¥–∞–µ–º BM25 –∏–Ω–¥–µ–∫—Å –¥–ª—è {len(all_chunks)} —á–∞–Ω–∫–æ–≤...")
        bm25_corpus = []
        for chunk in all_chunks:
            # –¢–æ–∫–µ–Ω–∏–∑–∏—Ä—É–µ–º —Ç–µ–∫—Å—Ç –¥–ª—è BM25
            tokens = re.findall(r'\w+', chunk.text.lower())
            bm25_corpus.append(tokens)
        
        bm25_index = BM25Okapi(bm25_corpus)
        print(f"‚úÖ BM25 –∏–Ω–¥–µ–∫—Å —Å–æ–∑–¥–∞–Ω")
    
    # –û—Ç–ª–∞–¥–æ—á–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ —á–∞–Ω–∫–∞—Ö
    for chunk in all_chunks[:5]:  # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –ø–µ—Ä–≤—ã–µ 5 —á–∞–Ω–∫–æ–≤
        print(f"  üìÑ {chunk.file_name}: {chunk.text[:80]}...")
    
    # –ü–µ—Ä–µ—Å–æ–±–∏—Ä–∞–µ–º regex –¥–ª—è –≤—Ä–∞—á–µ–π
    _rebuild_doctor_regex()
    print(f"–í—Ä–∞—á–∏: –°–æ–±—Ä–∞–Ω—ã –∏–º–µ–Ω–∞ –≤—Ä–∞—á–µ–π: {len(DOCTOR_NAME_TOKENS)} -> {sorted(list(DOCTOR_NAME_TOKENS))[:6]} ...")
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ –µ—Å—Ç—å —á–∞–Ω–∫–∏ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏
    if len(all_chunks) == 0:
        print("‚ö†Ô∏è –ü—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏–µ: –ù–µ –Ω–∞–π–¥–µ–Ω–æ –Ω–∏ –æ–¥–Ω–æ–≥–æ —á–∞–Ω–∫–∞ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏")
        # –°–æ–∑–¥–∞–µ–º –ø—É—Å—Ç–æ–π –∏–Ω–¥–µ–∫—Å
        dimension = 1536  # —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å text-embedding-3-small
        index = faiss.IndexFlatL2(dimension)
    else:
        # –ü–æ–ª—É—á–∞–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ –¥–ª—è –≤—Å–µ—Ö —Ñ—Ä–∞–≥–º–µ–Ω—Ç–æ–≤
        print(f"\u23f3 –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –¥–ª—è {len(all_chunks)} —á–∞–Ω–∫–æ–≤...")
        
        def get_embedding(text: str) -> List[float]:
            resp = openai_client.embeddings.create(
                model="text-embedding-3-small",
                input=text,
                encoding_format="float"
            )
            return resp.data[0].embedding
        
        # –î–µ–ª–∞–µ–º —Ñ—É–Ω–∫—Ü–∏—é –≥–ª–æ–±–∞–ª—å–Ω–æ –¥–æ—Å—Ç—É–ø–Ω–æ–π
        globals()['get_embedding'] = get_embedding
        
        # –°–æ–∑–¥–∞–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ —Ç–æ–ª—å–∫–æ –¥–ª—è —Ç–µ–∫—Å—Ç–∞ —á–∞–Ω–∫–æ–≤
        chunk_texts = [chunk.text for chunk in all_chunks]
        embeddings = [get_embedding(text) for text in chunk_texts]
        
        dimension = len(embeddings[0])
        index = faiss.IndexFlatL2(dimension)
        index.add(np.array(embeddings).astype("float32"))
        
        print(f"‚úÖ –ò–Ω–¥–µ–∫—Å —Å–æ–∑–¥–∞–Ω —Å {len(all_chunks)} —á–∞–Ω–∫–∞–º–∏")

except Exception as e:
    print(f"‚ùå –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞ –ø—Ä–∏ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏: {e}")
    import traceback
    traceback.print_exc()
    print("‚ö†Ô∏è Embeddings –Ω–µ–¥–æ—Å—Ç—É–ø–Ω—ã, —Ä–∞–±–æ—Ç–∞–µ–º —Ç–æ–ª—å–∫–æ –Ω–∞ BM25 –∏ –ø—Ä–∞–≤–∏–ª–∞—Ö")
    index = None
    # –Ω–µ—Ç FAISS, –Ω–æ —á–∞–Ω–∫–∏ –æ—Å—Ç–∞–≤–ª—è–µ–º!

# Fallback —Ñ—É–Ω–∫—Ü–∏—è get_embedding
def get_embedding(text: str) -> List[float]:
    """Fallback —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤"""
    try:
        resp = openai_client.embeddings.create(
            model="text-embedding-3-small",
            input=text,
            encoding_format="float"
        )
        return resp.data[0].embedding
    except Exception as e:
        print(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–æ–∑–¥–∞–Ω–∏–∏ —ç–º–±–µ–¥–¥–∏–Ω–≥–∞: {e}")
        # –í–æ–∑–≤—Ä–∞—â–∞–µ–º –Ω—É–ª–µ–≤–æ–π –≤–µ–∫—Ç–æ—Ä
        return [0.0] * 1536

def generate_query_variants(query: str) -> List[str]:
    """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç 2-3 –≤–∞—Ä–∏–∞–Ω—Ç–∞ –ø–µ—Ä–µ—Ñ—Ä–∞–∑–∏—Ä–æ–≤–∫–∏ –∑–∞–ø—Ä–æ—Å–∞ –¥–ª—è –ª—É—á—à–µ–≥–æ –ø–æ–∏—Å–∫–∞"""
    if not openai_client:
        return [query]  # Fallback –∫ –∏—Å—Ö–æ–¥–Ω–æ–º—É –∑–∞–ø—Ä–æ—Å—É
    
    prompt = f"""–ü–µ—Ä–µ—Ñ—Ä–∞–∑–∏—Ä—É–π –≤–æ–ø—Ä–æ—Å –ø–∞—Ü–∏–µ–Ω—Ç–∞ –æ —Å—Ç–æ–º–∞—Ç–æ–ª–æ–≥–∏–∏ –≤ 2-3 —Ä–∞–∑–Ω—ã—Ö –≤–∞—Ä–∏–∞–Ω—Ç–∞ –¥–ª—è –ø–æ–∏—Å–∫–∞ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏.

–ò—Å—Ö–æ–¥–Ω—ã–π –≤–æ–ø—Ä–æ—Å: "{query}"

–°–æ–∑–¥–∞–π –≤–∞—Ä–∏–∞–Ω—Ç—ã:
1. –ë–æ–ª–µ–µ —Ñ–æ—Ä–º–∞–ª—å–Ω—ã–π/–º–µ–¥–∏—Ü–∏–Ω—Å–∫–∏–π
2. –ë–æ–ª–µ–µ –ø—Ä–æ—Å—Ç–æ–π/–±—ã—Ç–æ–≤–æ–π  
3. –° —Å–∏–Ω–æ–Ω–∏–º–∞–º–∏ –∏ –∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω—ã–º–∏ —Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–∫–∞–º–∏

–í–µ—Ä–Ω–∏ –¢–û–õ–¨–ö–û JSON:
{{"variants": ["–≤–∞—Ä–∏–∞–Ω—Ç 1", "–≤–∞—Ä–∏–∞–Ω—Ç 2", "–≤–∞—Ä–∏–∞–Ω—Ç 3"]}}

–ü—Ä–∏–º–µ—Ä—ã:
- "–±–æ–ª—å–Ω–æ –ª–∏ —Å—Ç–∞–≤–∏—Ç—å –∏–º–ø–ª–∞–Ω—Ç" ‚Üí ["–±–æ–ª–µ–∑–Ω–µ–Ω–Ω–æ—Å—Ç—å –∏–º–ø–ª–∞–Ω—Ç–∞—Ü–∏–∏", "–¥–∏—Å–∫–æ–º—Ñ–æ—Ä—Ç –ø—Ä–∏ —É—Å—Ç–∞–Ω–æ–≤–∫–µ –∏–º–ø–ª–∞–Ω—Ç–∞", "–æ—â—É—â–µ–Ω–∏—è –≤–æ –≤—Ä–µ–º—è –∏–º–ø–ª–∞–Ω—Ç–∞—Ü–∏–∏"]
- "—Å–∫–æ–ª—å–∫–æ —Å—Ç–æ–∏—Ç –∏–º–ø–ª–∞–Ω—Ç–∞—Ü–∏—è" ‚Üí ["—Å—Ç–æ–∏–º–æ—Å—Ç—å –∏–º–ø–ª–∞–Ω—Ç–∞—Ü–∏–∏ –∑—É–±–æ–≤", "—Ü–µ–Ω–∞ –Ω–∞ –∏–º–ø–ª–∞–Ω—Ç—ã", "—Ä–∞—Å—Ü–µ–Ω–∫–∏ –Ω–∞ –∏–º–ø–ª–∞–Ω—Ç–∞—Ü–∏—é"]"""
    
    try:
        response = openai_client.chat.completions.create(
            model="gpt-4o-mini",
            messages=[{"role": "user", "content": prompt}],
            max_tokens=150,
            temperature=0.3
        )
        
        result = json.loads(response.choices[0].message.content)
        variants = result.get("variants", [])
        
        # –î–æ–±–∞–≤–ª—è–µ–º –∏—Å—Ö–æ–¥–Ω—ã–π –∑–∞–ø—Ä–æ—Å –≤ –Ω–∞—á–∞–ª–æ
        all_variants = [query] + variants
        # –£–±–∏—Ä–∞–µ–º –¥—É–±–ª–∏–∫–∞—Ç—ã, —Å–æ—Ö—Ä–∞–Ω—è—è –ø–æ—Ä—è–¥–æ–∫
        unique_variants = []
        seen = set()
        for variant in all_variants:
            if variant.lower() not in seen:
                unique_variants.append(variant)
                seen.add(variant.lower())
        
        print(f"üîç Multi-query: —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–æ {len(unique_variants)} –≤–∞—Ä–∏–∞–Ω—Ç–æ–≤ –∑–∞–ø—Ä–æ—Å–∞")
        return unique_variants[:3]  # –ú–∞–∫—Å–∏–º—É–º 3 –≤–∞—Ä–∏–∞–Ω—Ç–∞
        
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –≤–∞—Ä–∏–∞–Ω—Ç–æ–≤ –∑–∞–ø—Ä–æ—Å–∞: {e}")
        return [query]  # Fallback –∫ –∏—Å—Ö–æ–¥–Ω–æ–º—É –∑–∞–ø—Ä–æ—Å—É

def hybrid_retriever(query: str, top_n: int = 20) -> List[Tuple[RetrievedChunk, float]]:
    """–ì–∏–±—Ä–∏–¥–Ω—ã–π —Ä–µ—Ç—Ä–∏–≤–µ—Ä: –æ–±—ä–µ–¥–∏–Ω—è–µ—Ç BM25 –∏ —ç–º–±–µ–¥–¥–∏–Ω–≥–∏"""
    if not all_chunks or len(all_chunks) == 0:
        return []
    
    candidates = []
    
    # ==== BM25 –ø–æ–∏—Å–∫ ====
    if bm25_index:
        query_tokens = re.findall(r'\w+', query.lower())
        bm25_scores = bm25_index.get_scores(query_tokens)
        bm25_candidates = []
        
        for i, score in enumerate(bm25_scores):
            if score > 0:
                bm25_candidates.append((all_chunks[i], score))
        
        # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º BM25 scores
        if bm25_candidates:
            max_score = max(score for _, score in bm25_candidates)
            if max_score > 0:
                bm25_candidates = [(chunk, score / max_score * 0.6) for chunk, score in bm25_candidates]
                candidates.extend(bm25_candidates[:top_n])
    
    # ==== –≠–º–±–µ–¥–¥–∏–Ω–≥–∏ –ø–æ–∏—Å–∫ ====
    try:
        query_embedding = get_embedding(query)
        D, I = index.search(np.array([query_embedding]).astype("float32"), k=min(top_n, len(all_chunks)))
        
        # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º embedding scores
        max_dist = max(D[0]) if len(D[0]) > 0 else 1.0
        if max_dist > 0:
            embedding_candidates = []
            for i, dist in zip(I[0], D[0]):
                score = (1.0 - dist / max_dist) * 0.4  # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º –∏ –ø—Ä–∏–º–µ–Ω—è–µ–º –≤–µ—Å
                embedding_candidates.append((all_chunks[i], score))
            candidates.extend(embedding_candidates)
    except Exception as e:
        print(f"–û—à–∏–±–∫–∞ –≤ embedding –ø–æ–∏—Å–∫–µ: {e}")
    
    # ==== –û–±—ä–µ–¥–∏–Ω—è–µ–º –∏ —É–±–∏—Ä–∞–µ–º –¥—É–±–ª–∏ ====
    seen_chunks = set()
    unique_candidates = []
    
    for chunk, score in candidates:
        if chunk.id not in seen_chunks:
            seen_chunks.add(chunk.id)
            unique_candidates.append((chunk, score))
    
    # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ score –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ–º top_n
    unique_candidates.sort(key=lambda x: x[1], reverse=True)
    return unique_candidates[:top_n]

def llm_rerank(candidates: List[Tuple[RetrievedChunk, float]], query: str) -> List[Tuple[RetrievedChunk, float]]:
    """LLM-—Ä–µ—Ä–∞–Ω–∫–∏–Ω–≥ –¥–ª—è —Ç–æ—á–Ω–æ–π –æ—Ü–µ–Ω–∫–∏ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏ —á–∞–Ω–∫–æ–≤"""
    if not candidates or not openai_client:
        return candidates
    
    # –ë–µ—Ä–µ–º —Ç–æ–ª—å–∫–æ top-6 –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤ –¥–ª—è —Ä–µ—Ä–∞–Ω–∫–∏–Ω–≥–∞
    top_candidates = candidates[:6]
    
    # –§–æ—Ä–º–∏—Ä—É–µ–º –ø—Ä–æ–º–ø—Ç –¥–ª—è LLM
    chunks_text = ""
    for i, (chunk, score) in enumerate(top_candidates):
        # –ë–µ—Ä–µ–º —Ç–æ–ª—å–∫–æ –∑–∞–≥–æ–ª–æ–≤–æ–∫ –∏ –ø–µ—Ä–≤—ã–µ 200 —Å–∏–º–≤–æ–ª–æ–≤ –¥–ª—è —ç–∫–æ–Ω–æ–º–∏–∏ —Ç–æ–∫–µ–Ω–æ–≤
        header_match = re.search(r'(?m)^##\s+(.+?)\s*$', chunk.text)
        header = header_match.group(1) if header_match else "–ë–µ–∑ –∑–∞–≥–æ–ª–æ–≤–∫–∞"
        preview = chunk.text[:200] + "..." if len(chunk.text) > 200 else chunk.text
        chunks_text += f"{i+1}. –ó–∞–≥–æ–ª–æ–≤–æ–∫: {header}\n–¢–µ–∫—Å—Ç: {preview}\n\n"
    
    prompt = f"""–û—Ü–µ–Ω–∏ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å –∫–∞–∂–¥–æ–≥–æ —Ñ—Ä–∞–≥–º–µ–Ω—Ç–∞ –¥–ª—è –æ—Ç–≤–µ—Ç–∞ –Ω–∞ –≤–æ–ø—Ä–æ—Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è.

–í–æ–ø—Ä–æ—Å: "{query}"

–§—Ä–∞–≥–º–µ–Ω—Ç—ã:
{chunks_text}

–í–µ—Ä–Ω–∏ –¢–û–õ–¨–ö–û JSON —Å –æ—Ü–µ–Ω–∫–∞–º–∏ –æ—Ç 0.0 –¥–æ 1.0:
{{"scores": [0.8, 0.3, 0.9, 0.1, 0.7, 0.2]}}

–ì–¥–µ 1.0 = –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ, 0.0 = –Ω–µ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ."""
    
    try:
        response = openai_client.chat.completions.create(
            model="gpt-4o-mini",
            messages=[{"role": "user", "content": prompt}],
            max_tokens=100,
            temperature=0.1
        )
        
        result = json.loads(response.choices[0].message.content)
        llm_scores = result.get("scores", [])
        
        # –ü—Ä–∏–º–µ–Ω—è–µ–º LLM-–æ—Ü–µ–Ω–∫–∏ –∫ –∫–∞–Ω–¥–∏–¥–∞—Ç–∞–º
        reranked = []
        for i, (chunk, base_score) in enumerate(top_candidates):
            if i < len(llm_scores):
                # –ö–æ–º–±–∏–Ω–∏—Ä—É–µ–º –±–∞–∑–æ–≤—ã–π score —Å LLM-–æ—Ü–µ–Ω–∫–æ–π (70% LLM, 30% –±–∞–∑–æ–≤—ã–π)
                final_score = llm_scores[i] * 0.7 + base_score * 0.3
                reranked.append((chunk, final_score))
            else:
                reranked.append((chunk, base_score))
        
        # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ —Ñ–∏–Ω–∞–ª—å–Ω–æ–º—É score
        reranked.sort(key=lambda x: x[1], reverse=True)
        print(f"üîç LLM-—Ä–µ—Ä–∞–Ω–∫–∏–Ω–≥: {len(reranked)} –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤ –ø–µ—Ä–µ–æ—Ü–µ–Ω–µ–Ω—ã")
        
        return reranked
        
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ LLM-—Ä–µ—Ä–∞–Ω–∫–∏–Ω–≥–∞: {e}")
        return candidates

def reranker(candidates: List[Tuple[RetrievedChunk, float]], query: str, detected_topics: Set[str]) -> List[RetrievedChunk]:
    """–†–µ—Ä–∞–Ω–∫–µ—Ä —Å LLM-–æ—Ü–µ–Ω–∫–æ–π —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏"""
    if not candidates:
        return []
    
    # –°–Ω–∞—á–∞–ª–∞ –ø—Ä–∏–º–µ–Ω—è–µ–º LLM-—Ä–µ—Ä–∞–Ω–∫–∏–Ω–≥
    llm_reranked = llm_rerank(candidates, query)
    
    # –ó–∞—Ç–µ–º –ø—Ä–∏–º–µ–Ω—è–µ–º —ç–≤—Ä–∏—Å—Ç–∏—á–µ—Å–∫–∏–µ –±–æ–Ω—É—Å—ã
    scored_candidates = []
    query_lower = query.lower()
    
    for chunk, base_score in llm_reranked:
        final_score = base_score
        
        # +0.2 –µ—Å–ª–∏ —Ç–µ–º–∞ –æ—Ç router —Å–æ–≤–ø–∞–ª–∞ —Å —Ç–µ–º–æ–π —á–∞–Ω–∫–∞
        if detected_topics and chunk.metadata.topic:
            if chunk.metadata.topic in detected_topics:
                final_score += 0.2
        
        # +0.1 –µ—Å–ª–∏ –Ω–∞–π–¥–µ–Ω alias —á–µ—Ä–µ–∑ ENTITY_INDEX
        for alias, meta in ENTITY_INDEX.items():
            if alias in query_lower:
                if meta["doc_id"] == chunk.file_name:
                    final_score += 0.1
                    break
        
        scored_candidates.append((chunk, final_score))
    
    # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ —Ñ–∏–Ω–∞–ª—å–Ω–æ–º—É score
    scored_candidates.sort(key=lambda x: x[1], reverse=True)
    
    # –í–æ–∑–≤—Ä–∞—â–∞–µ–º 2-3 –ª—É—á—à–∏—Ö —á–∞–Ω–∫–∞
    return [chunk for chunk, _ in scored_candidates[:3]]



# === 6) –†–∞–Ω–Ω–∏–π –¥–µ—Ç–µ–∫—Ç–æ—Ä H2 ===
def detect_section_early(user_q: str):
    q = norm_text(user_q)
    hit = H2_INDEX.get(q)
    if not hit:
        # –º—è–≥–∫–æ–µ –≤—Ö–æ–∂–¥–µ–Ω–∏–µ (–º–∏–Ω–∏–º—É–º 3 —Å–∏–º–≤–æ–ª–∞ –¥–ª—è –∏–∑–±–µ–∂–∞–Ω–∏—è –ª–æ–∂–Ω—ã—Ö —Å—Ä–∞–±–∞—Ç—ã–≤–∞–Ω–∏–π)
        for k, v in H2_INDEX.items():
            if k and len(k) > 2 and k in q:
                hit = v; break
    if not hit: return None, {}
    ch = _find_chunk(hit["file"], hit["h2_id"])
    if not ch: return None, {}
    return ch, {"source":"alias","exact_h2_match":True,"topic":hit["topic"]}

# === 7) –ì–ª–∞–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è —Ä–µ—Ç—Ä–∏–≤–∞ ===
def retrieve_relevant_chunks_new(user_q: str, theme_hint: str|None, candidates_func):
    """–ù–æ–≤–∞—è —Ñ—É–Ω–∫—Ü–∏—è —Ä–µ—Ç—Ä–∏–≤–∞ —Å –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–Ω—ã–º –ø–æ–∏—Å–∫–æ–º"""
    # 1) —Ç–æ—á–Ω—ã–π H2
    ch, flags = detect_section_early(user_q)
    if ch: return [ch], flags

    # 2) –≥–ª–æ–±–∞–ª—å–Ω—ã–π –∞–ª–∏–∞—Å –∏–∑ —à–∞–ø–æ–∫ ‚Üí –¥–µ—Ñ–æ–ª—Ç –ø–æ —Ç–µ–º–µ
    a = ALIAS_MAP_GLOBAL.get(norm_text(user_q))
    if not a:
        for k, v in ALIAS_MAP_GLOBAL.items():
            if k in norm_text(user_q): a = v; break
    if a:
        ch, flags = get_default_chunk_for_topic(a["topic"])
        if ch: return [ch], {"source":"alias","exact_h2_match":True,"topic":a["topic"]}

    # 3) —Ç–µ–º–∞ –∏–∑ —Ä–æ—É—Ç–µ—Ä–∞ ‚Üí –¥–µ—Ñ–æ–ª—Ç —Ç–µ–º—ã
    if theme_hint in CANON:
        ch, flags = get_default_chunk_for_topic(theme_hint)
        if ch: return [ch], flags

    # 4) –æ–±—ã—á–Ω—ã–π –ø–æ–∏—Å–∫ (–≤–∞—à–∞ —Å—É—â–µ—Å—Ç–≤—É—é—â–∞—è —Ñ—É–Ω–∫—Ü–∏—è –≤—ã–¥–∞—ë—Ç –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤)
    cands = candidates_func(user_q)
    return cands, {"source":"search","exact_h2_match":False}

def retrieve_relevant_chunks(query: str, top_k: int = 8) -> List[RetrievedChunk]:
    """–ò–∑–≤–ª–µ–∫–∞–µ—Ç —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–µ —á–∞–Ω–∫–∏ —Å multi-query rewrite –∏ —É–ª—É—á—à–µ–Ω–Ω—ã–º —Ä–∞–Ω–∂–∏—Ä–æ–≤–∞–Ω–∏–µ–º"""
    if len(all_chunks) == 0:
        print("‚ö†Ô∏è –ù–µ—Ç —á–∞–Ω–∫–æ–≤ –¥–ª—è –ø–æ–∏—Å–∫–∞")
        return []
    
    # ==== –†–û–£–¢–ï–† –ü–û –¢–ï–ú–ê–ú ====
    detected_topics = route_topics(query)
    print(f"üéØ –†–æ—É—Ç–µ—Ä –æ–ø—Ä–µ–¥–µ–ª–∏–ª —Ç–µ–º—ã: {detected_topics}")
    
    # –ü—Ä—è–º–æ–π alias-fallback –ø–æ –∫–∞—Ä—Ç–µ frontmatter
    q = _norm(query or "")
    hit_map = ALIAS_MAP.get(q)
    if not hit_map:
        # –¥–æ–ø—É—Å–∫–∞–µ–º "alias ‚äÜ query" (–Ω–∞–ø—Ä–∏–º–µ—Ä, –∑–∞–ø—Ä–æ—Å –¥–ª–∏–Ω–Ω–µ–µ)
        for a, meta in ALIAS_MAP.items():
            if a and a in q:
                hit_map = meta
                break

    if hit_map:
        # –Ω–∞—Ö–æ–¥–∏–º –ø–µ—Ä–≤—ã–π —á–∞–Ω–∫ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–µ–≥–æ —Ñ–∞–π–ª–∞ –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –µ–≥–æ
        target = Path(hit_map["file"]).name
        for ch in all_chunks:
            if ch.file_name == target:
                print(f"‚úÖ Alias-fallback: {q} -> {hit_map['file']}")
                return [ch]
    
    # ==== –ë–´–°–¢–†–´–ô –ü–£–¢–¨: –î–ï–¢–ï–ö–¢ –°–£–©–ù–û–°–¢–ò ====
    q = _norm(query or "")
    hit = None
    
    # –ü—Ä—è–º–æ–µ –≤—Ö–æ–∂–¥–µ–Ω–∏–µ –∞–ª–∏–∞—Å–∞
    for alias, meta in ENTITY_INDEX.items():
        if alias in q:
            hit = ENTITY_CHUNKS.get((meta["topic"], meta["entity"]))
            if not hit:
                hit = next((ch for ch in all_chunks if ch.id == meta["entity"]), None) \
                    or next((ch for ch in all_chunks if ch.file_name == meta["doc_id"]), None)
            if hit:
                break
    
    # –ú—è–≥–∫–∏–π –º–∞—Ç—á (—Ä–∞–∑–¥–µ–ª–∏—Ç–µ–ª–∏ -/-/- –∏ –ø—Ä–æ–±–µ–ª—ã)
    if not hit:
        q2 = re.sub(r'[---]', '', q)
        for alias, meta in ENTITY_INDEX.items():
            a2 = re.sub(r'[---]', '', alias)
            if a2 in q2:
                hit = ENTITY_CHUNKS.get((meta["topic"], meta["entity"]))
                if not hit:
                    hit = next((ch for ch in all_chunks if ch.id == meta["entity"]), None) \
                        or next((ch for ch in all_chunks if ch.file_name == meta["doc_id"]), None)
                if hit:
                    break
    
    if hit:
        print(f"‚úÖ –ù–∞–π–¥–µ–Ω–∞ —Å—É—â–Ω–æ—Å—Ç—å –∫–∞—Ç–∞–ª–æ–≥–∞: '{query}' ‚Üí {hit.id}")
        return [hit]  # —Ä–æ–≤–Ω–æ –Ω—É–∂–Ω–∞—è —Å–µ–∫—Ü–∏—è –∫–∞—Ç–∞–ª–æ–≥–∞
    
    # ==== –°–ü–ï–¶–ò–ê–õ–¨–ù–´–ï –ó–ê–ü–†–û–°–´: –õ–ò–°–¢–ò–ù–ì –ò –°–†–ê–í–ù–ï–ù–ò–ï ====
    # –õ–∏—Å—Ç–∏–Ω–≥ ("–∫–∞–∫–∏–µ –≤–∏–¥—ã")
    if re.search(r'(–∫–∞–∫–∏–µ|–∫–∞–∫–æ–π|—á—Ç–æ –∑–∞).* (–≤–∏–¥|–≤–∞—Ä–∏–∞–Ω—Ç).* (–∏–º–ø–ª–∞–Ω—Ç–∞—Ü|–∏–º–ø–ª–∞–Ω—Ç)', q):
        kinds_order = ["single-stage", "classic", "all-on-4", "all-on-6"]
        out = [ENTITY_CHUNKS[("implants", k)] for k in kinds_order if ("implants", k) in ENTITY_CHUNKS]
        print(f"üìã –õ–∏—Å—Ç–∏–Ω–≥ –≤–∏–¥–æ–≤ –∏–º–ø–ª–∞–Ω—Ç–∞—Ü–∏–∏: –Ω–∞–π–¥–µ–Ω–æ {len(out)} —Ç–∏–ø–æ–≤")
        return out[:4]
    
    # –°—Ä–∞–≤–Ω–µ–Ω–∏–µ ("—á–µ–º –æ—Ç–ª–∏—á–∞–µ—Ç—Å—è / —á—Ç–æ –ª—É—á—à–µ")
    if re.search(r'(—á–µ–º\s+–æ—Ç–ª–∏—á|—Ä–∞–∑–ª–∏—á|—Ä–∞–∑–Ω–∏—Ü–∞|—Å—Ä–∞–≤–Ω|—á—Ç–æ\s+–ª—É—á—à–µ|vs)', q):
        kinds_order = ["single-stage", "classic", "all-on-4", "all-on-6"]
        out = [ENTITY_CHUNKS[("implants", k)] for k in kinds_order if ("implants", k) in ENTITY_CHUNKS]
        print(f"üìä –°—Ä–∞–≤–Ω–µ–Ω–∏–µ –≤–∏–¥–æ–≤ –∏–º–ø–ª–∞–Ω—Ç–∞—Ü–∏–∏: –Ω–∞–π–¥–µ–Ω–æ {len(out)} —Ç–∏–ø–æ–≤")
        return out[:4]
    
    # 0) –µ—Å–ª–∏ –Ω–∞—à–ª–∏ –≤—Ä–∞—á–∞ –Ω–∞–ø—Ä—è–º—É—é ‚Äî —Å—Ä–∞–∑—É –æ—Ç–¥–∞—ë–º –∫–∞—Ä—Ç–æ—á–∫—É, –º–∏–Ω—É—è FAISS
    hit = _find_doctor_direct_or_fuzzy(query)
    if hit:
        print(f"‚úÖ –ö–∞—Ä—Ç–æ—á–∫–∞ –≤—Ä–∞—á–∞ –ø–æ –∑–∞–ø—Ä–æ—Å—É: {query} ‚Üí {getattr(hit, 'section', hit.file_name)}")
        return [hit] # –±–µ–∑ –¥–µ–¥—É–ø–∞
    
    try:
        print(f"üîç –ü–æ–∏—Å–∫: '{query}' –≤ {len(all_chunks)} —á–∞–Ω–∫–∞—Ö")
        
        # ==== MULTI-QUERY REWRITE ====
        query_variants = generate_query_variants(query)
        print(f"üîç Multi-query: –∏—Å–ø–æ–ª—å–∑—É–µ–º {len(query_variants)} –≤–∞—Ä–∏–∞–Ω—Ç–æ–≤ –∑–∞–ø—Ä–æ—Å–∞")
        
        # ==== –ì–ò–ë–†–ò–î–ù–´–ô –†–ï–¢–†–ò–í–ï–† –î–õ–Ø –ö–ê–ñ–î–û–ì–û –í–ê–†–ò–ê–ù–¢–ê ====
        all_candidates = []
        for variant in query_variants:
            candidates = hybrid_retriever(variant, top_n=15)  # –ú–µ–Ω—å—à–µ –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤ –Ω–∞ –≤–∞—Ä–∏–∞–Ω—Ç
            all_candidates.extend(candidates)
            print(f"üîç –í–∞—Ä–∏–∞–Ω—Ç '{variant[:30]}...': –Ω–∞–π–¥–µ–Ω–æ {len(candidates)} –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤")
        
        # ==== –û–ë–™–ï–î–ò–ù–ï–ù–ò–ï –ò –î–ï–î–£–ü–õ–ò–ö–ê–¶–ò–Ø ====
        seen_chunks = set()
        unique_candidates = []
        
        for chunk, score in all_candidates:
            if chunk.id not in seen_chunks:
                seen_chunks.add(chunk.id)
                unique_candidates.append((chunk, score))
        
        # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ score –∏ –±–µ—Ä–µ–º top
        unique_candidates.sort(key=lambda x: x[1], reverse=True)
        top_candidates = unique_candidates[:25]  # –ë–æ–ª—å—à–µ –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤ –¥–ª—è —Ä–µ—Ä–∞–Ω–∫–∏–Ω–≥–∞
        
        print(f"üîç Multi-query: –æ–±—ä–µ–¥–∏–Ω–µ–Ω–æ {len(unique_candidates)} —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤")
        
        # ==== –†–ï–†–ê–ù–ö–ï–† ====
        final_chunks = reranker(top_candidates, query, detected_topics)
        print(f"üîç –†–µ—Ä–∞–Ω–∫–µ—Ä –æ—Ç–æ–±—Ä–∞–ª {len(final_chunks)} —Ñ–∏–Ω–∞–ª—å–Ω—ã—Ö —á–∞–Ω–∫–æ–≤")
        
        # –µ—Å–ª–∏ –Ω–∏—á–µ–≥–æ –≤–Ω—è—Ç–Ω–æ–≥–æ –Ω–µ –ø–æ–ø–∞–ª–æ –∏ —Ç–µ–º–∞ –∏–∑–≤–µ—Å—Ç–Ω–∞ ‚Äî –∂—ë—Å—Ç–∫–∏–π fallback
        if not final_chunks and detected_topics:
            theme_key = list(detected_topics)[0]
            final_chunks = fallback_theme_chunks(theme_key, limit=top_k)
        
        # –µ—Å–ª–∏ –∏—â–µ–º –≤—Ä–∞—á–∞ - –Ω–µ —Ä–µ–∂–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ file_name
        if DOCTOR_NAME_TO_CHUNK and _find_doctor_direct_or_fuzzy(query):
            return final_chunks[:top_k] # –±–µ–∑ –¥–µ–¥—É–ø–∞
        
        # –∏–Ω–∞—á–µ –æ—Å—Ç–∞–≤–ª—è–µ–º –∑–∞—â–∏—Ç–Ω—ã–π –¥–µ–¥—É–ø –ø–æ —Ñ–∞–π–ª–∞–º
        seen = set()
        out = []
        for chunk in final_chunks:
            if chunk.file_name not in seen:
                seen.add(chunk.file_name)
                out.append(chunk)
        
        print(f"‚úÖ –í–æ–∑–≤—Ä–∞—â–∞–µ–º {len(out)} —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —á–∞–Ω–∫–æ–≤")
        return out[:top_k]
        
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–∏—Å–∫–µ —á–∞–Ω–∫–æ–≤: {e}")
        return []

def synthesize_answer(chunks: List[RetrievedChunk], user_query: str, allow_cta: bool) -> SynthJSON:
    """–°–∏–Ω—Ç–µ–∑–∏—Ä—É–µ—Ç —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π JSON –æ—Ç–≤–µ—Ç"""
    
    # –§–æ—Ä–º–∏—Ä—É–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç –∏–∑ —á–∞–Ω–∫–æ–≤
    context_parts = []
    for chunk in chunks:
        context_parts.append(f"ID: {chunk.id}\n–û–±–Ω–æ–≤–ª–µ–Ω–æ: {chunk.updated}\n–ö—Ä–∏—Ç–∏—á–Ω–æ—Å—Ç—å: {chunk.criticality}\n–ö–æ–Ω—Ç–µ–Ω—Ç:\n{chunk.text}\n")
    
    context = "\n---\n".join(context_parts)
    print(f"üìù –ö–æ–Ω—Ç–µ–∫—Å—Ç –¥–ª—è —Å–∏–Ω—Ç–µ–∑–∞: {len(context)} —Å–∏–º–≤–æ–ª–æ–≤")
    print(f"üìù –ü–µ—Ä–≤—ã–µ 200 —Å–∏–º–≤–æ–ª–æ–≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞: {context[:200]}...")
    
    # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –¥–ª—è –æ—Ç–≤–µ—Ç–∞ (–±–µ—Ä–µ–º –∏–∑ –ø–µ—Ä–≤–æ–≥–æ —á–∞–Ω–∫–∞)
    primary_chunk = chunks[0] if chunks else None
    tone = primary_chunk.metadata.tone if primary_chunk else "friendly"
    preferred_format = primary_chunk.metadata.preferred_format if primary_chunk else ["short", "bullets", "cta"]
    verbatim = primary_chunk.metadata.verbatim if primary_chunk else False
    
    # CTA –¥–ª—è –ø—Ä–æ–º–ø—Ç–∞ (—Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ —Ä–∞–∑—Ä–µ—à–µ–Ω)
    cta_for_prompt = (primary_chunk.metadata.cta_text if (primary_chunk and allow_cta) else "")
    
    # –°–∏—Å—Ç–µ–º–Ω—ã–π –ø—Ä–æ–º–ø—Ç —Å —É—á–µ—Ç–æ–º verbatim
    if verbatim:
        system_prompt = f"""
–¢—ã ‚Äî –¥—Ä—É–∂–µ–ª—é–±–Ω—ã–π –∏ –≤–Ω–∏–º–∞—Ç–µ–ª—å–Ω—ã–π –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç —Å—Ç–æ–º–∞—Ç–æ–ª–æ–≥–∏—á–µ—Å–∫–æ–π –∫–ª–∏–Ω–∏–∫–∏ –¶–≠–°–ò –Ω–∞ –ö–∞–º—á–∞—Ç–∫–µ.
–¢–≤–æ—è –∑–∞–¥–∞—á–∞ ‚Äî –æ—Ç–≤–µ—á–∞—Ç—å –Ω–∞ –≤–æ–ø—Ä–æ—Å—ã –ø–∞—Ü–∏–µ–Ω—Ç–æ–≤ —Å—Ç—Ä–æ–≥–æ –ø–æ —Ñ–∞–∫—Ç–∞–º –∏–∑ –±–∞–∑—ã –∑–Ω–∞–Ω–∏–π.

–ü—Ä–∞–≤–∏–ª–∞:
0. –¢–û–õ–¨–ö–û —Ñ–∞–∫—Ç—ã –∏–∑ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞, –±–µ–∑ –≤—ã–¥—É–º–æ–∫
1. –ö–æ—Ä–æ—Ç–∫–æ –∏ –ø–æ –¥–µ–ª—É
2. –ü—É–Ω–∫—Ç—ã/—Å–ø–∏—Å–∫–∏
3. –ï—Å–ª–∏ –≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ –µ—Å—Ç—å —á–∏—Å–ª–∞/—Å—Ä–æ–∫–∏ ‚Äî –ù–ï –∏–∑–º–µ–Ω—è—Ç—å
4. –ë–µ–∑ —ç–º–ø–∞—Ç–∏–∏/–æ–±—â–∏—Ö —Ñ—Ä–∞–∑ ‚Äî —Ç–æ–ª—å–∫–æ —Ñ–∞–∫—Ç—ã
5. –ë–µ–∑ —Å–ª—É–∂–µ–±–Ω—ã—Ö —Å–∏–º–≤–æ–ª–æ–≤ Markdown –≤ –æ—Ç–≤–µ—Ç–µ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—é
6. –†–∞–∑–±–∏–≤–∞–π —Ç–µ–∫—Å—Ç –Ω–∞ –∞–±–∑–∞—Ü—ã –∏ —Å–º—ã—Å–ª–æ–≤—ã–µ –±–ª–æ–∫–∏
7. –ï—Å–ª–∏ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –Ω–µ—Ç ‚Äî —á–µ—Å—Ç–Ω–æ —Å–∫–∞–∂–∏ –æ–± —ç—Ç–æ–º

## –ö–æ–Ω—Ç–µ–∫—Å—Ç –¥–ª—è –æ—Ç–≤–µ—Ç–∞:
{context}

## –í–æ–ø—Ä–æ—Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è:
{user_query}

## –ú–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ:
- –¢–æ–Ω: {tone}
- –ü—Ä–µ–¥–ø–æ—á—Ç–∏—Ç–µ–ª—å–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç: {preferred_format}
{f"- CTA —Ç–µ–∫—Å—Ç: {cta_for_prompt}" if cta_for_prompt else ""}

–û—Ç–≤–µ—á–∞–π —Å—Ç—Ä–æ–≥–æ –≤ —Ñ–æ—Ä–º–∞—Ç–µ JSON:
{{
    "short": "–ö–æ—Ä–æ—Ç–∫–∏–π –æ—Ç–≤–µ—Ç –ø–æ —Å—É—Ç–∏ –≤–æ–ø—Ä–æ—Å–∞",
    "bullets": ["–î–µ—Ç–∞–ª—å 1", "–î–µ—Ç–∞–ª—å 2", "–î–µ—Ç–∞–ª—å 3"],
    "cta": "–ú—è–≥–∫–∏–π –ø—Ä–∏–∑—ã–≤ –∫ –¥–µ–π—Å—Ç–≤–∏—é",
    "used_chunks": ["ID1", "ID2"],
    "tone": "{tone}",
    "warnings": []
}}
"""
    else:
        # –ê–¥–∞–ø—Ç–∏–≤–Ω–∞—è –ª–æ–≥–∏–∫–∞ –¥–ª—è verbatim: false —Ñ–∞–π–ª–æ–≤
        format_instruction = ""
        if "detailed" in preferred_format:
            format_instruction = "\n\n–í–ê–ñ–ù–û: –î–∞–π —Ä–∞–∑–≤–µ—Ä–Ω—É—Ç—ã–π, –ø–æ–¥—Ä–æ–±–Ω—ã–π –æ—Ç–≤–µ—Ç —Å –º–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ–º –¥–µ—Ç–∞–ª–µ–π –∏ –æ–±—ä—è—Å–Ω–µ–Ω–∏–π. –ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å –Ω—É–∂–¥–∞–µ—Ç—Å—è –≤ –ø–æ–ª–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏."
        elif "short" in preferred_format:
            format_instruction = "\n\n–í–ê–ñ–ù–û: –î–∞–π –∫—Ä–∞—Ç–∫–∏–π, –ª–∞–∫–æ–Ω–∏—á–Ω—ã–π –æ—Ç–≤–µ—Ç –ø–æ —Å—É—Ç–∏ –≤–æ–ø—Ä–æ—Å–∞. –ë–µ–∑ –ª–∏—à–Ω–∏—Ö –¥–µ—Ç–∞–ª–µ–π."
        
        system_prompt = f"""
–¢—ã ‚Äî –¥—Ä—É–∂–µ–ª—é–±–Ω—ã–π –∏ –≤–Ω–∏–º–∞—Ç–µ–ª—å–Ω—ã–π –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç —Å—Ç–æ–º–∞—Ç–æ–ª–æ–≥–∏—á–µ—Å–∫–æ–π –∫–ª–∏–Ω–∏–∫–∏ –¶–≠–°–ò –Ω–∞ –ö–∞–º—á–∞—Ç–∫–µ.
–¢–≤–æ—è –∑–∞–¥–∞—á–∞ ‚Äî –æ—Ç–≤–µ—á–∞—Ç—å –Ω–∞ –≤–æ–ø—Ä–æ—Å—ã –ø–∞—Ü–∏–µ–Ω—Ç–æ–≤ –ø—Ä–æ—Å—Ç–æ, –ø–æ–Ω—è—Ç–Ω–æ –∏ —Å –∑–∞–±–æ—Ç–æ–π, –Ω–æ —Å—Ç—Ä–æ–≥–æ –ø–æ –¥–µ–ª—É –∏ –Ω–∞ –æ—Å–Ω–æ–≤–µ –±–∞–∑—ã –∑–Ω–∞–Ω–∏–π.
–¢–æ–Ω –æ–±—â–µ–Ω–∏—è ‚Äî —Ç—ë–ø–ª—ã–π, —ç–∫—Å–ø–µ—Ä—Ç–Ω—ã–π –∏ —É–≤–∞–∂–∏—Ç–µ–ª—å–Ω—ã–π.

–ü—Ä–∞–≤–∏–ª–∞:
0. –í–ê–ñ–ù–û: –ï—Å–ª–∏ –≤–æ–ø—Ä–æ—Å –æ –±–æ–ª–∏, —Å—Ç—Ä–∞—Ö–µ, –¥–∏—Å–∫–æ–º—Ñ–æ—Ä—Ç–µ ‚Äî –í–°–ï–ì–î–ê —Å–Ω–∞—á–∞–ª–∞ –¥–∞–π –ø–æ–¥—Ä–æ–±–Ω—ã–π –∏–Ω—Ñ–æ—Ä–º–∞—Ç–∏–≤–Ω—ã–π –æ—Ç–≤–µ—Ç –Ω–∞ –æ—Å–Ω–æ–≤–µ –±–∞–∑—ã –∑–Ω–∞–Ω–∏–π, –∞ –ø–æ—Ç–æ–º –∫–æ—Ä–æ—Ç–∫—É—é —É—Å–ø–æ–∫–∞–∏–≤–∞—é—â—É—é —Ñ—Ä–∞–∑—É. –ù–ò–ö–û–ì–î–ê –Ω–µ –ø—Ä–µ–¥–ª–∞–≥–∞–π —Å—Ä–∞–∑—É –∫–æ–Ω—Å—É–ª—å—Ç–∞—Ü–∏—é –±–µ–∑ –æ—Ç–≤–µ—Ç–∞!
1. –í—Å–µ–≥–¥–∞ –ø–∏—à–∏ –≤ —Ñ–æ—Ä–º–∞—Ç–µ –æ–±—ã—á–Ω–æ–≥–æ –¥–∏–∞–ª–æ–≥–∞, –±–µ–∑ —Å–ª—É–∂–µ–±–Ω—ã—Ö —Å–∏–º–≤–æ–ª–æ–≤ Markdown –≤ –æ—Ç–≤–µ—Ç–µ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—é.
2. –†–∞–∑–±–∏–≤–∞–π —Ç–µ–∫—Å—Ç –Ω–∞ –∞–±–∑–∞—Ü—ã –∏ —Å–º—ã—Å–ª–æ–≤—ã–µ –±–ª–æ–∫–∏.
3. –ò—Å–ø–æ–ª—å–∑—É–π —á–µ–ª–æ–≤–µ—á–µ—Å–∫–∏–µ —Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–∫–∏, –∏–∑–±–µ–≥–∞–π –ø—É—Å—Ç—ã—Ö —Ñ—Ä–∞–∑.
4. –≠–º–æ–¥–∑–∏ –∏—Å–ø–æ–ª—å–∑—É–π —Ç–æ–ª—å–∫–æ –ø—Ä–∏ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç–∏ –∏ —É–º–µ—Ä–µ–Ω–Ω–æ.
5. –ï—Å–ª–∏ –≤–æ–ø—Ä–æ—Å —Ç—Ä–µ–≤–æ–∂–Ω—ã–π ‚Äî —Å–Ω–∞—á–∞–ª–∞ —Ñ–∞–∫—Ç—ã –∏–∑ –±–∞–∑—ã –∑–Ω–∞–Ω–∏–π, –ø–æ—Ç–æ–º –º—è–≥–∫–∞—è —É—Å–ø–æ–∫–∞–∏–≤–∞—é—â–∞—è —Ñ—Ä–∞–∑–∞.
6. –§–æ—Ä–º—É–ª–∏—Ä–æ–≤–∫–∏ –∏ —Ü–∏—Ñ—Ä—ã –∏–∑ –±–∞–∑—ã –∑–Ω–∞–Ω–∏–π —Å–æ—Ö—Ä–∞–Ω—è–π –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ —Ç–æ—á–Ω–æ, –∞–¥–∞–ø—Ç–∏—Ä—É—è –ø–æ–¥ –∂–∏–≤–æ–π –¥–∏–∞–ª–æ–≥.
7. –ï—Å–ª–∏ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –Ω–µ—Ç ‚Äî —á–µ—Å—Ç–Ω–æ —Å–∫–∞–∂–∏ –æ–± —ç—Ç–æ–º –∏ –ø—Ä–µ–¥–ª–æ–∂–∏ –∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤—É: –∫–æ–Ω—Å—É–ª—å—Ç–∞—Ü–∏—é –∏–ª–∏ –¥—Ä—É–≥–æ–π —Å–ø–æ—Å–æ–± —É–∑–Ω–∞—Ç—å –æ—Ç–≤–µ—Ç.
8. –í –∫–æ–Ω—Ü–µ –ø—Ä–∏ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏ –¥–æ–±–∞–≤—å –º—è–≥–∫–∏–π CTA —Å –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ–º –∫–æ–Ω—Å—É–ª—å—Ç–∞—Ü–∏–∏.{format_instruction}

–ö–∞–∫ —Ä–∞–±–æ—Ç–∞—Ç—å —Å –±–∞–∑–æ–π:
1. YAML-—Ñ—Ä–æ–Ω—Ç–º–∞—Ç—Ç–µ—Ä –∏–≥–Ω–æ—Ä–∏—Ä—É–π –ø—Ä–∏ —Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–∞–Ω–∏–∏ –æ—Ç–≤–µ—Ç–∞, –Ω–æ –∏—Å–ø–æ–ª—å–∑—É–π –µ–≥–æ –∑–Ω–∞—á–µ–Ω–∏—è –¥–ª—è —Å—Ç–∏–ª—è –∏ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã.
2. –ü—Ä–∏ –ø–æ—Å—Ç—Ä–æ–µ–Ω–∏–∏ –æ—Ç–≤–µ—Ç–∞ –æ—Ä–∏–µ–Ω—Ç–∏—Ä—É–π—Å—è –Ω–∞ –ø–æ–ª—è:
   - 'preferred_format' ‚Äî —Å—Ç—Ä—É–∫—Ç—É—Ä–∞ –æ—Ç–≤–µ—Ç–∞.
   - 'cta_text' –∏ 'cta_link' ‚Äî –¥–ª—è —Ñ–∏–Ω–∞–ª—å–Ω–æ–π —Ñ—Ä–∞–∑—ã.
   - 'tone', 'emotion' ‚Äî –¥–ª—è —Å—Ç–∏–ª—è.
3. –ò—Å–ø–æ–ª—å–∑—É–π –∫–æ–Ω—Ç–µ–Ω—Ç –∏–∑ —Ä–∞–∑–¥–µ–ª–æ–≤:
   - **–ö–æ—Ä–æ—Ç–∫–æ** ‚Äî –¥–ª—è –ø–µ—Ä–≤–æ–≥–æ –∞–±–∑–∞—Ü–∞ (–ø—Ä—è–º–æ–π –æ—Ç–≤–µ—Ç).
   - **–î–µ—Ç–∞–ª–∏** ‚Äî –¥–ª—è –≤—Ç–æ—Ä–æ–≥–æ –∞–±–∑–∞—Ü–∞ (—Ñ–∞–∫—Ç—ã –∏ —É—Ç–æ—á–Ω–µ–Ω–∏—è).
   - **–°–ª–µ–¥—É—é—â–∏–π —à–∞–≥** ‚Äî –¥–ª—è –∑–∞–∫–ª—é—á–µ–Ω–∏—è –∏ CTA.

## –ö–æ–Ω—Ç–µ–∫—Å—Ç –¥–ª—è –æ—Ç–≤–µ—Ç–∞:
{context}

## –í–æ–ø—Ä–æ—Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è:
{user_query}

## –ú–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ:
- –¢–æ–Ω: {tone}
- –ü—Ä–µ–¥–ø–æ—á—Ç–∏—Ç–µ–ª—å–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç: {preferred_format}
{f"- CTA —Ç–µ–∫—Å—Ç: {cta_for_prompt}" if cta_for_prompt else ""}

–í–ê–ñ–ù–û: –ï—Å–ª–∏ –≤–æ–ø—Ä–æ—Å –æ –≤—Ä–∞—á–µ –∏–ª–∏ —Å–æ—Ç—Ä—É–¥–Ω–∏–∫–µ –∫–ª–∏–Ω–∏–∫–∏, –æ—Ç–≤–µ—á–∞–π —Ç–æ–ª—å–∫–æ –ø–æ –±–∞–∑–µ. –ï—Å–ª–∏ –¥–∞–Ω–Ω—ã—Ö –Ω–µ—Ç, —Å–∫–∞–∂–∏, —á—Ç–æ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –Ω–µ—Ç, –±–µ–∑ –ø—Ä–µ–¥–ø–æ–ª–æ–∂–µ–Ω–∏–π.

–û—Ç–≤–µ—á–∞–π —Å—Ç—Ä–æ–≥–æ –≤ —Ñ–æ—Ä–º–∞—Ç–µ JSON:
{{
    "short": "–ö–æ—Ä–æ—Ç–∫–∏–π –æ—Ç–≤–µ—Ç –ø–æ —Å—É—Ç–∏ –≤–æ–ø—Ä–æ—Å–∞",
    "bullets": ["–î–µ—Ç–∞–ª—å 1", "–î–µ—Ç–∞–ª—å 2", "–î–µ—Ç–∞–ª—å 3"],
    "cta": "–ú—è–≥–∫–∏–π –ø—Ä–∏–∑—ã–≤ –∫ –¥–µ–π—Å—Ç–≤–∏—é",
    "used_chunks": ["ID1", "ID2"],
    "tone": "{tone}",
    "warnings": []
}}
"""

    completion = openai_client.chat.completions.create(
        model="gpt-4o-mini",
        temperature=0.2,  # –Ω–∏–∑–∫–∞—è —Ç–µ–º–ø–µ—Ä–∞—Ç—É—Ä–∞ –¥–ª—è —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏
        max_tokens=250,   # –æ–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –¥–ª–∏–Ω—É
        top_p=0.8,       # –æ–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –≤—ã–±–æ—Ä —Ç–æ–∫–µ–Ω–æ–≤
        response_format={"type": "json_object"},
        messages=[
            {"role": "system", "content": "–¢—ã –æ—Ç–≤–µ—á–∞–µ—à—å —Å—Ç—Ä–æ–≥–æ JSON –ø–æ —Å—Ö–µ–º–µ {short, bullets[], cta, used_chunks, tone, warnings}. –ù–∏–∫–æ–≥–¥–∞ –Ω–µ –∏—Å–ø–æ–ª—å–∑—É–π Markdown –∑–∞–≥–æ–ª–æ–≤–∫–∏ –≤ –æ—Ç–≤–µ—Ç–µ."},
            {"role": "user", "content": system_prompt}
        ]
    )
    
    try:
        json_response = json.loads(completion.choices[0].message.content)
        result_cta = json_response.get("cta", "")
        if not allow_cta:
            result_cta = ""
        return SynthJSON(
            short=json_response.get("short", ""),
            bullets=json_response.get("bullets", []),
            cta=result_cta,
            used_chunks=json_response.get("used_chunks", []),
            tone=json_response.get("tone", tone),
            warnings=json_response.get("warnings", [])
        )
    except json.JSONDecodeError:
        # Fallback –µ—Å–ª–∏ JSON –Ω–µ–∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã–π
        return SynthJSON(
            short="–ò–∑–≤–∏–Ω–∏—Ç–µ, –ø—Ä–æ–∏–∑–æ—à–ª–∞ –æ—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ –æ—Ç–≤–µ—Ç–∞.",
            bullets=["–ü–æ–ø—Ä–æ–±—É–π—Ç–µ –ø–µ—Ä–µ—Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–∞—Ç—å –≤–æ–ø—Ä–æ—Å"],
            cta="–û–±—Ä–∞—Ç–∏—Ç–µ—Å—å –∫ –∞–¥–º–∏–Ω–∏—Å—Ç—Ä–∞—Ç–æ—Ä—É",
            used_chunks=[],
            tone="friendly",
            warnings=["JSON parsing error"]
        )

def compress_answer(text: str, max_length: int = 800) -> str:
    """–°–∂–∏–º–∞–µ—Ç –¥–ª–∏–Ω–Ω—ã–π –æ—Ç–≤–µ—Ç –¥–æ —É–∫–∞–∑–∞–Ω–Ω–æ–π –¥–ª–∏–Ω—ã –±–µ–∑ –ø–æ—Ç–µ—Ä–∏ –∫–ª—é—á–µ–≤–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏"""
    if len(text) <= max_length or not openai_client:
        return text
    
    # –°—á–∏—Ç–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π
    sentences = re.split(r'[.!?]+', text)
    if len(sentences) <= 3:
        return text  # –ï—Å–ª–∏ –º–∞–ª–æ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π, –Ω–µ —Å–∂–∏–º–∞–µ–º
    
    prompt = f"""–°–æ–∂–º–∏ —ç—Ç–æ—Ç –æ—Ç–≤–µ—Ç –æ —Å—Ç–æ–º–∞—Ç–æ–ª–æ–≥–∏–∏ –¥–æ {max_length} —Å–∏–º–≤–æ–ª–æ–≤, —Å–æ—Ö—Ä–∞–Ω–∏–≤ –í–°–ï –≤–∞–∂–Ω—ã–µ —Ñ–∞–∫—Ç—ã –∏ —Ü–∏—Ñ—Ä—ã.

–í–ê–ñ–ù–û:
- –°–æ—Ö—Ä–∞–Ω–∏ –í–°–ï —á–∏—Å–ª–∞, –ø—Ä–æ—Ü–µ–Ω—Ç—ã, —Å—Ä–æ–∫–∏, —Ü–µ–Ω—ã
- –°–æ—Ö—Ä–∞–Ω–∏ –∫–ª—é—á–µ–≤—É—é –º–µ–¥–∏—Ü–∏–Ω—Å–∫—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é
- –£–±–µ—Ä–∏ —Ç–æ–ª—å–∫–æ "–≤–æ–¥—É" –∏ –ø–æ–≤—Ç–æ—Ä–µ–Ω–∏—è
- –û—Å—Ç–∞–≤—å —Å—Ç—Ä—É–∫—Ç—É—Ä—É: –∫–æ—Ä–æ—Ç–∫–∏–π –æ—Ç–≤–µ—Ç + –¥–µ—Ç–∞–ª–∏ + –ø—Ä–∏–∑—ã–≤ –∫ –¥–µ–π—Å—Ç–≤–∏—é

–ò—Å—Ö–æ–¥–Ω—ã–π –æ—Ç–≤–µ—Ç:
{text}

–°–∂–∞—Ç—ã–π –æ—Ç–≤–µ—Ç:"""
    
    try:
        response = openai_client.chat.completions.create(
            model="gpt-4o-mini",
            messages=[{"role": "user", "content": prompt}],
            max_tokens=300,
            temperature=0.1
        )
        
        compressed = response.choices[0].message.content.strip()
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ —Å–∂–∞—Ç–∏–µ –Ω–µ —Å–ª–∏—à–∫–æ–º –∞–≥—Ä–µ—Å—Å–∏–≤–Ω–æ–µ
        if len(compressed) < max_length * 0.5:
            print(f"‚ö†Ô∏è –°–∂–∞—Ç–∏–µ —Å–ª–∏—à–∫–æ–º –∞–≥—Ä–µ—Å—Å–∏–≤–Ω–æ–µ, –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –∏—Å—Ö–æ–¥–Ω—ã–π —Ç–µ–∫—Å—Ç")
            return text
        
        print(f"üîç Answer compression: {len(text)} ‚Üí {len(compressed)} —Å–∏–º–≤–æ–ª–æ–≤")
        return compressed
        
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ —Å–∂–∞—Ç–∏—è –æ—Ç–≤–µ—Ç–∞: {e}")
        return text

def render_markdown(synth: SynthJSON) -> str:
    """–†–µ–Ω–¥–µ—Ä–∏—Ç JSON –≤ –∫—Ä–∞—Å–∏–≤—ã–π Markdown –±–µ–∑ —Å–ª—É–∂–µ–±–Ω—ã—Ö –∑–∞–≥–æ–ª–æ–≤–∫–æ–≤"""
    
    def clean_text(text: str) -> str:
        """–û—á–∏—â–∞–µ—Ç —Ç–µ–∫—Å—Ç –æ—Ç —Å–ª—É–∂–µ–±–Ω—ã—Ö —Å–∏–º–≤–æ–ª–æ–≤ –∏ –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–π"""
        # –£–±–∏—Ä–∞–µ–º —Å–ª—É–∂–µ–±–Ω—ã–µ —Å–∏–º–≤–æ–ª—ã –∏ –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏
        cleaned = re.sub(r'^(?:\*|–û—Ç–≤–µ—Ç:|–ò–Ω—Å—Ç—Ä—É–∫—Ü–∏—è:|JSON:|–ü—Ä–æ–º–ø—Ç:)\s*', '', text, flags=re.IGNORECASE)
        # –£–±–∏—Ä–∞–µ–º Markdown –∑–∞–≥–æ–ª–æ–≤–∫–∏
        cleaned = re.sub(r'^\s*#{1,6}\s*', '', cleaned)  # <- —Å—Ä–µ–∑–∞–µ–º '## ...'
        # –£–±–∏—Ä–∞–µ–º –ª–∏—à–Ω–∏–µ –ø—Ä–æ–±–µ–ª—ã
        cleaned = re.sub(r'\s+', ' ', cleaned)
        # –£–±–∏—Ä–∞–µ–º –∫–∞–≤—ã—á–∫–∏ –≤ –Ω–∞—á–∞–ª–µ –∏ –∫–æ–Ω—Ü–µ
        cleaned = re.sub(r'^["\']+|["\']+$', '', cleaned)
        return cleaned.strip()
    
    # –û—á–∏—â–∞–µ–º –∏ —Ñ–æ—Ä–º–∞—Ç–∏—Ä—É–µ–º —á–∞—Å—Ç–∏
    short = clean_text(synth.short)
    cta = clean_text(synth.cta)
    
    # –§–æ—Ä–º–∞—Ç–∏—Ä—É–µ–º bullets
    bullets = []
    for bullet in synth.bullets:
        clean_bullet = clean_text(bullet)
        if clean_bullet and len(clean_bullet) > 5:  # –ú–∏–Ω–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏–Ω–∞
            bullets.append(f"- {clean_bullet}")
    
    # –°–æ–±–∏—Ä–∞–µ–º –æ—Ç–≤–µ—Ç –ë–ï–ó —Å–ª—É–∂–µ–±–Ω—ã—Ö –∑–∞–≥–æ–ª–æ–≤–∫–æ–≤
    blocks = []
    
    # –ö–æ—Ä–æ—Ç–∫–∏–π –æ—Ç–≤–µ—Ç
    if short:
        blocks.append(short)
    
    # –î–µ—Ç–∞–ª–∏
    if bullets:
        if blocks:  # –ï—Å–ª–∏ —É–∂–µ –µ—Å—Ç—å –∫–æ–Ω—Ç–µ–Ω—Ç, –¥–æ–±–∞–≤–ª—è–µ–º —Ä–∞–∑–¥–µ–ª–µ–Ω–∏–µ
            blocks.append("")
        blocks.append("\n".join(bullets))
    
    # –°–ª–µ–¥—É—é—â–∏–π —à–∞–≥ (CTA) - –±–µ–∑ –ª–∏—à–Ω–∏—Ö –æ—Ç—Å—Ç—É–ø–æ–≤
    if cta:
        if blocks:  # –ï—Å–ª–∏ —É–∂–µ –µ—Å—Ç—å –∫–æ–Ω—Ç–µ–Ω—Ç, –¥–æ–±–∞–≤–ª—è–µ–º —Ä–∞–∑–¥–µ–ª–µ–Ω–∏–µ
            blocks.append("")
        blocks.append(cta)
    
    # –°–æ–±–∏—Ä–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç
    markdown = "\n".join(blocks)
    
    # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º –ø—É—Å—Ç—ã–µ —Å—Ç—Ä–æ–∫–∏ (–º–∞–∫—Å–∏–º—É–º 1 –ø–æ–¥—Ä—è–¥)
    markdown = re.sub(r'\n{2,}', '\n', markdown)
    
    # –ü—Ä–∏–º–µ–Ω—è–µ–º –∞–Ω—Ç–∏-–≤–æ–¥—É
    markdown = strip_fluff_start(markdown)
    
    return markdown

# –û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è
def get_rag_answer(user_message: str, history: List[Dict] = []) -> tuple[str, dict]:
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è –æ—Ç–≤–µ—Ç–∞ –∏ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö"""
    # ‚úÖ –ó–∞–≤–æ–¥–∏–º –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ-—Ñ–ª–∞–≥–∏ –¥–ª—è bypass guard
    alias_used = False
    doctor_hit = False
    exact_h2 = False
    
    # –£–≤–µ–ª–∏—á–∏–≤–∞–µ–º top_k –¥–ª—è –ø–æ–∏—Å–∫–∞ –≤—Ä–∞—á–µ–π
    if DOCTOR_REGEX and DOCTOR_REGEX.search(user_message):
        top_k = 12
        print(f"üîç –ü–æ–∏—Å–∫ –≤—Ä–∞—á–µ–π: –∏—Å–ø–æ–ª—å–∑—É–µ–º top_k={top_k}")
    else:
        top_k = 8  # –Ω–µ–º–Ω–æ–≥–æ —à–∏—Ä–µ –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é
    
    # –ò–∑–≤–ª–µ–∫–∞–µ–º —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–µ —á–∞–Ω–∫–∏
    relevant_chunks = retrieve_relevant_chunks(user_message, top_k=top_k)
    
    # –ï—Å–ª–∏ –Ω–µ—Ç –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤, –ø—Ä–æ–±—É–µ–º alias fallback
    if not relevant_chunks:
        from core.md_loader import alias_fallback
        fallback_candidates = alias_fallback(user_message)
        if fallback_candidates:
            # ‚úÖ –ö–æ–≥–¥–∞ —Å—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç alias-fallback ‚Äî —Å—Ç–∞–≤–∏–º alias_used = True
            alias_used = True
            print(f"‚úÖ Alias fallback —Å—Ä–∞–±–æ—Ç–∞–ª: {len(fallback_candidates)} –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤")
            
            # –°–æ–∑–¥–∞–µ–º —Ñ–∏–∫—Ç–∏–≤–Ω—ã–µ —á–∞–Ω–∫–∏ –¥–ª—è fallback
            from dataclasses import dataclass
            @dataclass
            class FallbackChunk:
                id: str
                text: str
                file_name: str
                metadata: object
                score: float = 1.0
            
            relevant_chunks = []
            for candidate in fallback_candidates:
                # –ß–∏—Ç–∞–µ–º —Ñ–∞–π–ª –∏ —Å–æ–∑–¥–∞–µ–º —á–∞–Ω–∫
                try:
                    with open(candidate["file"], 'r', encoding='utf-8') as f:
                        content = f.read()
                    chunk = FallbackChunk(
                        id=candidate["file"],
                        text=content,
                        file_name=candidate["file"].split('/')[-1],
                        metadata=type('obj', (object,), {'h2_id': candidate["h2_id"]}),
                        score=candidate["score"]
                    )
                    relevant_chunks.append(chunk)
                except:
                    continue
    
    if not relevant_chunks:
        return """–ö —Å–æ–∂–∞–ª–µ–Ω–∏—é, –≤ –º–æ–µ–π –±–∞–∑–µ –Ω–µ—Ç –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –ø–æ —ç—Ç–æ–º—É –≤–æ–ø—Ä–æ—Å—É.

–ó–∞–ø–∏—à–∏—Ç–µ—Å—å –Ω–∞ –±–µ—Å–ø–ª–∞—Ç–Ω—É—é –∫–æ–Ω—Å—É–ª—å—Ç–∞—Ü–∏—é ‚Äî –Ω–∞—à —Å–ø–µ—Ü–∏–∞–ª–∏—Å—Ç –æ—Ç–≤–µ—Ç–∏—Ç –Ω–∞ –≤—Å–µ –≤–∞—à–∏ –≤–æ–ø—Ä–æ—Å—ã.""", {}
    
    # –õ–æ–≥–∏—Ä—É–µ–º –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤ –¥–ª—è –æ—Ç–ª–∞–¥–∫–∏
    print("üîç CANDIDATES:")
    for i, chunk in enumerate(relevant_chunks[:5]):
        print(f"  {i+1}. {chunk.file_name}: {chunk.text[:100]}...")
    
    # –§–æ—Ä–º–∏—Ä—É–µ–º candidates_with_scores –¥–ª—è –Ω–æ–≤–æ–π —Å–∏—Å—Ç–µ–º—ã
    candidates_with_scores = []
    for chunk in relevant_chunks:
        candidates_with_scores.append({
            "chunk": chunk,
            "score": getattr(chunk, 'score', 0.5),  # –ë–∞–∑–æ–≤—ã–π —Å–∫–æ—Ä
            "doc_id": chunk.id,
            "file_name": chunk.file_name,
            "topic": getattr(chunk.metadata, 'topic', None),
            "h2_id": getattr(chunk.metadata, 'h2_id', None)
        })
    
    # ‚úÖ –û–ø—Ä–µ–¥–µ–ª—è–µ–º doctor_hit –∏ exact_h2
    if relevant_chunks:
        primary_chunk = relevant_chunks[0]
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —è–≤–ª—è–µ—Ç—Å—è –ª–∏ —ç—Ç–æ –∫–∞—Ä—Ç–æ—á–∫–æ–π –≤—Ä–∞—á–∞
        if hasattr(primary_chunk.metadata, 'doc_type') and primary_chunk.metadata.doc_type in ('doctor', 'doctors'):
            doctor_hit = True
            print(f"‚úÖ Doctor card hit: {primary_chunk.file_name}")
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ç–æ—á–Ω–æ–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ H2 –∑–∞–≥–æ–ª–æ–≤–∫–∞
        if hasattr(primary_chunk.metadata, 'h2_id') and primary_chunk.metadata.h2_id:
            # –ú–æ–∂–Ω–æ –¥–æ–±–∞–≤–∏—Ç—å –±–æ–ª–µ–µ —Å–ª–æ–∂–Ω—É—é –ª–æ–≥–∏–∫—É –¥–ª—è exact_h2_match
            exact_h2 = True
            print(f"‚úÖ Exact H2 match: {primary_chunk.metadata.h2_id}")
    
    # –°—Ä–∞–∑—É –ø–æ—Å–ª–µ –ø–æ–ª—É—á–µ–Ω–∏—è relevant_chunks –≤—ã—á–∏—Å–ª—è–µ–º —Ñ–ª–∞–≥–∏ —Ä–∞–∑—Ä–µ—à–µ–Ω–∏–π
    primary_chunk = relevant_chunks[0] if relevant_chunks else None
    doc_type = (primary_chunk.metadata.doc_type if primary_chunk else "info")
    _cfg = EMPATHY_CFG.get("features", {})

    allow_cta       = _cfg.get("cta_enabled", True) and doc_type not in set(_cfg.get("disable_cta_on_doc_types", []))
    allow_post      = _cfg.get("postprocess_enabled", True) and doc_type not in set(_cfg.get("disable_postprocess_on_doc_types", []))
    allow_empathy   = _cfg.get("empathy_enabled", True) and doc_type not in set(_cfg.get("disable_empathy_on_doc_types", []))
    
    # –°–∏–Ω—Ç–µ–∑–∏—Ä—É–µ–º —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –æ—Ç–≤–µ—Ç
    synth_response = synthesize_answer(relevant_chunks, user_message, allow_cta)
    
    # –†–µ–Ω–¥–µ—Ä–∏–º –≤ Markdown
    markdown_response = render_markdown(synth_response)
    
    # ==== –≠–ú–ü–ê–¢–ò–Ø ====
    # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —ç–º–æ—Ü–∏—é –Ω–∞ –æ—Å–Ω–æ–≤–µ –∑–∞–ø—Ä–æ—Å–∞ –∏ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
    retrieved_snippet = "\n".join([ch.text[:200] for ch in relevant_chunks[:3]])
    fm = {}
    if relevant_chunks:
        try:
            meta = relevant_chunks[0].metadata
            fm = {
                "emotion": getattr(meta, "emotion", None),
                "verbatim": getattr(meta, "verbatim", False),
                "doc_type": getattr(meta, "doc_type", None)
            }
        except Exception:
            pass
    
    emotion, detector, confidence = detect_emotion(
        user_query=user_message,
        retrieved_snippet=retrieved_snippet,
        fm=fm,
        empathy_cfg=EMPATHY_CFG,
        triggers_bank=EMPATHY_TRIGGERS,
        llm_client=openai_client,
        model="gpt-4o-mini"
    )
    
    # –°–æ–±–∏—Ä–∞–µ–º —Ñ–∏–Ω–∞–ª—å–Ω—ã–π —Ç–µ–∫—Å—Ç —Å —ç–º–ø–∞—Ç–∏–µ–π
    core_text = markdown_response
    
    # --- CTA
    cta_text = primary_chunk.metadata.cta_text if primary_chunk else None
    cta_link = primary_chunk.metadata.cta_link if primary_chunk else None
    if not allow_cta:
        cta_text, cta_link = None, None
    
    # --- Postprocess
    final_text = core_text
    if allow_post:
        final_text = postprocess_answer_with_empathy(
            base_text=core_text, tone="friendly", emotion=emotion, cta_text=cta_text, cta_link=cta_link
        )
    
    # --- Empathy
    if allow_empathy:
        final_text = build_answer(final_text, emotion, EMPATHY_BANK, EMPATHY_CFG, _RNG)
    
    # –ü–æ—Å—Ç–ø—Ä–æ—Ü–µ—Å—Å–æ—Ä: –Ω–µ –¥–∞—ë–º ¬´–ø—É—Å—Ç—ã—Ö¬ª –æ—Ç–≤–µ—Ç–æ–≤
    if not relevant_chunks or len(markdown_response.strip()) < 40:
        print(f"‚ö†Ô∏è –ü—É—Å—Ç–æ–π –æ—Ç–≤–µ—Ç, –ø—Ä–æ–±—É–µ–º —Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏–π fallback...")
        # –ü–æ–ø—Ä–æ–±—É–µ–º –¥–æ—Å—Ç–∞—Ç—å —Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏–π —Ñ–∞–∫—Ç –¥–ª—è –±–æ–ª–∏/—Ü–µ–Ω—ã/–≥–∞—Ä–∞–Ω—Ç–∏–∏
        detected_topics = route_topics(user_message)
        print(f"üîç –û–±–Ω–∞—Ä—É–∂–µ–Ω—ã —Ç–µ–º—ã: {detected_topics}")
        if detected_topics:
            theme_key = list(detected_topics)[0]
            fb = fallback_theme_chunks(theme_key, limit=2)
            print(f"üîç Fallback —á–∞–Ω–∫–æ–≤: {len(fb)}")
            if fb:
                markdown_response = "\n\n".join(ch.text.strip() for ch in fb if ch and ch.text)[:1200]
                print(f"‚úÖ Fallback –ø—Ä–∏–º–µ–Ω–µ–Ω, –Ω–æ–≤—ã–π —Ä–∞–∑–º–µ—Ä: {len(markdown_response)} —Å–∏–º–≤–æ–ª–æ–≤")
            else:
                print(f"‚ùå Fallback –Ω–µ —Å—Ä–∞–±–æ—Ç–∞–ª –¥–ª—è —Ç–µ–º—ã: {theme_key}")
    
    # –°–æ–±–∏—Ä–∞–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –∏–∑ –ø–µ—Ä–≤–æ–≥–æ —á–∞–Ω–∫–∞ –¥–ª—è CTA
    rag_meta = {}
    cta_text = None
    cta_link = None
    
    for ch in relevant_chunks:
        try:
            meta = ch.metadata
            if getattr(meta, "cta_text", None): cta_text = meta.cta_text
            if getattr(meta, "cta_link", None): cta_link = meta.cta_link
            
            # –°–æ–±–∏—Ä–∞–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –¥–ª—è CTA
            rag_meta = {
                "topic": getattr(meta, "topic", None),
                "doc_type": getattr(meta, "doc_type", None),
                "section": getattr(meta, "section", None),
                "cta_action": getattr(meta, "cta_action", None),
                "cta_text": cta_text,
                "cta_link": cta_link
            }
            break  # –±–µ—Ä–µ–º —Ç–æ–ª—å–∫–æ –∏–∑ –ø–µ—Ä–≤–æ–≥–æ —á–∞–Ω–∫–∞
        except Exception:
            pass
    
    # –ü—Ä–∏–º–µ–Ω—è–µ–º –∞–Ω—Ç–∏-—Ñ–ª—É—Ñ—Ñ —Ñ–∏–ª—å—Ç—Ä –∫ —Ñ–∏–Ω–∞–ª—å–Ω–æ–º—É —Ç–µ–∫—Å—Ç—É
    final_text = strip_fluff_start(final_text)
    
    # ==== ANSWER COMPRESSION ====
    if len(final_text) > 800:  # –°–∂–∏–º–∞–µ–º —Ç–æ–ª—å–∫–æ –¥–ª–∏–Ω–Ω—ã–µ –æ—Ç–≤–µ—Ç—ã
        final_text = compress_answer(final_text, max_length=800)
    
    # –û—Ç–ª–∞–¥–æ—á–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è
    print(f"üîç –í–æ–ø—Ä–æ—Å: '{user_message}'")
    print(f"üìÑ –ù–∞–π–¥–µ–Ω–æ —á–∞–Ω–∫–æ–≤: {len(relevant_chunks)}")
    for i, chunk in enumerate(relevant_chunks[:3]):  # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –ø–µ—Ä–≤—ã–µ 3
        print(f"  {i+1}. {chunk.file_name}: {chunk.text[:100]}...")
    print(f"üìù –û—Ç–≤–µ—Ç: {final_text[:200]}...")
    
    # –î–æ–±–∞–≤–ª—è–µ–º used_chunks –≤ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
    used_ids = [ch.id for ch in relevant_chunks]
    rag_meta["used_chunks"] = used_ids
    rag_meta["candidates_with_scores"] = candidates_with_scores
    
    # ‚úÖ –î–æ–±–∞–≤–ª—è–µ–º —Ñ–ª–∞–≥–∏ –¥–ª—è bypass guard
    rag_meta.update({
        "used_chunks": [
            {
                "file": chunk.file_name,
                "h2_id": getattr(chunk.metadata, "h2_id", None),
                "score": getattr(chunk, "score", 0.0)
            }
            for chunk in relevant_chunks
        ],
        # —Ñ–ª–∞–≥–∏ –¥–ª—è bypass guard
        "source": "alias" if alias_used else "search",
        "doctor_card_hit": doctor_hit,
        "exact_h2_match": exact_h2,
    })
    
    # –î–æ–±–∞–≤–ª—è–µ–º –ø–æ–ª—è —ç–º–ø–∞—Ç–∏–∏ –≤ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
    rag_meta.update({
        "emotion": emotion,
        "detector": detector,
        "confidence": round(float(confidence), 2),
        "opener_used": (emotion != "none" and any(final_text.startswith(x) for x in EMPATHY_BANK.get(emotion, {}).get("openers", []))),
        "closer_used": (emotion != "none" and any(final_text.endswith(x) for x in EMPATHY_BANK.get(emotion, {}).get("closers", []))),
        # SAFE-—Ä–µ–∂–∏–º —Å—Ç–∞—Ç—É—Å—ã
        "empathy_enabled": allow_empathy,
        "postprocess_enabled": allow_post,
        "cta_enabled": allow_cta
    })
    
    print(f"üìã –ú–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ: {rag_meta}")
    
    return final_text, rag_meta

def log_query_response(user_message: str, response: str, metadata: dict, chunks_used: List[str] = None):
    """–õ–æ–≥–∏—Ä—É–µ—Ç –≤–æ–ø—Ä–æ—Å/–æ—Ç–≤–µ—Ç –≤ —Ñ–∞–π–ª –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞"""
    import json
    from datetime import datetime
    
    log_entry = {
        "timestamp": datetime.now().isoformat(),
        "question": user_message,
        "answer": response,
        "metadata": metadata,
        "chunks_used": chunks_used or [],
        "answer_length": len(response),
        "has_cta": bool(metadata.get("cta_action")),
        "topic": metadata.get("topic", "unknown"),
        "doc_type": metadata.get("doc_type", "unknown"),
        # –ü–æ–ª—è —ç–º–ø–∞—Ç–∏–∏ –¥–ª—è –æ—Ç–ª–∞–¥–∫–∏
        "emotion": metadata.get("emotion", "none"),
        "emotion_source": metadata.get("detector", "unknown"),
        "emotion_confidence": metadata.get("confidence", 0.0),
        "opener_used": metadata.get("opener_used", False),
        "closer_used": metadata.get("closer_used", False)
    }
    
    try:
        # –°–æ–∑–¥–∞–µ–º –ø–∞–ø–∫—É logs –µ—Å–ª–∏ –µ—ë –Ω–µ—Ç
        logs_dir = Path("logs")
        logs_dir.mkdir(exist_ok=True)
        
        # –ó–∞–ø–∏—Å—ã–≤–∞–µ–º –≤ JSONL —Ñ–∞–π–ª
        log_file = logs_dir / "queries.jsonl"
        with open(log_file, "a", encoding="utf-8") as f:
            f.write(json.dumps(log_entry, ensure_ascii=False) + "\n")
        
        print(f"üìù –õ–æ–≥ –∑–∞–ø–∏—Å–∞–Ω: {log_file}")
        
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è: {e}")


