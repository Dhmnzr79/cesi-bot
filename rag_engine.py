# rag_engine.py
import os
import numpy as np
from openai import OpenAI
from core.faiss_compat import IndexFlatIP, normalize_L2_inplace, HAS_FAISS
import yaml
import re
import json
import difflib
import random
from pathlib import Path
from dotenv import load_dotenv
from typing import Dict, List, Optional, Any, Set, Tuple
from textwrap import dedent
from rank_bm25 import BM25Okapi
from rapidfuzz import fuzz
# from core.empathy import detect_emotion, build_answer  # –§—É–Ω–∫—Ü–∏–∏ –Ω–µ –∏—Å–ø–æ–ª—å–∑—É—é—Ç—Å—è –≤ –Ω–æ–≤–æ–º –∫–æ–¥–µ

# ==== –ö–û–ù–°–¢–ê–ù–¢–´ ====
CONFIG_DIR = Path("config")
MD_DIR = Path("md")
THEMES_PATH = CONFIG_DIR / "themes.json"

# ==== –ê–ù–¢–ò-–í–û–î–ê –†–ï–ì–ï–ö–°–´ ====
ANTI_FLUFF = [
    r"^–º—ã\s+—Ä–∞–¥—ã\s+—Å–æ–æ–±—â–∏—Ç—å.*",
    r"^—Å\s+—Ä–∞–¥–æ—Å—Ç—å—é\s+—Å–æ–æ–±—â–∞–µ–º.*",
    r"^–º—ã\s+–≥–æ—Ä–¥–∏–º—Å—è.*",
    r"^–≤\s+–Ω–∞—à–µ–π\s+–∫–ª–∏–Ω–∏–∫–µ\s+–º—ã.*",
    r"^—ç—Ç–æ\s+–∑–∞–º–µ—á–∞—Ç–µ–ª—å–Ω[–æ—ã].*",
    r"–ø–æ—Å—Ç–∞—Ä–∞–µ–º—Å—è\s+–º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ\s+–ø–æ–Ω—è—Ç—å.*",
    r"^—Å–æ–≤—Ä–µ–º–µ–Ω–Ω[–∞-—è]+\s+—Ç–µ—Ö–Ω–æ–ª–æ–≥–∏.*",
    r"^–æ—Ç–∫—Ä—ã–≤–∞–µ—Ç\s+–≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏.*",
]

# ==== –ì–õ–û–ë–ê–õ–¨–ù–´–ï –°–¢–†–£–ö–¢–£–†–´ –î–õ–Ø –ö–ê–¢–ê–õ–û–ì–ê –°–£–©–ù–û–°–¢–ï–ô ====
ENTITY_INDEX = {}  # alias_norm -> {"topic": str, "entity": str, "doc_id": str, "section": str}
ENTITY_CHUNKS = {}  # (topic, entity) -> RetrievedChunk
ALIAS_MAP = {}  # normalize(alias) -> {"file": path, "primary_h2_id": ...}

# ==== –ù–û–í–ê–Ø –ê–†–•–ò–¢–ï–ö–¢–£–†–ê –ò–ù–î–ï–ö–°–û–í ====
# === 1) –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è —Ç–µ–º ===
CANON = {"doctors","consultation","prices","warranty","contacts","implants","safety","clinic"}

def norm_topic(x: str) -> str:
    return (x or "").strip().lower()

def norm_text(x: str) -> str:
    import unicodedata
    x = unicodedata.normalize("NFKD", (x or "").lower().strip())
    x = re.sub(r"\s+", " ", x)
    return x

# === 2) –ö–æ–Ω—Ç–µ–π–Ω–µ—Ä—ã –∏–Ω–¥–µ–∫—Å–æ–≤ ===
ALIAS_MAP_GLOBAL = {}   # norm(alias) -> {"topic":..., "file":...}
H2_INDEX = {}           # norm(h2_text/h2_id/local_alias) -> {"topic":..., "file":..., "h2_id":...}
FILE_META = {}          # file -> {"topic":..., "aliases": [...], "mini_links":[...]}
ALL_CHUNKS = []         # –≤–∞—à–∏ —á–∞–Ω–∫-–æ–±—ä–µ–∫—Ç—ã (–∫–∞–∫ –∏ —Ä–∞–Ω—å—à–µ)

# ==== BM25 –ò–ù–î–ï–ö–° ====
bm25_index = None
bm25_corpus = []

def _normalize(text: str) -> str:
    # NBSP -> –æ–±—ã—á–Ω—ã–π –ø—Ä–æ–±–µ–ª; CRLF -> LF
    return text.replace('\u00A0', ' ').replace('\r\n', '\n').replace('\r', '\n')

def _norm(s: str) -> str:
    """–ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è —Å—Ç—Ä–æ–∫–∏ –¥–ª—è –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏"""
    return re.sub(r'\s+', ' ', s.lower().replace('\u00a0', ' ')).strip()

def _slugify_implant_kind(name: str) -> str:
    """–°–æ–∑–¥–∞–µ—Ç slug –¥–ª—è –≤–∏–¥–∞ –∏–º–ø–ª–∞–Ω—Ç–∞—Ü–∏–∏"""
    n = _norm(name)
    
    # –°–ø–µ—Ü–∏—Ñ–∏—á–Ω—ã–µ –º–∞–ø–ø–∏–Ω–≥–∏
    if re.search(r'all\s*[- ]?on\s*[- ]?4', n):
        return "all-on-4"
    if re.search(r'all\s*[- ]?on\s*[- ]?6', n):
        return "all-on-6"
    if n.startswith("–æ–¥–Ω–æ"):
        return "single-stage"
    if n.startswith("–∫–ª–∞—Å—Å"):
        return "classic"
    
    # –û–±—â–∏–π slug
    return re.sub(r'[^a-z0-9\-]+', '-', n)  # unidecode(n) - –æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ

load_dotenv()

# –ó–∞–≥—Ä—É–∑–∫–∞ –∫–æ–Ω—Ñ–∏–≥–æ–≤ —ç–º–ø–∞—Ç–∏–∏ (—Å—Ç–∞—Ä—ã–µ –∫–æ–Ω—Ñ–∏–≥–∏ - –Ω–µ –∏—Å–ø–æ–ª—å–∑—É—é—Ç—Å—è –≤ –Ω–æ–≤–æ–π –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–µ)
# with open(os.path.join("config", "empathy_config.yaml"), "r", encoding="utf-8") as f:
#     EMPATHY_CFG = yaml.safe_load(f)
# with open(os.path.join("config","empathy.yaml"), "r", encoding="utf-8") as f:
#     EMPATHY_BANK = yaml.safe_load(f)
# with open(os.path.join("config", "empathy_triggers.yaml"), "r", encoding="utf-8") as f:
#     EMPATHY_TRIGGERS = yaml.safe_load(f)
# _RNG = random.Random()

# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è OpenAI –∫–ª–∏–µ–Ω—Ç–∞ v1
try:
    openai_client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"), timeout=30.0)
    print("‚úÖ OpenAI –∫–ª–∏–µ–Ω—Ç v1 —É—Å–ø–µ—à–Ω–æ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω")
except Exception as e:
    print(f"‚ùå –û—à–∏–±–∫–∞ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ OpenAI –∫–ª–∏–µ–Ω—Ç–∞: {e}")
    openai_client = None

# –¢–∏–ø—ã –¥–∞–Ω–Ω—ã—Ö
class Frontmatter:
    def __init__(self, data: Dict[str, Any]):
        self.id = data.get('id', '')
        self.slug = data.get('slug', '')
        self.title = data.get('title', '')
        self.description = data.get('description', '')
        self.doc_type = data.get('doc_type', 'info')
        self.topic = data.get('topic', '')
        self.tags = data.get('tags', [])
        self.aliases = data.get('aliases', [])  # ‚úÖ –î–æ–±–∞–≤–ª—è–µ–º aliases
        self.audience = data.get('audience', '')
        self.updated = data.get('updated', '')
        self.locale = data.get('locale', 'ru-RU')
        self.tone = data.get('tone', 'friendly')
        self.emotion = data.get('emotion', '')
        self.criticality = data.get('criticality', 'medium')
        self.verbatim = data.get('verbatim', False)
        self.policy_ref = data.get('policy_ref', [])
        self.source = data.get('source', [])
        self.preferred_format = data.get('preferred_format', ['short', 'bullets', 'cta'])
        self.canonical_url = data.get('canonical_url', '')
        self.noindex = data.get('noindex', False)
        self.cta_action = data.get('cta_action', '')
        self.cta_text = data.get('cta_text', '')
        self.cta_link = data.get('cta_link', '')

class RetrievedChunk:
    def __init__(self, id: str, text: str, metadata: Frontmatter, file_name: str):
        self.id = id
        self.text = text
        self.metadata = metadata
        self.file_name = file_name
        # –ë–µ–∑–æ–ø–∞—Å–Ω–æ–µ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ –∞—Ç—Ä–∏–±—É—Ç–æ–≤ —Å fallback –∑–Ω–∞—á–µ–Ω–∏—è–º–∏
        self.updated = metadata.updated if metadata else ""
        self.criticality = metadata.criticality if metadata else "medium"
        self.doc_type = metadata.doc_type if metadata else "info"
        self.tags = metadata.tags if metadata else []

class SynthJSON:
    def __init__(self, short: str, bullets: List[str], cta: str, used_chunks: List[str], tone: str, warnings: Optional[List[str]] = None):
        self.short = short
        self.bullets = bullets
        self.cta = cta
        self.used_chunks = used_chunks
        self.tone = tone
        self.warnings = warnings or []

# === 3) –ü–∞—Ä—Å MD ===
RX_H2 = re.compile(r"(?m)^##\s+([^\n{]+?)(?:\s*\{#([^\}]+)\})?\s*$")
RX_LOC = re.compile(r"<!--\s*aliases:\s*\[(.*?)\]\s*-->", re.S|re.I)

def parse_frontmatter(text: str):
    if not text.startswith("---"): return {}, text
    m = re.match(r"^---\s*\n(.*?)\n---\s*\n(.*)$", text, re.S)
    if not m: return {}, text
    fm = yaml.safe_load(m.group(1)) or {}
    body = m.group(2)
    return fm, body

def parse_h2_sections(body: str):
    sections = []
    for m in RX_H2.finditer(body):
        title = m.group(1).strip()
        h2_id = m.group(2) or slugify(title)
        tail = body[m.end(): m.end()+400]
        loc = RX_LOC.search(tail)
        local_aliases = []
        if loc:
            raw = loc.group(1)
            local_aliases = [a.strip().strip("'\"") for a in re.split(r",\s*", raw) if a.strip()]
        sections.append({"title": title, "h2_id": h2_id, "local_aliases": local_aliases})
    return sections

def slugify(s: str) -> str:
    import unicodedata
    s = unicodedata.normalize("NFKD", s.lower().strip())
    s = re.sub(r"\s+", "-", s)
    s = re.sub(r"[^a-z0-9\-–∞-—è—ë]", "", s)
    return s

# === 4) –†–µ–≥–∏—Å—Ç—Ä–∞—Ü–∏—è –∏–Ω–¥–µ–∫—Å–æ–≤ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ —Ñ–∞–π–ª–∞ ===
def register_file(file_name: str, md_text: str):
    fm, body = parse_frontmatter(md_text)
    doc_type = norm_topic(fm.get("doc_type"))
    topic = norm_topic(fm.get("topic") or doc_type)
    if topic not in CANON:
        topic = "clinic"  # fallback –Ω–∞ clinic

    aliases = fm.get("aliases") or []
    if isinstance(aliases, str): aliases = [aliases]
    mini_links = fm.get("mini_links") or []

    FILE_META[file_name] = {"topic": topic, "aliases": aliases, "mini_links": mini_links}

    # –≥–ª–æ–±–∞–ª—å–Ω—ã–µ –∞–ª–∏–∞—Å—ã ‚Üí —Ñ–∞–π–ª/—Ç–µ–º–∞
    for a in aliases:
        ALIAS_MAP_GLOBAL[norm_text(a)] = {"topic": topic, "file": file_name}

    # H2 –∏ –ª–æ–∫–∞–ª—å–Ω—ã–µ –∞–ª–∏–∞—Å—ã ‚Üí —Ç–æ—á–Ω—ã–π –∏–Ω–¥–µ–∫—Å
    for s in parse_h2_sections(body):
        # –∏–Ω–¥–µ–∫—Å–∏—Ä—É–µ–º –∑–∞–≥–æ–ª–æ–≤–æ–∫ –∏ h2_id
        for key in [s["title"], s["h2_id"], *s["local_aliases"]]:
            H2_INDEX[norm_text(key)] = {"topic": topic, "file": file_name, "h2_id": s["h2_id"]}

# === 5) DEFAULT_H2 (—Å—Ç—Ä–∞—Ö–æ–≤–∫–∞) ===
DEFAULT_H2 = {
  "consultation": ("consultation-free.md",         "–æ–±–∑–æ—Ä"),
  "prices":       ("prices-clinic.md",             None),
  "warranty":     ("warranty.md",                  "–æ–±–∑–æ—Ä"),
  "contacts":     ("clinic-contacts.md",           None),
  "implants":     ("implants-overview.md",         "–æ–±–∑–æ—Ä"),
  "safety":       ("implants-contraindications.md","–æ–±–∑–æ—Ä"),
  "doctors":      ("doctors.md",                   "–æ–±–∑–æ—Ä"),
  "clinic":       ("advantages-general.md",        "–æ–±–∑–æ—Ä")
}

def _find_chunk(file_name: str, h2_id: str|None):
    for ch in ALL_CHUNKS:
        if ch.file_name == file_name:
            if h2_id is None or getattr(ch.metadata, "h2_id", None) == h2_id:
                return ch
    return None

def get_default_chunk_for_topic(topic: str):
    file_name, h2_id = DEFAULT_H2.get(topic, (None, None))
    if file_name:
        ch = _find_chunk(file_name, h2_id)
        if ch: return ch, {"source":"default","topic":topic,"exact_h2_match":bool(h2_id)}
    # –∑–∞–ø–∞—Å–Ω–æ–π –ø—É—Ç—å ‚Äî –ø–µ—Ä–≤—ã–π —á–∞–Ω–∫ —ç—Ç–æ–π —Ç–µ–º—ã
    for ch in ALL_CHUNKS:
        if getattr(ch.metadata, "topic", None) == topic:
            return ch, {"source":"default-any","topic":topic,"exact_h2_match":False}
    return None, {}

def parse_yaml_front_matter(text: str):
    """–ü–∞—Ä—Å–∏—Ç YAML front matter –∏–∑ markdown —Ñ–∞–π–ª–∞"""
    pattern = r'^---\s*\n(.*?)\n---\s*\n'
    m = re.search(pattern, text, re.DOTALL | re.MULTILINE)
    
    if m:
        yaml_content = m.group(1)
        try:
            metadata = yaml.safe_load(yaml_content) or {}
        except yaml.YAMLError:
            metadata = {}
        # –ö–õ–Æ–ß–ï–í–ê–Ø –ü–†–ê–í–ö–ê: –≤—ã—Ä–µ–∑–∞–µ–º —Ç–æ–ª—å–∫–æ –ø–µ—Ä–≤—ã–π front-matter
        content = text[m.end():]
        return Frontmatter(metadata), content.strip()
    else:
        return Frontmatter({}), text

def chunk_text_by_sections(content: str, file_name: str) -> List[RetrievedChunk]:
    """–†–∞–∑–±–∏–≤–∞–µ—Ç —Ç–µ–∫—Å—Ç –Ω–∞ —á–∞–Ω–∫–∏ –ø–æ —Å–µ–∫—Ü–∏—è–º (## –∏ ###)"""
    chunks = []
    
    # –†–∞–∑—Ä–µ–∑–∞–µ–º –ù2 (## ...)
    h2_blocks = re.split(r'(?m)^\s*##\s+', content)
    
    for block in h2_blocks:
        block = block.strip()
        if not block:
            continue
        
        lines = block.splitlines()
        h2_title = lines[0]
        h2_body = '\n'.join(lines[1:])
        
        # –ü–∞—Ä—Å–∏–º H2-–∞–ª–∏–∞—Å—ã –∏–∑ HTML-–∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–µ–≤
        h2_aliases = []
        h2_id = None
        
        # –ò—â–µ–º {#id} –≤ –∑–∞–≥–æ–ª–æ–≤–∫–µ
        id_match = re.search(r'\{#([^}]+)\}', h2_title)
        if id_match:
            h2_id = id_match.group(1)
            h2_title = re.sub(r'\s*\{#[^}]+\}\s*', '', h2_title).strip()
        
        # –ò—â–µ–º –∞–ª–∏–∞—Å—ã –≤ –ø–µ—Ä–≤–æ–π —Å—Ç—Ä–æ–∫–µ –ø–æ—Å–ª–µ –∑–∞–≥–æ–ª–æ–≤–∫–∞
        if lines and len(lines) > 1:
            first_line = lines[1].strip()
            alias_match = re.search(r'<!--\s*aliases:\s*\[(.*?)\]\s*-->', first_line, re.IGNORECASE)
            if alias_match:
                aliases_str = alias_match.group(1)
                # –ü–∞—Ä—Å–∏–º –∞–ª–∏–∞—Å—ã –∏–∑ —Å—Ç—Ä–æ–∫–∏
                h2_aliases = [a.strip().strip('"\'') for a in re.split(r',\s*', aliases_str) if a.strip()]
        
        # –ï—Å–ª–∏ –≤–Ω—É—Ç—Ä–∏ –µ—Å—Ç—å –ù3 —Ä–µ–∂–µ–º –ø–æ –Ω–∏–º (### ...)
        h3_blocks = re.split(r'(?m)^\s*###\s+', block)
        if len(h3_blocks) > 1:
            # parts[0] ‚Äî —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ –¥–æ –ø–µ—Ä–≤–æ–≥–æ ### <- –≠–¢–û –ù–£–ñ–ù–û –°–û–•–†–ê–ù–ò–¢–¨!
            if h3_blocks[0].strip():
                preamble_text = f"## {h2_title}\n{h3_blocks[0].strip()}"
                chunk_id = f"{file_name}#{h2_title}_preamble"
                
                # –°–æ–∑–¥–∞–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ —Å H2-–∞–ª–∏–∞—Å–∞–º–∏
                temp_metadata = Frontmatter({
                    "h2_id": h2_id,
                    "h2_title": h2_title,
                    "h2_aliases": h2_aliases
                })
                
                # –ê–ª–∏–∞—Å—ã —Ç–æ–ª—å–∫–æ –¥–ª—è –ø–æ–∏—Å–∫–∞, –Ω–µ –≤ —Ç–µ–∫—Å—Ç–µ –æ—Ç–≤–µ—Ç–∞
                index_text = preamble_text
                chunks.append(RetrievedChunk(chunk_id, index_text.strip(), temp_metadata, file_name))
            
            # –¥–∞–ª—å—à–µ —Å–æ–∑–¥–∞—Ç—å —á–∞–Ω–∫–∏ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ ### –∫–∞–∫ —Å–µ–π—á–∞—Å
            for h3 in h3_blocks[1:]:
                h3 = h3.strip()
                if not h3:
                    continue
                
                lines = h3.splitlines()
                h3_title = lines[0]
                h3_body = '\n'.join(lines[1:])
                
                text = f"## {h2_title}\n### {h3_title}\n{h3_body}"
                chunk_id = f"{file_name}#{h2_title}_{h3_title}"
                
                # –°–æ–∑–¥–∞–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ —Å H2-–∞–ª–∏–∞—Å–∞–º–∏
                temp_metadata = Frontmatter({
                    "h2_id": h2_id,
                    "h2_title": h2_title,
                    "h2_aliases": h2_aliases
                })
                
                # –ê–ª–∏–∞—Å—ã —Ç–æ–ª—å–∫–æ –¥–ª—è –ø–æ–∏—Å–∫–∞, –Ω–µ –≤ —Ç–µ–∫—Å—Ç–µ –æ—Ç–≤–µ—Ç–∞
                index_text = text
                chunks.append(RetrievedChunk(chunk_id, index_text.strip(), temp_metadata, file_name))
        else:
            # –ù3 –Ω–µ—Ç - —Å–æ—Ö—Ä–∞–Ω—è–µ–º –ù2 –∫–∞–∫ –µ–¥–∏–Ω—ã–π —á–∞–Ω–∫
            text = f"## {h2_title}\n{h2_body}"
            chunk_id = f"{file_name}#{h2_title}"
            
            # –°–æ–∑–¥–∞–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ —Å H2-–∞–ª–∏–∞—Å–∞–º–∏
            temp_metadata = Frontmatter({
                "h2_id": h2_id,
                "h2_title": h2_title,
                "h2_aliases": h2_aliases
            })
            
            # –ê–ª–∏–∞—Å—ã —Ç–æ–ª—å–∫–æ –¥–ª—è –ø–æ–∏—Å–∫–∞, –Ω–µ –≤ —Ç–µ–∫—Å—Ç–µ –æ—Ç–≤–µ—Ç–∞
            index_text = text
            chunks.append(RetrievedChunk(chunk_id, index_text.strip(), temp_metadata, file_name))
    
    return chunks

def extract_aliases_from_chunk(chunk_text: str) -> List[str]:
    """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –∞–ª–∏–∞—Å—ã –∏–∑ HTML-–∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–µ–≤ –≤ —á–∞–Ω–∫–µ"""
    m = re.search(r'<!--\s*aliases:\s*\[(.*?)\]\s*-->', chunk_text, re.S)
    if not m:
        return []
    raw = m.group(1)
    pairs = re.findall(r'"([^"]+)"|\'([^\']+)\'', raw)
    aliases = [a or b for a, b in pairs]
    return [a.strip() for a in aliases if a and a.strip()]

def update_entity_index(chunk: RetrievedChunk, topic: str, entity_key: str):
    """–û–±–Ω–æ–≤–ª—è–µ—Ç ENTITY_INDEX —Å –∞–ª–∏–∞—Å–∞–º–∏ –∏–∑ —á–∞–Ω–∫–∞"""
    # –ò–∑–≤–ª–µ–∫–∞–µ–º –∑–∞–≥–æ–ª–æ–≤–æ–∫ ##
    header_match = re.search(r'(?m)^##\s+(.+?)\s*$', chunk.text)
    title = header_match.group(1).strip() if header_match else ""
    
    # –ê–ª–∏–∞—Å—ã: HTML-–∫–æ–º–º–µ–Ω—Ç –≤ —Ç–µ–∫—Å—Ç–µ + —Ñ—Ä–æ–Ω—Ç–º–∞—Ç—Ç–µ—Ä
    aliases = extract_aliases_from_chunk(chunk.text)
    if hasattr(chunk, "metadata") and getattr(chunk.metadata, "aliases", None):
        aliases.extend(chunk.metadata.aliases)
    if title:
        aliases.append(title)
    
    # –ò–Ω–¥–µ–∫—Å–∏—Ä—É–µ–º –≤—Å–µ –∞–ª–∏–∞—Å—ã
    for alias in aliases:
        alias_norm = _norm(alias)
        if alias_norm:
            ENTITY_INDEX[alias_norm] = {
                "topic": topic,
                "entity": entity_key,
                "doc_id": chunk.file_name,
                "section": title
            }

# –ó–∞–≥—Ä—É–∂–∞–µ–º –∏ —Ä–∞–∑–±–∏–≤–∞–µ–º –≤—Å–µ markdown-—Ñ–∞–π–ª—ã
folder_path = Path("md/")
all_chunks: List[RetrievedChunk] = []

# –ì–ª–æ–±–∞–ª—å–Ω—ã–µ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –¥–ª—è –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–æ–≥–æ –ø–æ–∏—Å–∫–∞ –≤—Ä–∞—á–µ–π
DOCTOR_NAME_TOKENS: set[str] = set()
# –ü—Ä—è–º–∞—è –∞–¥—Ä–µ—Å–∞—Ü–∏—è –≤—Ä–∞—á–µ–π: –∏–º—è -> —á–∞–Ω–∫
DOCTOR_NAME_TO_CHUNK: dict[str, RetrievedChunk] = {}
DOCTOR_REGEX = None
DOCTOR_NAME_REGEX = None  # —Ç–æ–ª—å–∫–æ –∏–º–µ–Ω–∞
DOCTOR_QUERY_REGEX = None  # –∏–º–µ–Ω–∞ + —Å–ª–æ–≤–∞ "–≤—Ä–∞—á/–¥–æ–∫—Ç–æ—Ä/..."

# ==== THEMES (—É—Å–∏–ª–µ–Ω–∏–µ –ø–æ —Ç–µ–º–∞–º) ====
try:
    from pathlib import Path
    import json, re
    
    BASE_DIR = Path(__file__).resolve().parent
    THEMES_PATH = BASE_DIR / "config" / "themes.json"
    
    with open(THEMES_PATH, "r", encoding="utf-8") as f:
        THEME_MAP = json.load(f)
    
    # –∫–æ–º–ø–∏–ª—è—Ü–∏—è —Ä–µ–≥–µ–∫—Å–æ–≤ –∏ –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è
    for k, cfg in THEME_MAP.items():
        cfg["query_regex_compiled"] = re.compile(cfg["query_regex"], re.IGNORECASE)
        cfg["tag_aliases"] = [str(t).lower() for t in cfg.get("tag_aliases", [])]
        cfg["weight"] = float(cfg.get("weight", 0.2))
    print(f"OK: themes.json loaded: {len(THEME_MAP)} themes from {THEMES_PATH}")
    for theme, cfg in THEME_MAP.items():
        print(f"  Theme: {theme}: weight={cfg.get('weight', 'N/A')}, regex={cfg.get('query_regex', 'N/A')[:30]}...")
except Exception as e:
    print(f"WARNING: –ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å themes.json: {e}")
    THEME_MAP = {}

def _extract_doctor_names_from_text(text: str) -> list[str]:
    """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –∏–º–µ–Ω–∞ –≤—Ä–∞—á–µ–π –∏–∑ —Ç–µ–∫—Å—Ç–∞"""
    names = set()
    
    for line in text.split('\n'):
        line = line.strip()
        if not line:
            continue
            
        # —Å—Ä–µ–∑–∞—Ç—å –ø—Ä–µ—Ñ–∏–∫—Å ### / ##
        if line.startswith('### '):
            line = line[4:]
        elif line.startswith('## '):
            line = line[3:]
            
        # –ò–º—è –§–∞–º–∏–ª–∏—è [–û—Ç—á–µ—Å—Ç–≤–æ]
        match = re.match(r'^[–ê-–Ø–Å][–∞-—è—ë]+(?:\s+[–ê-–Ø–Å][–∞-—è—ë]+){1,2}$', line)
        if match:
            full = match.group(0)
            parts = full.split()
            if len(parts) >= 2:
                names.add(full)  # –ø–æ–ª–Ω–æ–µ –§–ò–û
                names.add(parts[0])  # —Ñ–∞–º–∏–ª–∏—è
                names.add(f"{parts[0]} {parts[1]}")  # –§–∞–º–∏–ª–∏—è –ò–º—è
                names.add(f"{parts[1]} {parts[0]}")  # –ò–º—è –§–∞–º–∏–ª–∏—è
                print(f"    üè• –ò–∑–≤–ª–µ—á–µ–Ω–æ –∏–º—è: '{full}' ‚Üí {parts[0]}, {parts[1]} {parts[0]}")
    
    return sorted(list(names))

def _rebuild_doctor_regex():
    """–ü–µ—Ä–µ—Å–æ–±–∏—Ä–∞–µ—Ç regex –¥–ª—è –ø–æ–∏—Å–∫–∞ –≤—Ä–∞—á–µ–π"""
    global DOCTOR_REGEX, DOCTOR_NAME_REGEX, DOCTOR_QUERY_REGEX
    
    base = ['–≤—Ä–∞—á', '–¥–æ–∫—Ç–æ—Ä', '—Ö–∏—Ä—É—Ä–≥', '–∏–º–ø–ª–∞–Ω—Ç–æ–ª–æ–≥', '—Å–ø–µ—Ü–∏–∞–ª–∏—Å—Ç']
    name_tokens = sorted([re.escape(t) for t in DOCTOR_NAME_TOKENS], key=len, reverse=True)
    base_tokens = [re.escape(t) for t in base]

    if name_tokens:
        DOCTOR_NAME_REGEX = re.compile(r'(' + '|'.join(name_tokens) + r')', re.IGNORECASE)
        DOCTOR_QUERY_REGEX = re.compile(r'(' + '|'.join(name_tokens + base_tokens) + r')', re.IGNORECASE)
    else:
        DOCTOR_NAME_REGEX = None
        DOCTOR_QUERY_REGEX = re.compile(r'(' + '|'.join(base_tokens) + r')', re.IGNORECASE)
    
    # –û—Å—Ç–∞–≤–ª—è–µ–º —Å—Ç–∞—Ä—ã–π DOCTOR_REGEX –¥–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏
    DOCTOR_REGEX = DOCTOR_QUERY_REGEX

def build_empathy_prompt(tone: str = "friendly", emotion: str = "empathy", allow_emoji: bool = True, cta_text: str | None = None, cta_link: str | None = None) -> str:
    """–°–æ–±–∏—Ä–∞–µ—Ç –∫–æ—Ä–æ—Ç–∫–∏–π –ø—Ä–æ–º–ø—Ç –¥–ª—è ¬´–æ–∂–∏–≤–ª–µ–Ω–∏—è¬ª –æ—Ç–≤–µ—Ç–∞ –±–µ–∑ –∏—Å–∫–∞–∂–µ–Ω–∏—è —Ñ–∞–∫—Ç–æ–≤."""
    
    emoji_rule = "–ò–Ω–æ–≥–¥–∞ (–Ω–µ —á–∞—â–µ –æ–¥–Ω–æ–≥–æ-–¥–≤—É—Ö –Ω–∞ –≤–µ—Å—å –æ—Ç–≤–µ—Ç) –¥–æ–±–∞–≤–ª—è–π —É–º–µ—Å—Ç–Ω—ã–µ —ç–º–æ–¥–∑–∏ ü¶∑üòåüìÖüí¨. –ï—Å–ª–∏ –Ω–µ—É–º–µ—Å—Ç–Ω–æ ‚Äî –Ω–µ –¥–æ–±–∞–≤–ª—è–π." if allow_emoji else "–ù–µ –∏—Å–ø–æ–ª—å–∑—É–π —ç–º–æ–¥–∑–∏."
    
    cta_rule = ""
    if cta_text:
        cta_rule = f"–ó–∞–≤–µ—Ä—à–∏ –º—è–≥–∫–æ–π —Ñ—Ä–∞–∑–æ–π-–ø—Ä–∏–≥–ª–∞—à–µ–Ω–∏–µ–º: ¬´{cta_text}¬ª"
        if cta_link:
            cta_rule += f" (—Å—Å—ã–ª–∫–∞: {cta_link})."
    
    prompt = dedent(f"""
    –¢—ã - –≤–Ω–∏–º–∞—Ç–µ–ª—å–Ω—ã–π –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç —Å—Ç–æ–º–∞—Ç–æ–ª–æ–≥–∏—á–µ—Å–∫–æ–π –∫–ª–∏–Ω–∏–∫–∏.
    –¢–æ–Ω: {tone}, —ç–º–æ—Ü–∏—è: {emotion}. –ü–∏—à–∏ –ø—Ä–æ—Å—Ç–æ –∏ –ø–æ-—á–µ–ª–æ–≤–µ—á–µ—Å–∫–∏.
    
    –ó–∞–¥–∞—á–∞:
    1. –í–æ–∑—å–º–∏ —Ç–µ–∫—Å—Ç –∏ –ø–µ—Ä–µ—Ñ–æ—Ä–º—É–ª–∏—Ä—É–π —Ç–µ–ø–ª–µ–µ, –Ω–æ –°–û–•–†–ê–ù–ò –í–°–ï –ß–ò–°–õ–ê –ò –ü–†–û–¶–ï–ù–¢–´ –ë–ï–ó –ò–ó–ú–ï–ù–ï–ù–ò–ô (–Ω–∞–ø—Ä–∏–º–µ—Ä: 99,8%, 0,2%, 97-98%).
    2. –ù–∏—á–µ–≥–æ –Ω–µ –≤—ã–¥—É–º—ã–≤–∞–π ‚Äî —Ç–æ–ª—å–∫–æ —Ñ–∞–∫—Ç—ã –∏–∑ —Ç–µ–∫—Å—Ç–∞.
    3. –°—Ç—Ä—É–∫—Ç—É—Ä–∞: –∫–æ—Ä–æ—Ç–∫–æ–µ –≤—Å—Ç—É–ø–ª–µ–Ω–∏–µ -> –ø–æ–ª–µ–∑–Ω—ã–µ –¥–µ—Ç–∞–ª–∏ (–∞–±–∑–∞—Ü—ã –∏–ª–∏ —Å–ø–∏—Å–æ–∫) -> –º—è–≥–∫–æ–µ –∑–∞–≤–µ—Ä—à–µ–Ω–∏–µ.
    4. –ë–µ–∑ —Å–ª—É–∂–µ–±–Ω—ã—Ö –ø–æ–º–µ—Ç–æ–∫ –∏ –∑–∞–≥–æ–ª–æ–≤–∫–æ–≤ ##, –±–µ–∑ HTML. –†–∞–∑–¥–µ–ª—è–π –∞–±–∑–∞—Ü—ã –ø—É—Å—Ç–æ–π —Å—Ç—Ä–æ–∫–æ–π; —Å–ø–∏—Å–∫–∏ ‚Äî –¥–µ—Ñ–∏—Å
    5. {emoji_rule}
    6. {cta_rule}
    
    –û—Ç–≤–µ—á–∞–π —Ç–æ–ª—å–∫–æ —Ç–µ–∫—Å—Ç–æ–º.
    """).strip()
    
    return prompt

# Back-compat —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è —Å—Ç–∞—Ä–æ–≥–æ –∫–æ–¥–∞
from core.answer_builder import postprocess, LOW_REL_JSON

def postprocess_answer_with_empathy(answer_text, user_text, intent, topic_meta, session):
    """Back-compat: –∏—Å–ø–æ–ª—å–∑—É–µ–º –Ω–æ–≤—ã–π –ø–æ—Å—Ç–ø—Ä–æ—Ü–µ—Å—Å–æ—Ä."""
    return postprocess(answer_text, user_text, intent, topic_meta, session)

def theme_boost(score: float, theme_key: str, cfg: dict, chunk) -> float:
    # –±—É—Å—Ç–∏–º, –µ—Å–ª–∏ –≤ —Ç–µ–≥–∞—Ö –∏–ª–∏ —Ç–µ–∫—Å—Ç–µ –µ—Å—Ç—å —Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏–µ –∞–ª–∏–∞—Å—ã
    text_l = chunk.text.lower()
    tags_l = getattr(chunk.metadata, 'tags_lower', [])
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ç–µ–≥–∏
    tag_match = any(alias in tags_l for alias in cfg["tag_aliases"])
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ç–µ–∫—Å—Ç
    text_match = any(alias in text_l for alias in cfg["tag_aliases"])
    
    if tag_match or text_match:
        boosted_score = score + cfg["weight"]
        print(f"      üéØ –ë—É—Å—Ç {theme_key}: —Ç–µ–≥–∏={tags_l}, —Ç–µ–∫—Å—Ç —Å–æ–¥–µ—Ä–∂–∏—Ç={[alias for alias in cfg['tag_aliases'] if alias in text_l]}")
        return boosted_score
    return score

# –£—Ç–∏–ª–∏—Ç—ã –¥–ª—è –ø–æ–∏—Å–∫–∞ —Ü–∏—Ñ—Ä –∏ –ø—Ä–æ—Ü–µ–Ω—Ç–æ–≤
PCT_RE = re.compile(r'\b\d{1,3}(?:[.,]\d{1,2})?\s?%')
NUM_RE = re.compile(r'\b\d{1,5}\b')

def has_numeric_facts(text: str) -> bool:
    """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç –Ω–∞–ª–∏—á–∏–µ —Ü–∏—Ñ—Ä –∏–ª–∏ –ø—Ä–æ—Ü–µ–Ω—Ç–æ–≤ –≤ —Ç–µ–∫—Å—Ç–µ"""
    text_l = text.lower()
    return bool(PCT_RE.search(text_l) or NUM_RE.search(text_l))

def _find_doctor_direct_or_fuzzy(query: str):
    q = (query or "").lower().replace("\u00a0", "")
    # 1) –ø—Ä—è–º–æ–µ –ø–æ–¥—Å—Ç—Ä–æ—á–Ω–æ–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ –ø–æ –∫–ª—é—á–∞–º
    for k, ch in DOCTOR_NAME_TO_CHUNK.items():
        if k in q:
            return ch
    # 2) –º–æ—Ä—Ñ–æ–ª–æ–≥–∏—á–µ—Å–∫–∏ ¬´–º—è–≥–∫–æ¬ª –ø–æ —Ñ–∞–º–∏–ª–∏–∏: –ú–æ–∏—Å–µ–µ–≤/–ú–æ–∏—Å–µ–µ–≤–∞/–ú–æ–∏—Å–µ–µ–≤—É...
    for k, ch in DOCTOR_NAME_TO_CHUNK.items():
        if " " not in k: # —Ç–æ–ª—å–∫–æ –æ–¥–Ω–æ—Å–ª–æ–≤–Ω—ã–µ –∫–ª—é—á–∏ = —Ñ–∞–º–∏–ª–∏–∏
            if re.search(rf"\b{k}\w*\b", q, flags=re.IGNORECASE):
                return ch
    # 3) –æ–ø–µ—á–∞—Ç–∫–∏
    tokens = re.findall(r"[–∞-—è—ë]{4,}", q)
    keys = list(DOCTOR_NAME_TO_CHUNK.keys())
    for t in tokens:
        best = difflib.get_close_matches(t, keys, n=1, cutoff=0.84)
        if best:
            return DOCTOR_NAME_TO_CHUNK[best[0]]
    return None

def fallback_theme_chunks(theme_key: str, limit: int = 3):
    """–ï—Å–ª–∏ —Å–µ–º–∞–Ω—Ç–∏–∫–∞ –ø—Ä–æ–º–∞—Ö–Ω—É–ª–∞—Å—å - –≤–µ—Ä–Ω—É—Ç—å –Ω–µ—Å–∫–æ–ª—å–∫–æ —è–≤–Ω—ã—Ö —Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏—Ö —á–∞–Ω–∫–æ–≤."""
    if not theme_key:
        return []
    
    cfg = THEME_MAP[theme_key]
    out = []
    
    print(f"üîç Fallback –¥–ª—è —Ç–µ–º—ã '{theme_key}': –∏—â–µ–º —Ç–µ–≥–∏ {cfg['tag_aliases']}")
    
    for ch in ALL_CHUNKS:
        tags_l = getattr(ch.metadata, 'tags_lower', [])
        text_l = ch.text.lower()
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ç–µ–≥–∏
        tag_match = any(alias in (tags_l or []) for alias in cfg["tag_aliases"])
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ç–µ–∫—Å—Ç
        text_match = any(alias in text_l for alias in cfg["tag_aliases"])
        
        if tag_match or text_match:
            out.append(ch)
            print(f"  ‚úÖ –ù–∞–π–¥–µ–Ω —á–∞–Ω–∫: {ch.file_name} (—Ç–µ–≥–∏: {tags_l}, —Ç–µ–∫—Å—Ç —Å–æ–¥–µ—Ä–∂–∏—Ç: {[alias for alias in cfg['tag_aliases'] if alias in text_l]})")
            if len(out) >= limit:
                break
    
    print(f"üîç Fallback –≤–µ—Ä–Ω—É–ª {len(out)} —á–∞–Ω–∫–æ–≤")
    return out

def route_topics(query: str) -> Set[str]:
    """–†–æ—É—Ç–µ—Ä –ø–æ —Ç–µ–º–∞–º - –æ–ø—Ä–µ–¥–µ–ª—è–µ—Ç —Ç–µ–º—ã –∑–∞–ø—Ä–æ—Å–∞"""
    if not hasattr(route_topics, 'theme_map'):
        try:
            with open(THEMES_PATH, "r", encoding="utf-8") as f:
                route_topics.theme_map = json.load(f)
        except Exception as e:
            print(f"‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å themes.json: {e}")
            route_topics.theme_map = {}
    
    detected_topics = set()
    query_lower = query.lower()
    
    for topic, config in route_topics.theme_map.items():
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º regex
        if "query_regex" in config:
            try:
                pattern = re.compile(config["query_regex"], re.IGNORECASE)
                if pattern.search(query):
                    detected_topics.add(topic)
                    continue
            except Exception:
                pass
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º tag_aliases
        if "tag_aliases" in config:
            for alias in config["tag_aliases"]:
                if alias.lower() in query_lower:
                    detected_topics.add(topic)
                    break
    
    return detected_topics

def strip_fluff_start(text: str) -> str:
    """–£–¥–∞–ª—è–µ—Ç –≤—Å—Ç—É–ø–∏—Ç–µ–ª—å–Ω—É—é '–≤–æ–¥—É-–∞–±–∑–∞—Ü' –∏–∑ —Ç–µ–∫—Å—Ç–∞"""
    lines = text.split('\n')
    start_removed = False
    
    for i, line in enumerate(lines):
        line_stripped = line.strip()
        if not line_stripped:
            continue
            
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –≤—Å–µ –∞–Ω—Ç–∏-—Ñ–ª—É—Ñ—Ñ –ø–∞—Ç—Ç–µ—Ä–Ω—ã
        for pattern in ANTI_FLUFF:
            if re.match(pattern, line_stripped, re.IGNORECASE):
                # –£–¥–∞–ª—è–µ–º —ç—Ç—É —Å—Ç—Ä–æ–∫—É –∏ –≤—Å–µ –ø—É—Å—Ç—ã–µ —Å—Ç—Ä–æ–∫–∏ –ø–æ—Å–ª–µ –Ω–µ—ë
                lines = lines[i+1:]
                start_removed = True
                break
        
        if start_removed:
            break
    
    # –£–±–∏—Ä–∞–µ–º –ø—É—Å—Ç—ã–µ —Å—Ç—Ä–æ–∫–∏ –≤ –Ω–∞—á–∞–ª–µ
    while lines and not lines[0].strip():
        lines = lines[1:]
    
    return '\n'.join(lines)

try:
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ –ø–∞–ø–∫–∏ md
    if not folder_path.exists():
        print(f"ERROR: –ü–∞–ø–∫–∞ {folder_path} –ù–ï –Ω–∞–π–¥–µ–Ω–∞!")
        raise Exception(f"–ü–∞–ø–∫–∞ {folder_path} –Ω–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç")
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ doctors.md
    doctors_file = Path("md/doctors.md")
    if doctors_file.exists():
        print(f"OK: –§–∞–π–ª doctors.md –Ω–∞–π–¥–µ–Ω")
    else:
        print(f"ERROR: –§–∞–π–ª doctors.md –ù–ï –Ω–∞–π–¥–µ–Ω!")
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ themes.json
    themes_file = Path("config/themes.json")
    if themes_file.exists():
        print(f"OK: –§–∞–π–ª config/themes.json –Ω–∞–π–¥–µ–Ω")
    else:
        print(f"ERROR: –§–∞–π–ª config/themes.json –ù–ï –Ω–∞–π–¥–µ–Ω!")
    
    # –ü–æ–¥–∫–ª—é—á–∞–µ–º —Ñ–∏–ª—å—Ç—Ä –¥–ª—è —Å–ª—É–∂–µ–±–Ω—ã—Ö MD —Ñ–∞–π–ª–æ–≤
    from core.md_filter import is_index_like
    import json, logging
    log_m = logging.getLogger("cesi.minimal_logs")
    
    all_md_files = list(folder_path.rglob("*.md"))
    skipped = 0
    docs = []
    
    for file in all_md_files:
        try:
            print(f"üìÑ –û–±—Ä–∞–±–∞—Ç—ã–≤–∞—é —Ñ–∞–π–ª: {file}")
            text = file.read_text(encoding="utf-8")
            
            # –§–∏–ª—å—Ç—Ä—É–µ–º —Å–ª—É–∂–µ–±–Ω—ã–µ —Ñ–∞–π–ª—ã
            if is_index_like(file, text):
                skipped += 1
                print(f"  ‚è≠Ô∏è –ü—Ä–æ–ø—É—Å–∫–∞–µ–º —Å–ª—É–∂–µ–±–Ω—ã–π —Ñ–∞–π–ª: {file.name}")
                continue
            
            print(f"  üìñ –ü—Ä–æ—á–∏—Ç–∞–Ω —Ñ–∞–π–ª: {file.name} ({len(text)} —Å–∏–º–≤–æ–ª–æ–≤)")
            
            # –†–µ–≥–∏—Å—Ç—Ä–∏—Ä—É–µ–º —Ñ–∞–π–ª –≤ –Ω–æ–≤—ã—Ö –∏–Ω–¥–µ–∫—Å–∞—Ö
            register_file(file.name, text)
            
            # –ü–∞—Ä—Å–∏–º YAML front matter
            metadata, content = parse_yaml_front_matter(text)
            content = _normalize(content) # –Ω–æ—Ä–º–∞–ª–∏–∑—É–µ–º –ø—Ä–æ–±–µ–ª—ã
            print(f"  ‚úÖ YAML –ø–∞—Ä—Å–∏–Ω–≥: {metadata.id if metadata.id else '–±–µ–∑ ID'}")
            
            # –†–µ–≥–∏—Å—Ç—Ä–∏—Ä—É–µ–º –∞–ª–∏–∞—Å—ã –¥–ª—è fallback –ø–æ–∏—Å–∫–∞
            try:
                from core.md_loader import register_aliases
                # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º metadata –≤ dict –¥–ª—è register_aliases
                frontmatter_dict = {
                    "aliases": getattr(metadata, 'aliases', []),
                    "primary_h2_id": getattr(metadata, 'primary_h2_id', None)
                }
                register_aliases(frontmatter_dict, str(file))
            except Exception as e:
                print(f"  ‚ö†Ô∏è –û—à–∏–±–∫–∞ —Ä–µ–≥–∏—Å—Ç—Ä–∞—Ü–∏–∏ –∞–ª–∏–∞—Å–æ–≤: {e}")
            
            # –õ–æ–∫–∞–ª—å–Ω–∞—è —Ä–µ–≥–∏—Å—Ç—Ä–∞—Ü–∏—è –∞–ª–∏–∞—Å–æ–≤
            for a in (getattr(metadata, 'aliases', ()) or []):
                ALIAS_MAP[_norm(a)] = {"file": str(file), "primary_h2_id": getattr(metadata, 'primary_h2_id', None)}
            
            # –µ—Å–ª–∏ —ç—Ç–æ —Ñ–∞–π–ª —Å –≤—Ä–∞—á–∞–º–∏ - —Å–æ–±—Ä–∞—Ç—å –∏–º–µ–Ω–∞
            if getattr(metadata, 'doc_type', '') in ('doctor', 'doctors') or file.name == "doctors.md":
                found = _extract_doctor_names_from_text(content)
                if found:
                    DOCTOR_NAME_TOKENS.update(found)
                    print(f"  üè• –ù–∞–π–¥–µ–Ω—ã –≤—Ä–∞—á–∏ –≤ {file.name}: {found}")
            
            # –†–∞–∑–±–∏–≤–∞–µ–º –Ω–∞ —á–∞–Ω–∫–∏ –ø–æ —Å–µ–∫—Ü–∏—è–º
            file_chunks = chunk_text_by_sections(content, file.name)
            print(f"  üìù –°–æ–∑–¥–∞–Ω–æ —á–∞–Ω–∫–æ–≤: {len(file_chunks)}")
            
            # ==== –ò–ù–î–ï–ö–°–ê–¶–ò–Ø –ö–ê–¢–ê–õ–û–ì–ê –ò–ú–ü–õ–ê–ù–¢–û–í ====
            if metadata.doc_type == "catalog" and metadata.topic == "implants":
                print(f"  üè∑Ô∏è –ò–Ω–¥–µ–∫—Å–∏—Ä—É–µ–º –∫–∞—Ç–∞–ª–æ–≥ –∏–º–ø–ª–∞–Ω—Ç–æ–≤ –∏–∑ {file.name}")
                for ch in file_chunks:
                    # –ò—â–µ–º —Å–µ–∫—Ü–∏–∏ –ø–æ ### –ó–∞–≥–æ–ª–æ–≤–æ–∫
                    m = re.search(r'(?m)^###\s+(.+?)\s*$', ch.text)
                    if not m:
                        continue
                    
                    name = m.group(1).strip()
                    
                    # –ü–∞—Ä—Å–∏–º –∞–ª–∏–∞—Å—ã –∏–∑ HTML-–∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–µ–≤
                    alias_m = re.search(r'<!--\s*aliases:\s*\[(.*?)\]\s*-->', ch.text)
                    aliases = [name]
                    if alias_m:
                        aliases.extend([a.strip() for a in alias_m.group(1).split(',')])
                    
                    # –°–æ–∑–¥–∞–µ–º entity_key
                    entity_key = _slugify_implant_kind(name)
                    print(f"    üìã –°–µ–∫—Ü–∏—è: '{name}' ‚Üí '{entity_key}' (–∞–ª–∏–∞—Å—ã: {aliases})")
                    
                    # –°–æ—Ö—Ä–∞–Ω—è–µ–º —á–∞–Ω–∫
                    ENTITY_CHUNKS[("implants", entity_key)] = ch
                    
                    # –ò–Ω–¥–µ–∫—Å–∏—Ä—É–µ–º –∞–ª–∏–∞—Å—ã
                    for a in aliases:
                        ENTITY_INDEX[_norm(a)] = {
                            "topic": "implants", 
                            "entity": entity_key, 
                            "doc_id": metadata.id or file.name, 
                            "section": name
                        }
            
            # –ü—Ä–∏–≤—è–∑—ã–≤–∞–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –∫ –∫–∞–∂–¥–æ–º—É —á–∞–Ω–∫—É
            for chunk in file_chunks:
                chunk.metadata = metadata
                
                # –û–±–Ω–æ–≤–ª—è–µ–º ENTITY_INDEX –¥–ª—è –≤—Å–µ—Ö —á–∞–Ω–∫–æ–≤
                if metadata.topic:
                    update_entity_index(chunk, metadata.topic, chunk.id)
                
                # –ø–µ—Ä–µ–¥ append(chunk) - –Ω–æ—Ä–º–∞–ª–∏–∑—É–π —Ç–µ–≥–∏ –≤ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö
                try:
                    if hasattr(chunk.metadata, "tags") and isinstance(chunk.metadata.tags, list):
                        chunk.metadata.tags_lower = [str(t).strip().lower() for t in chunk.metadata.tags]
                    else:
                        chunk.metadata.tags_lower = []
                except Exception:
                    chunk.metadata.tags_lower = []
                
                all_chunks.append(chunk)
                
                # –†–µ–≥–∏—Å—Ç—Ä–∏—Ä—É–µ–º –≤—Å–µ —á–∞–Ω–∫–∏ –≤ ENTITY_CHUNKS (–Ω–µ —Ç–æ–ª—å–∫–æ –∏–º–ø–ª–∞–Ω—Ç—ã)
                ENTITY_CHUNKS[(chunk.metadata.topic or "general", chunk.id)] = chunk
                
                # –ï—Å–ª–∏ —ç—Ç–æ –∫–∞—Ä—Ç–æ—á–∫–∞ –≤—Ä–∞—á–∞ (–ø–æ–¥–∑–∞–≥–æ–ª–æ–≤–æ–∫ ## –∏–ª–∏ ### –ò–º—è –§–∞–º–∏–ª–∏—è [–û—Ç—á–µ—Å—Ç–≤–æ]) ‚Äî –∑–∞–ø–æ–º–∏–Ω–∞–µ–º –ø—Ä—è–º—É—é —Å—Å—ã–ª–∫—É
                hdr = re.search(r'(?m)^#{2,3}\s+([–ê-–Ø–Å][–∞-—è—ë]+(?:\s+[–ê-–Ø–Å][–∞-—è—ë]+){1,2})\s*$', chunk.text)
                if hdr:
                    full = hdr.group(1).strip()
                    parts = full.split()
                    last = parts[0] # –§–∞–º–∏–ª–∏—è
                    first = parts[1] if len(parts) > 1 else ""
                    patr = parts[2] if len(parts) > 2 else ""
                    
                    # –ö–ª—é—á–∏, –ø–æ –∫–æ—Ç–æ—Ä—ã–º —Ä–µ–∞–ª—å–Ω–æ —Å–ø—Ä–∞—à–∏–≤–∞—é—Ç
                    keys = {
                        full,                               # "–ú–æ–∏—Å–µ–µ–≤ –ö–∏—Ä–∏–ª–ª –ù–∏–∫–æ–ª–∞–µ–≤–∏—á"
                        last,                               # "–ú–æ–∏—Å–µ–µ–≤"
                        (f"{last} {first}").strip(),        # "–ú–æ–∏—Å–µ–µ–≤ –ö–∏—Ä–∏–ª–ª"
                        (f"{first} {last}").strip(),        # "–ö–∏—Ä–∏–ª–ª –ú–æ–∏—Å–µ–µ–≤"
                    }
                    
                    for k in keys:
                        DOCTOR_NAME_TO_CHUNK[k.lower()] = chunk
                    
                    # –î–ª—è —Ä–µ–≥—ç–∫—Å–ø–∞/–ø–æ–¥—Å–≤–µ—Ç–∫–∏ - –±–µ–∑ –æ—Ç—á–µ—Å—Ç–≤, —á—Ç–æ–±—ã –Ω–µ –∑–∞—Å–æ—Ä—è—Ç—å
                    DOCTOR_NAME_TOKENS.update({full, last, f"{first} {last}".strip(), f"{last} {first}".strip()})
                    print(f"üîó –ö–∞—Ä—Ç–æ—á–∫–∞ –≤—Ä–∞—á–∞: {full} ‚Üí {getattr(chunk, 'section', chunk.file_name)}; –∫–ª—é—á–∏: {sorted(keys)}")
            
            print(f"  ‚úÖ –§–∞–π–ª –æ–±—Ä–∞–±–æ—Ç–∞–Ω —É—Å–ø–µ—à–Ω–æ")
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ —Ñ–∞–π–ª–∞ {file}: {e}")
            import traceback
            traceback.print_exc()
            continue
    
    # –õ–æ–≥–∏—Ä—É–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏
    log_m.info(json.dumps({"ev":"filter_index_like","skipped":skipped,"total":len(all_md_files)}, ensure_ascii=False))
    
    print(f"\u23f3 –ù–∞–π–¥–µ–Ω–æ {len(all_chunks)} —á–∞–Ω–∫–æ–≤")
    
    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º ALL_CHUNKS –¥–ª—è –Ω–æ–≤—ã—Ö –∏–Ω–¥–µ–∫—Å–æ–≤
    ALL_CHUNKS.extend(all_chunks)
    print(f"‚úÖ ALL_CHUNKS –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω: {len(ALL_CHUNKS)} —á–∞–Ω–∫–æ–≤")
    print(f"‚úÖ ALIAS_MAP_GLOBAL: {len(ALIAS_MAP_GLOBAL)} –∞–ª–∏–∞—Å–æ–≤")
    print(f"‚úÖ H2_INDEX: {len(H2_INDEX)} –∑–∞–≥–æ–ª–æ–≤–∫–æ–≤")
    print(f"‚úÖ FILE_META: {len(FILE_META)} —Ñ–∞–π–ª–æ–≤")
    
    # –õ–æ–≥–∏—Ä—É–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –∏–Ω–¥–µ–∫—Å–æ–≤
    print(f"üìä INDEX STATS: docs={len(FILE_META)}, chunks={len(ALL_CHUNKS)}, aliases={len(ALIAS_MAP_GLOBAL)}, h2s={len(H2_INDEX)}")
    
    # –°–æ–∑–¥–∞–µ–º BM25 –∏–Ω–¥–µ–∫—Å
    if ALL_CHUNKS:
        print(f"üîç –°–æ–∑–¥–∞–µ–º BM25 –∏–Ω–¥–µ–∫—Å –¥–ª—è {len(ALL_CHUNKS)} —á–∞–Ω–∫–æ–≤...")
        bm25_corpus = []
        for chunk in ALL_CHUNKS:
            # –¢–æ–∫–µ–Ω–∏–∑–∏—Ä—É–µ–º —Ç–µ–∫—Å—Ç + –∞–ª–∏–∞—Å—ã –¥–ª—è BM25
            alias_boost = " ".join(getattr(chunk.metadata, 'h2_aliases', ()) or [])
            tokens = re.findall(r'\w+', (chunk.text + " " + alias_boost).lower())
            bm25_corpus.append(tokens)
        
        bm25_index = BM25Okapi(bm25_corpus)
        print(f"‚úÖ BM25 –∏–Ω–¥–µ–∫—Å —Å–æ–∑–¥–∞–Ω")
    
    # –û—Ç–ª–∞–¥–æ—á–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ —á–∞–Ω–∫–∞—Ö
    for chunk in ALL_CHUNKS[:5]:  # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –ø–µ—Ä–≤—ã–µ 5 —á–∞–Ω–∫–æ–≤
        print(f"  üìÑ {chunk.file_name}: {chunk.text[:80]}...")
    
    # –ü–µ—Ä–µ—Å–æ–±–∏—Ä–∞–µ–º regex –¥–ª—è –≤—Ä–∞—á–µ–π
    _rebuild_doctor_regex()
    print(f"–í—Ä–∞—á–∏: –°–æ–±—Ä–∞–Ω—ã –∏–º–µ–Ω–∞ –≤—Ä–∞—á–µ–π: {len(DOCTOR_NAME_TOKENS)} -> {sorted(list(DOCTOR_NAME_TOKENS))[:6]} ...")
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ –µ—Å—Ç—å —á–∞–Ω–∫–∏ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏
    if len(all_chunks) == 0:
        print("‚ö†Ô∏è –ü—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏–µ: –ù–µ –Ω–∞–π–¥–µ–Ω–æ –Ω–∏ –æ–¥–Ω–æ–≥–æ —á–∞–Ω–∫–∞ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏")
        # –°–æ–∑–¥–∞–µ–º –ø—É—Å—Ç–æ–π –∏–Ω–¥–µ–∫—Å
        dimension = 1536  # —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å text-embedding-3-small
        index = faiss.IndexFlatL2(dimension)
    else:
        # –ü–æ–ª—É—á–∞–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ –¥–ª—è –≤—Å–µ—Ö —Ñ—Ä–∞–≥–º–µ–Ω—Ç–æ–≤
        print(f"\u23f3 –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –¥–ª—è {len(all_chunks)} —á–∞–Ω–∫–æ–≤...")
        
        def get_embedding(text: str) -> List[float]:
            resp = openai_client.embeddings.create(
                model="text-embedding-3-small",
                input=text,
                encoding_format="float"
            )
            return resp.data[0].embedding
        
        # –î–µ–ª–∞–µ–º —Ñ—É–Ω–∫—Ü–∏—é –≥–ª–æ–±–∞–ª—å–Ω–æ –¥–æ—Å—Ç—É–ø–Ω–æ–π
        globals()['get_embedding'] = get_embedding
        
        # –°–æ–∑–¥–∞–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥–∏: —Ç–µ–∫—Å—Ç + –∞–ª–∏–∞—Å—ã –¥–ª—è –ø–æ–∏—Å–∫–∞
        chunk_texts = []
        for chunk in ALL_CHUNKS:
            alias_boost = " ".join(getattr(chunk.metadata, 'h2_aliases', ()) or [])
            chunk_texts.append((chunk.text + " " + alias_boost).strip())
        embeddings = [get_embedding(text) for text in chunk_texts]
        
        dimension = len(embeddings[0])
        xb = np.asarray(embeddings, dtype="float32")
        normalize_L2_inplace(xb)
        index = IndexFlatIP(dimension)
        index.add(xb)
        
        print(f"‚úÖ –ò–Ω–¥–µ–∫—Å —Å–æ–∑–¥–∞–Ω —Å {len(ALL_CHUNKS)} —á–∞–Ω–∫–∞–º–∏")
        
        # –õ–æ–≥–∏—Ä—É–µ–º backend –ø—Ä–∏ —Å—Ç–∞—Ä—Ç–µ
        from core.logger import log_m
        log_m.info(json.dumps({"event": "faiss_backend", "value": "faiss" if HAS_FAISS else "numpy"}, ensure_ascii=False))

except Exception as e:
    print(f"‚ùå –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞ –ø—Ä–∏ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏: {e}")
    import traceback
    traceback.print_exc()
    print("‚ö†Ô∏è Embeddings –Ω–µ–¥–æ—Å—Ç—É–ø–Ω—ã, —Ä–∞–±–æ—Ç–∞–µ–º —Ç–æ–ª—å–∫–æ –Ω–∞ BM25 –∏ –ø—Ä–∞–≤–∏–ª–∞—Ö")
    index = None
    # –Ω–µ—Ç FAISS, –Ω–æ —á–∞–Ω–∫–∏ –æ—Å—Ç–∞–≤–ª—è–µ–º!

# Fallback —Ñ—É–Ω–∫—Ü–∏—è get_embedding
def get_embedding(text: str) -> List[float]:
    """Fallback —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤"""
    try:
        resp = openai_client.embeddings.create(
            model="text-embedding-3-small",
            input=text,
            encoding_format="float"
        )
        return resp.data[0].embedding
    except Exception as e:
        print(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–æ–∑–¥–∞–Ω–∏–∏ —ç–º–±–µ–¥–¥–∏–Ω–≥–∞: {e}")
        # –í–æ–∑–≤—Ä–∞—â–∞–µ–º –Ω—É–ª–µ–≤–æ–π –≤–µ–∫—Ç–æ—Ä
        return [0.0] * 1536

def generate_query_variants(query: str) -> List[str]:
    """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç 2-3 –≤–∞—Ä–∏–∞–Ω—Ç–∞ –ø–µ—Ä–µ—Ñ—Ä–∞–∑–∏—Ä–æ–≤–∫–∏ –∑–∞–ø—Ä–æ—Å–∞ –¥–ª—è –ª—É—á—à–µ–≥–æ –ø–æ–∏—Å–∫–∞"""
    if not openai_client:
        return [query]  # Fallback –∫ –∏—Å—Ö–æ–¥–Ω–æ–º—É –∑–∞–ø—Ä–æ—Å—É
    
    prompt = f"""–ü–µ—Ä–µ—Ñ—Ä–∞–∑–∏—Ä—É–π –≤–æ–ø—Ä–æ—Å –ø–∞—Ü–∏–µ–Ω—Ç–∞ –æ —Å—Ç–æ–º–∞—Ç–æ–ª–æ–≥–∏–∏ –≤ 2-3 —Ä–∞–∑–Ω—ã—Ö –≤–∞—Ä–∏–∞–Ω—Ç–∞ –¥–ª—è –ø–æ–∏—Å–∫–∞ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏.

–ò—Å—Ö–æ–¥–Ω—ã–π –≤–æ–ø—Ä–æ—Å: "{query}"

–°–æ–∑–¥–∞–π –≤–∞—Ä–∏–∞–Ω—Ç—ã:
1. –ë–æ–ª–µ–µ —Ñ–æ—Ä–º–∞–ª—å–Ω—ã–π/–º–µ–¥–∏—Ü–∏–Ω—Å–∫–∏–π
2. –ë–æ–ª–µ–µ –ø—Ä–æ—Å—Ç–æ–π/–±—ã—Ç–æ–≤–æ–π  
3. –° —Å–∏–Ω–æ–Ω–∏–º–∞–º–∏ –∏ –∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω—ã–º–∏ —Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–∫–∞–º–∏

–í–µ—Ä–Ω–∏ –¢–û–õ–¨–ö–û JSON:
{{"variants": ["–≤–∞—Ä–∏–∞–Ω—Ç 1", "–≤–∞—Ä–∏–∞–Ω—Ç 2", "–≤–∞—Ä–∏–∞–Ω—Ç 3"]}}

–ü—Ä–∏–º–µ—Ä—ã:
- "–±–æ–ª—å–Ω–æ –ª–∏ —Å—Ç–∞–≤–∏—Ç—å –∏–º–ø–ª–∞–Ω—Ç" ‚Üí ["–±–æ–ª–µ–∑–Ω–µ–Ω–Ω–æ—Å—Ç—å –∏–º–ø–ª–∞–Ω—Ç–∞—Ü–∏–∏", "–¥–∏—Å–∫–æ–º—Ñ–æ—Ä—Ç –ø—Ä–∏ —É—Å—Ç–∞–Ω–æ–≤–∫–µ –∏–º–ø–ª–∞–Ω—Ç–∞", "–æ—â—É—â–µ–Ω–∏—è –≤–æ –≤—Ä–µ–º—è –∏–º–ø–ª–∞–Ω—Ç–∞—Ü–∏–∏"]
- "—Å–∫–æ–ª—å–∫–æ —Å—Ç–æ–∏—Ç –∏–º–ø–ª–∞–Ω—Ç–∞—Ü–∏—è" ‚Üí ["—Å—Ç–æ–∏–º–æ—Å—Ç—å –∏–º–ø–ª–∞–Ω—Ç–∞—Ü–∏–∏ –∑—É–±–æ–≤", "—Ü–µ–Ω–∞ –Ω–∞ –∏–º–ø–ª–∞–Ω—Ç—ã", "—Ä–∞—Å—Ü–µ–Ω–∫–∏ –Ω–∞ –∏–º–ø–ª–∞–Ω—Ç–∞—Ü–∏—é"]"""
    
    try:
        response = openai_client.chat.completions.create(
            model="gpt-4o-mini",
            messages=[{"role": "user", "content": prompt}],
            max_tokens=150,
            temperature=0.3
        )
        
        result = json.loads(response.choices[0].message.content)
        variants = result.get("variants", [])
        
        # –î–æ–±–∞–≤–ª—è–µ–º –∏—Å—Ö–æ–¥–Ω—ã–π –∑–∞–ø—Ä–æ—Å –≤ –Ω–∞—á–∞–ª–æ
        all_variants = [query] + variants
        # –£–±–∏—Ä–∞–µ–º –¥—É–±–ª–∏–∫–∞—Ç—ã, —Å–æ—Ö—Ä–∞–Ω—è—è –ø–æ—Ä—è–¥–æ–∫
        unique_variants = []
        seen = set()
        for variant in all_variants:
            if variant.lower() not in seen:
                unique_variants.append(variant)
                seen.add(variant.lower())
        
        print(f"üîç Multi-query: —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–æ {len(unique_variants)} –≤–∞—Ä–∏–∞–Ω—Ç–æ–≤ –∑–∞–ø—Ä–æ—Å–∞")
        return unique_variants[:3]  # –ú–∞–∫—Å–∏–º—É–º 3 –≤–∞—Ä–∏–∞–Ω—Ç–∞
        
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –≤–∞—Ä–∏–∞–Ω—Ç–æ–≤ –∑–∞–ø—Ä–æ—Å–∞: {e}")
        return [query]  # Fallback –∫ –∏—Å—Ö–æ–¥–Ω–æ–º—É –∑–∞–ø—Ä–æ—Å—É

def hybrid_retriever(query: str, top_n: int = 20) -> List[Tuple[RetrievedChunk, float]]:
    """–ì–∏–±—Ä–∏–¥–Ω—ã–π —Ä–µ—Ç—Ä–∏–≤–µ—Ä: –æ–±—ä–µ–¥–∏–Ω—è–µ—Ç BM25 –∏ —ç–º–±–µ–¥–¥–∏–Ω–≥–∏"""
    if not all_chunks or len(all_chunks) == 0:
        return []
    
    candidates = []
    
    # ==== BM25 –ø–æ–∏—Å–∫ ====
    if bm25_index:
        query_tokens = re.findall(r'\w+', query.lower())
        bm25_scores = bm25_index.get_scores(query_tokens)
        bm25_candidates = []
        
        for i, score in enumerate(bm25_scores):
            if score > 0:
                bm25_candidates.append((all_chunks[i], score))
        
        # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º BM25 scores
        if bm25_candidates:
            max_score = max(score for _, score in bm25_candidates)
            if max_score > 0:
                bm25_candidates = [(chunk, score / max_score * 0.6) for chunk, score in bm25_candidates]
                candidates.extend(bm25_candidates[:top_n])
    
    # ==== –≠–º–±–µ–¥–¥–∏–Ω–≥–∏ –ø–æ–∏—Å–∫ ====
    try:
        query_embedding = get_embedding(query)
        q = np.asarray([query_embedding], dtype="float32")
        normalize_L2_inplace(q)
        D, I = index.search(q, min(top_n, len(all_chunks)))
        
        # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º embedding scores
        max_dist = max(D[0]) if len(D[0]) > 0 else 1.0
        if max_dist > 0:
            embedding_candidates = []
            for i, dist in zip(I[0], D[0]):
                score = (1.0 - dist / max_dist) * 0.4  # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º –∏ –ø—Ä–∏–º–µ–Ω—è–µ–º –≤–µ—Å
                embedding_candidates.append((all_chunks[i], score))
            candidates.extend(embedding_candidates)
    except Exception as e:
        print(f"–û—à–∏–±–∫–∞ –≤ embedding –ø–æ–∏—Å–∫–µ: {e}")
    
    # ==== –û–±—ä–µ–¥–∏–Ω—è–µ–º –∏ —É–±–∏—Ä–∞–µ–º –¥—É–±–ª–∏ ====
    seen_chunks = set()
    unique_candidates = []
    
    for chunk, score in candidates:
        if chunk.id not in seen_chunks:
            seen_chunks.add(chunk.id)
            unique_candidates.append((chunk, score))
    
    # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ score –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ–º top_n
    unique_candidates.sort(key=lambda x: x[1], reverse=True)
    return unique_candidates[:top_n]

def llm_rerank(candidates: List[Tuple[RetrievedChunk, float]], query: str) -> List[Tuple[RetrievedChunk, float]]:
    """LLM-—Ä–µ—Ä–∞–Ω–∫–∏–Ω–≥ –¥–ª—è —Ç–æ—á–Ω–æ–π –æ—Ü–µ–Ω–∫–∏ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏ —á–∞–Ω–∫–æ–≤"""
    if not candidates or not openai_client:
        return candidates
    
    # –ë–µ—Ä–µ–º —Ç–æ–ª—å–∫–æ top-6 –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤ –¥–ª—è —Ä–µ—Ä–∞–Ω–∫–∏–Ω–≥–∞
    top_candidates = candidates[:6]
    
    # –§–æ—Ä–º–∏—Ä—É–µ–º –ø—Ä–æ–º–ø—Ç –¥–ª—è LLM
    chunks_text = ""
    for i, (chunk, score) in enumerate(top_candidates):
        # –ë–µ—Ä–µ–º —Ç–æ–ª—å–∫–æ –∑–∞–≥–æ–ª–æ–≤–æ–∫ –∏ –ø–µ—Ä–≤—ã–µ 200 —Å–∏–º–≤–æ–ª–æ–≤ –¥–ª—è —ç–∫–æ–Ω–æ–º–∏–∏ —Ç–æ–∫–µ–Ω–æ–≤
        header_match = re.search(r'(?m)^##\s+(.+?)\s*$', chunk.text)
        header = header_match.group(1) if header_match else "–ë–µ–∑ –∑–∞–≥–æ–ª–æ–≤–∫–∞"
        preview = chunk.text[:200] + "..." if len(chunk.text) > 200 else chunk.text
        chunks_text += f"{i+1}. –ó–∞–≥–æ–ª–æ–≤–æ–∫: {header}\n–¢–µ–∫—Å—Ç: {preview}\n\n"
    
    prompt = f"""–û—Ü–µ–Ω–∏ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å –∫–∞–∂–¥–æ–≥–æ —Ñ—Ä–∞–≥–º–µ–Ω—Ç–∞ –¥–ª—è –æ—Ç–≤–µ—Ç–∞ –Ω–∞ –≤–æ–ø—Ä–æ—Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è.

–í–æ–ø—Ä–æ—Å: "{query}"

–§—Ä–∞–≥–º–µ–Ω—Ç—ã:
{chunks_text}

–í–µ—Ä–Ω–∏ –¢–û–õ–¨–ö–û JSON —Å –æ—Ü–µ–Ω–∫–∞–º–∏ –æ—Ç 0.0 –¥–æ 1.0:
{{"scores": [0.8, 0.3, 0.9, 0.1, 0.7, 0.2]}}

–ì–¥–µ 1.0 = –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ, 0.0 = –Ω–µ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ."""
    
    try:
        response = openai_client.chat.completions.create(
            model="gpt-4o-mini",
            messages=[{"role": "user", "content": prompt}],
            max_tokens=100,
            temperature=0.1
        )
        
        result = json.loads(response.choices[0].message.content)
        llm_scores = result.get("scores", [])
        
        # –ü—Ä–∏–º–µ–Ω—è–µ–º LLM-–æ—Ü–µ–Ω–∫–∏ –∫ –∫–∞–Ω–¥–∏–¥–∞—Ç–∞–º
        reranked = []
        for i, (chunk, base_score) in enumerate(top_candidates):
            if i < len(llm_scores):
                # –ö–æ–º–±–∏–Ω–∏—Ä—É–µ–º –±–∞–∑–æ–≤—ã–π score —Å LLM-–æ—Ü–µ–Ω–∫–æ–π (70% LLM, 30% –±–∞–∑–æ–≤—ã–π)
                final_score = llm_scores[i] * 0.7 + base_score * 0.3
                reranked.append((chunk, final_score))
            else:
                reranked.append((chunk, base_score))
        
        # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ —Ñ–∏–Ω–∞–ª—å–Ω–æ–º—É score
        reranked.sort(key=lambda x: x[1], reverse=True)
        print(f"üîç LLM-—Ä–µ—Ä–∞–Ω–∫–∏–Ω–≥: {len(reranked)} –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤ –ø–µ—Ä–µ–æ—Ü–µ–Ω–µ–Ω—ã")
        
        return reranked
        
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ LLM-—Ä–µ—Ä–∞–Ω–∫–∏–Ω–≥–∞: {e}")
        return candidates

def select_chunk_by_alias(chunks: List[RetrievedChunk], query: str) -> RetrievedChunk | None:
    """–§–æ—Ä—Å-–º–∞—Ç—á –ø–æ –∞–ª–∏–∞—Å–∞–º –ø–µ—Ä–µ–¥ —Ñ–∏–Ω–∞–ª—å–Ω—ã–º –≤—ã–±–æ—Ä–æ–º —á–∞–Ω–∫–∞"""
    ql = query.lower()
    for chunk in chunks:
        h2_aliases = getattr(chunk.metadata, 'h2_aliases', []) or []
        for alias in h2_aliases:
            al = alias.lower()
            if al == ql or al in ql:
                return chunk
    return None

def reranker(candidates: List[Tuple[RetrievedChunk, float]], query: str, detected_topics: Set[str]) -> List[RetrievedChunk]:
    """–†–µ—Ä–∞–Ω–∫–µ—Ä —Å LLM-–æ—Ü–µ–Ω–∫–æ–π —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏"""
    if not candidates:
        return []
    
    # –°–Ω–∞—á–∞–ª–∞ –ø—Ä–∏–º–µ–Ω—è–µ–º LLM-—Ä–µ—Ä–∞–Ω–∫–∏–Ω–≥
    llm_reranked = llm_rerank(candidates, query)
    
    # –ó–∞—Ç–µ–º –ø—Ä–∏–º–µ–Ω—è–µ–º —ç–≤—Ä–∏—Å—Ç–∏—á–µ—Å–∫–∏–µ –±–æ–Ω—É—Å—ã
    scored_candidates = []
    query_lower = query.lower()
    
    for chunk, base_score in llm_reranked:
        final_score = base_score
        
        # +0.2 –µ—Å–ª–∏ —Ç–µ–º–∞ –æ—Ç router —Å–æ–≤–ø–∞–ª–∞ —Å —Ç–µ–º–æ–π —á–∞–Ω–∫–∞
        if detected_topics and chunk.metadata.topic:
            if chunk.metadata.topic in detected_topics:
                final_score += 0.2
        
        # +0.1 –µ—Å–ª–∏ –Ω–∞–π–¥–µ–Ω alias —á–µ—Ä–µ–∑ ENTITY_INDEX
        for alias, meta in ENTITY_INDEX.items():
            if alias in query_lower:
                if meta["doc_id"] == chunk.file_name:
                    final_score += 0.1
                    break
        
        scored_candidates.append((chunk, final_score))
    
    # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ —Ñ–∏–Ω–∞–ª—å–Ω–æ–º—É score
    scored_candidates.sort(key=lambda x: x[1], reverse=True)
    
    # –í–æ–∑–≤—Ä–∞—â–∞–µ–º 2-3 –ª—É—á—à–∏—Ö —á–∞–Ω–∫–∞
    return [chunk for chunk, _ in scored_candidates[:3]]



# === 6) –†–∞–Ω–Ω–∏–π –¥–µ—Ç–µ–∫—Ç–æ—Ä H2 ===
def detect_section_early(user_q: str):
    q = norm_text(user_q)
    hit = H2_INDEX.get(q)
    if not hit:
        # –º—è–≥–∫–æ–µ –≤—Ö–æ–∂–¥–µ–Ω–∏–µ (–º–∏–Ω–∏–º—É–º 3 —Å–∏–º–≤–æ–ª–∞ –¥–ª—è –∏–∑–±–µ–∂–∞–Ω–∏—è –ª–æ–∂–Ω—ã—Ö —Å—Ä–∞–±–∞—Ç—ã–≤–∞–Ω–∏–π)
        for k, v in H2_INDEX.items():
            if k and len(k) > 2 and k in q:
                hit = v; break
    if not hit: return None, {}
    ch = _find_chunk(hit["file"], hit["h2_id"])
    if not ch: return None, {}
    return ch, {"source":"alias","exact_h2_match":True,"topic":hit["topic"]}

# === 7) –ì–ª–∞–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è —Ä–µ—Ç—Ä–∏–≤–∞ ===
def retrieve_relevant_chunks_new(user_q: str, theme_hint: str|None, candidates_func):
    """–ù–æ–≤–∞—è —Ñ—É–Ω–∫—Ü–∏—è —Ä–µ—Ç—Ä–∏–≤–∞: –ø–æ–¥—Å–∫–∞–∑–∫–∏ (H2/—Ç–µ–º–∞) –¥–æ–±–∞–≤–ª—è—é—Ç—Å—è –∫ –∫–∞–Ω–¥–∏–¥–∞—Ç–∞–º, –∞ –Ω–µ –ø—Ä–µ—Ä—ã–≤–∞—é—Ç –ø–æ–∏—Å–∫."""
    print(f"üîç NEW ENGINE: query='{user_q}', theme_hint='{theme_hint}'")

    # –ï—Å–ª–∏ —è–≤–Ω–æ —Ä–∞–∑—Ä–µ—à—ë–Ω —Å—Ç–∞—Ä—ã–π fastpath ‚Äî –æ—Å—Ç–∞–≤–∏–º –ø—Ä–µ–∂–Ω–µ–µ –ø–æ–≤–µ–¥–µ–Ω–∏–µ
    if os.getenv("DISABLE_ALIAS_FASTPATH", "true").lower() != "true":
        ch, flags = detect_section_early(user_q)
        if ch:
            print(f"‚úÖ H2 match found (fastpath): {ch.id}")
            return [ch], flags
        if theme_hint in CANON:
            ch, flags = get_default_chunk_for_topic(theme_hint)
            if ch:
                print(f"‚úÖ Theme match found (fastpath): {theme_hint} -> {ch.id}")
                return [ch], flags

    mixed: list[RetrievedChunk] = []

    # 1) —Ç–æ—á–Ω—ã–π H2 ‚Äî –¥–æ–±–∞–≤–ª—è–µ–º –∫ –∫–∞–Ω–¥–∏–¥–∞—Ç–∞–º
    ch, _flags = detect_section_early(user_q)
    if ch:
        print(f"‚úÖ H2 hint: {ch.id}")
        mixed.append(ch)

    # 2) –¥–µ—Ñ–æ–ª—Ç –ø–æ —Ç–µ–º–µ ‚Äî —Ç–æ–∂–µ –¥–æ–±–∞–≤–ª—è–µ–º
    if theme_hint in CANON:
        ch2, _flags2 = get_default_chunk_for_topic(theme_hint)
        if ch2:
            print(f"‚úÖ Theme hint: {theme_hint} -> {ch2.id}")
            mixed.append(ch2)

    # 3) –æ–±—ã—á–Ω—ã–π –ø–æ–∏—Å–∫
    search_cands = candidates_func(user_q) or []
    print(f"üîç Search candidates: {len(search_cands)} found")
    mixed.extend(search_cands)

    # 4) –¥–µ–¥—É–ø –ø–æ chunk.id –∏–ª–∏ file_name
    seen = set()
    out = []
    for c in mixed:
        cid = getattr(c, "id", None) or getattr(getattr(c, "meta", {}), "get", lambda k=None: None)("id")
        fid = getattr(c, "file_name", None)
        key = cid or fid
        if key in seen:
            continue
        seen.add(key)
        out.append(c)

    # 5) –†–µ—Ä–∞–Ω–∫–µ—Ä: –¥–∞—Ç—å –±–∞–∑–æ–≤—ã–π score –∏ —Ä–µ–∞–ª—å–Ω–æ –ø–µ—Ä–µ—Å–æ—Ä—Ç–∏—Ä–æ–≤–∞—Ç—å
    def _ensure_scores(cands):
        """–£–±–µ–∂–¥–∞–µ–º—Å—è, —á—Ç–æ —É –∫–∞–∂–¥–æ–≥–æ –∫–∞–Ω–¥–∏–¥–∞—Ç–∞ –µ—Å—Ç—å –±–∞–∑–æ–≤—ã–π score"""
        for i, c in enumerate(cands):
            score = getattr(c, "score", None) or getattr(c, "cosine", None) or getattr(c, "bm25", None)
            if score is None:
                score = 0.5  # –±–∞–∑–æ–≤—ã–π score
            c.score = float(score) - i * 1e-6  # —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç—å –ø–æ—Ä—è–¥–∫–∞
    
    def _bonus_for_query(c, q: str, theme: str) -> float:
        """–ë–æ–Ω—É—Å –∑–∞ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å –∫ –∑–∞–ø—Ä–æ—Å—É –∏ —Ç–µ–º–µ"""
        t = (getattr(c, "text", "") or "").lower()
        q = (q or "").lower()
        b = 0.0
        
        # –ø—Ä–∏–∂–∏–≤/–æ—Å—Å–µ–æ
        if "–ø—Ä–∏–∂–∏–≤" in q or "–æ—Å—Å–µ–æ" in q:
            if any(x in t for x in ["–ø—Ä–∏–∂–∏–≤", "–æ—Å—Å–µ–æ–∏–Ω—Ç–µ–≥—Ä"]): b += 0.10
        
        # –±–æ—è–∑–Ω—å/–±–æ–ª—å
        if any(x in q for x in ["–±–æ—é—Å—å", "–±–æ–ª—å", "–∞–Ω–µ—Å—Ç–µ–∑", "–æ–±–µ–∑–±–æ–ª"]):
            if any(x in t for x in ["–±–µ–∑ –±–æ–ª–∏", "–∞–Ω–µ—Å—Ç–µ–∑", "–æ–±–µ–∑–±–æ–ª"]): b += 0.10
        
        # –∫–æ–Ω—Ç–∞–∫—Ç—ã
        if theme == "contacts":
            if any(x in t for x in ["–∞–¥—Ä–µ—Å", "—Ç–µ–ª–µ—Ñ–æ–Ω", "–≥—Ä–∞—Ñ–∏–∫", "–∫–∞–∫ –¥–æ–±—Ä–∞—Ç—å—Å—è"]): b += 0.10
        
        # –≥–∞—Ä–∞–Ω—Ç–∏—è
        if theme == "warranty":
            if "–≥–∞—Ä–∞–Ω—Ç–∏" in t: b += 0.08
        
        # —Ü–µ–Ω—ã
        if theme == "prices":
            if any(x in t for x in ["—Ü–µ–Ω–∞", "—Å—Ç–æ–∏–º–æ—Å—Ç—å", "—Å–∫–æ–ª—å–∫–æ —Å—Ç–æ–∏—Ç", "—Ä–∞—Å—Å—Ä–æ—á–∫–∞"]): b += 0.08
        
        return b
    
    _ensure_scores(out)
    for c in out:
        c.score += _bonus_for_query(c, user_q, theme_hint or "")
    out.sort(key=lambda x: x.score, reverse=True)

    return out, {"source": "mix", "exact_h2_match": False}

def retrieve_relevant_chunks(query: str, top_k: int = None) -> List[RetrievedChunk]:
    if top_k is None:
        top_k = int(os.getenv("RAG_TOP_K", 5))  # –±—ã–ª–æ 8, —Ç–µ–ø–µ—Ä—å 5 –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é
    """–ò–∑–≤–ª–µ–∫–∞–µ—Ç —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–µ —á–∞–Ω–∫–∏ —Å multi-query rewrite –∏ —É–ª—É—á—à–µ–Ω–Ω—ã–º —Ä–∞–Ω–∂–∏—Ä–æ–≤–∞–Ω–∏–µ–º"""
    if len(ALL_CHUNKS) == 0:
        print("‚ö†Ô∏è –ù–µ—Ç —á–∞–Ω–∫–æ–≤ –¥–ª—è –ø–æ–∏—Å–∫–∞")
        return []
    
    # ==== –†–û–£–¢–ï–† –ü–û –¢–ï–ú–ê–ú ====
    detected_topics = route_topics(query)
    print(f"üéØ –†–æ—É—Ç–µ—Ä –æ–ø—Ä–µ–¥–µ–ª–∏–ª —Ç–µ–º—ã: {detected_topics}")
    
    # –ü—Ä—è–º–æ–π alias-fallback –ø–æ –∫–∞—Ä—Ç–µ frontmatter (–û–¢–ö–õ–Æ–ß–ï–ù)
    # q = _norm(query or "")
    # hit_map = ALIAS_MAP.get(q)
    # if not hit_map:
    #     # –¥–æ–ø—É—Å–∫–∞–µ–º "alias ‚äÜ query" (–Ω–∞–ø—Ä–∏–º–µ—Ä, –∑–∞–ø—Ä–æ—Å –¥–ª–∏–Ω–Ω–µ–µ)
    #     for a, meta in ALIAS_MAP.items():
    #         if a and a in q:
    #             hit_map = meta
    #             break
    #
    # if hit_map:
    #     # –Ω–∞—Ö–æ–¥–∏–º –ø–µ—Ä–≤—ã–π —á–∞–Ω–∫ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–µ–≥–æ —Ñ–∞–π–ª–∞ –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –µ–≥–æ
    #     target = Path(hit_map["file"]).name
    #     for ch in ALL_CHUNKS:
    #         if ch.file_name == target:
    #             print(f"‚úÖ Alias-fallback: {q} -> {hit_map['file']}")
    #             return [ch]
    
    # ==== –ë–´–°–¢–†–´–ô –ü–£–¢–¨: –î–ï–¢–ï–ö–¢ –°–£–©–ù–û–°–¢–ò ====
    q = _norm(query or "")
    hit = None
    
    # –ü—Ä—è–º–æ–µ –≤—Ö–æ–∂–¥–µ–Ω–∏–µ –∞–ª–∏–∞—Å–∞
    for alias, meta in ENTITY_INDEX.items():
        if alias in q:
            hit = ENTITY_CHUNKS.get((meta["topic"], meta["entity"]))
            if not hit:
                hit = next((ch for ch in all_chunks if ch.id == meta["entity"]), None) \
                    or next((ch for ch in all_chunks if ch.file_name == meta["doc_id"]), None)
            if hit:
                break
    
    # –ú—è–≥–∫–∏–π –º–∞—Ç—á (—Ä–∞–∑–¥–µ–ª–∏—Ç–µ–ª–∏ -/-/- –∏ –ø—Ä–æ–±–µ–ª—ã)
    if not hit:
        q2 = re.sub(r'[\-‚Äì‚Äî]', '', q)   # –¥–µ—Ñ–∏—Å/—Ç–∏—Ä–µ
        for alias, meta in ENTITY_INDEX.items():
            a2 = re.sub(r'[\-‚Äì‚Äî]', '', alias)   # –¥–µ—Ñ–∏—Å/—Ç–∏—Ä–µ
            if a2 in q2:
                hit = ENTITY_CHUNKS.get((meta["topic"], meta["entity"]))
                if not hit:
                    hit = next((ch for ch in all_chunks if ch.id == meta["entity"]), None) \
                        or next((ch for ch in all_chunks if ch.file_name == meta["doc_id"]), None)
                if hit:
                    break
    
    if hit:
        print(f"‚úÖ –ù–∞–π–¥–µ–Ω–∞ —Å—É—â–Ω–æ—Å—Ç—å –∫–∞—Ç–∞–ª–æ–≥–∞: '{query}' ‚Üí {hit.id}")
        return [hit]  # —Ä–æ–≤–Ω–æ –Ω—É–∂–Ω–∞—è —Å–µ–∫—Ü–∏—è –∫–∞—Ç–∞–ª–æ–≥–∞
    
    # ==== –°–ü–ï–¶–ò–ê–õ–¨–ù–´–ï –ó–ê–ü–†–û–°–´: –õ–ò–°–¢–ò–ù–ì –ò –°–†–ê–í–ù–ï–ù–ò–ï ====
    # –õ–∏—Å—Ç–∏–Ω–≥ ("–∫–∞–∫–∏–µ –≤–∏–¥—ã")
    if re.search(r'(–∫–∞–∫–∏–µ|–∫–∞–∫–æ–π|—á—Ç–æ –∑–∞).* (–≤–∏–¥|–≤–∞—Ä–∏–∞–Ω—Ç).* (–∏–º–ø–ª–∞–Ω—Ç–∞—Ü|–∏–º–ø–ª–∞–Ω—Ç)', q):
        kinds_order = ["single-stage", "classic", "all-on-4", "all-on-6"]
        out = [ENTITY_CHUNKS[("implants", k)] for k in kinds_order if ("implants", k) in ENTITY_CHUNKS]
        print(f"üìã –õ–∏—Å—Ç–∏–Ω–≥ –≤–∏–¥–æ–≤ –∏–º–ø–ª–∞–Ω—Ç–∞—Ü–∏–∏: –Ω–∞–π–¥–µ–Ω–æ {len(out)} —Ç–∏–ø–æ–≤")
        return out[:4]
    
    # –°—Ä–∞–≤–Ω–µ–Ω–∏–µ ("—á–µ–º –æ—Ç–ª–∏—á–∞–µ—Ç—Å—è / —á—Ç–æ –ª—É—á—à–µ")
    if re.search(r'(—á–µ–º\s+–æ—Ç–ª–∏—á|—Ä–∞–∑–ª–∏—á|—Ä–∞–∑–Ω–∏—Ü–∞|—Å—Ä–∞–≤–Ω|—á—Ç–æ\s+–ª—É—á—à–µ|vs)', q):
        kinds_order = ["single-stage", "classic", "all-on-4", "all-on-6"]
        out = [ENTITY_CHUNKS[("implants", k)] for k in kinds_order if ("implants", k) in ENTITY_CHUNKS]
        print(f"üìä –°—Ä–∞–≤–Ω–µ–Ω–∏–µ –≤–∏–¥–æ–≤ –∏–º–ø–ª–∞–Ω—Ç–∞—Ü–∏–∏: –Ω–∞–π–¥–µ–Ω–æ {len(out)} —Ç–∏–ø–æ–≤")
        return out[:4]
    
    # 0) –µ—Å–ª–∏ –Ω–∞—à–ª–∏ –≤—Ä–∞—á–∞ –Ω–∞–ø—Ä—è–º—É—é ‚Äî —Å—Ä–∞–∑—É –æ—Ç–¥–∞—ë–º –∫–∞—Ä—Ç–æ—á–∫—É, –º–∏–Ω—É—è FAISS
    hit = _find_doctor_direct_or_fuzzy(query)
    if hit:
        print(f"‚úÖ –ö–∞—Ä—Ç–æ—á–∫–∞ –≤—Ä–∞—á–∞ –ø–æ –∑–∞–ø—Ä–æ—Å—É: {query} ‚Üí {getattr(hit, 'section', hit.file_name)}")
        return [hit] # –±–µ–∑ –¥–µ–¥—É–ø–∞
    
    try:
        print(f"üîç –ü–æ–∏—Å–∫: '{query}' –≤ {len(all_chunks)} —á–∞–Ω–∫–∞—Ö")
        
        # ==== MULTI-QUERY REWRITE ====
        query_variants = generate_query_variants(query)
        print(f"üîç Multi-query: –∏—Å–ø–æ–ª—å–∑—É–µ–º {len(query_variants)} –≤–∞—Ä–∏–∞–Ω—Ç–æ–≤ –∑–∞–ø—Ä–æ—Å–∞")
        
        # ==== –ì–ò–ë–†–ò–î–ù–´–ô –†–ï–¢–†–ò–í–ï–† –î–õ–Ø –ö–ê–ñ–î–û–ì–û –í–ê–†–ò–ê–ù–¢–ê ====
        all_candidates = []
        for variant in query_variants:
            candidates = hybrid_retriever(variant, top_n=15)  # –ú–µ–Ω—å—à–µ –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤ –Ω–∞ –≤–∞—Ä–∏–∞–Ω—Ç
            all_candidates.extend(candidates)
            print(f"üîç –í–∞—Ä–∏–∞–Ω—Ç '{variant[:30]}...': –Ω–∞–π–¥–µ–Ω–æ {len(candidates)} –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤")
        
        # ==== –û–ë–™–ï–î–ò–ù–ï–ù–ò–ï –ò –î–ï–î–£–ü–õ–ò–ö–ê–¶–ò–Ø ====
        seen_chunks = set()
        unique_candidates = []
        
        for chunk, score in all_candidates:
            if chunk.id not in seen_chunks:
                seen_chunks.add(chunk.id)
                unique_candidates.append((chunk, score))
        
        # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ score –∏ –±–µ—Ä–µ–º top
        unique_candidates.sort(key=lambda x: x[1], reverse=True)
        top_candidates = unique_candidates[:25]  # –ë–æ–ª—å—à–µ –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤ –¥–ª—è —Ä–µ—Ä–∞–Ω–∫–∏–Ω–≥–∞
        
        print(f"üîç Multi-query: –æ–±—ä–µ–¥–∏–Ω–µ–Ω–æ {len(unique_candidates)} —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤")
        
        # ==== –§–û–†–°-–ú–ê–¢–ß –ü–û –ê–õ–ò–ê–°–ê–ú ====
        forced_chunk = select_chunk_by_alias([chunk for chunk, _ in top_candidates], query)
        if forced_chunk:
            print(f"üéØ –§–æ—Ä—Å-–º–∞—Ç—á –ø–æ –∞–ª–∏–∞—Å—É: {forced_chunk.id}")
            final_chunks = [forced_chunk]
        else:
            # ==== –†–ï–†–ê–ù–ö–ï–† ====
            final_chunks = reranker(top_candidates, query, detected_topics)
            print(f"üîç –†–µ—Ä–∞–Ω–∫–µ—Ä –æ—Ç–æ–±—Ä–∞–ª {len(final_chunks)} —Ñ–∏–Ω–∞–ª—å–Ω—ã—Ö —á–∞–Ω–∫–æ–≤")
        
        # –µ—Å–ª–∏ –Ω–∏—á–µ–≥–æ –≤–Ω—è—Ç–Ω–æ–≥–æ –Ω–µ –ø–æ–ø–∞–ª–æ –∏ —Ç–µ–º–∞ –∏–∑–≤–µ—Å—Ç–Ω–∞ ‚Äî –∂—ë—Å—Ç–∫–∏–π fallback
        if not final_chunks and detected_topics:
            theme_key = list(detected_topics)[0]
            final_chunks = fallback_theme_chunks(theme_key, limit=top_k)
        
        # –µ—Å–ª–∏ –∏—â–µ–º –≤—Ä–∞—á–∞ - –Ω–µ —Ä–µ–∂–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ file_name
        if DOCTOR_NAME_TO_CHUNK and _find_doctor_direct_or_fuzzy(query):
            return final_chunks[:top_k] # –±–µ–∑ –¥–µ–¥—É–ø–∞
        
        # –∏–Ω–∞—á–µ –æ—Å—Ç–∞–≤–ª—è–µ–º –∑–∞—â–∏—Ç–Ω—ã–π –¥–µ–¥—É–ø –ø–æ —Ñ–∞–π–ª–∞–º
        seen = set()
        out = []
        for chunk in final_chunks:
            if chunk.file_name not in seen:
                seen.add(chunk.file_name)
                out.append(chunk)
        
        print(f"‚úÖ –í–æ–∑–≤—Ä–∞—â–∞–µ–º {len(out)} —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —á–∞–Ω–∫–æ–≤")
        return out[:top_k]
        
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–∏—Å–∫–µ —á–∞–Ω–∫–æ–≤: {e}")
        return []

def _strip_md(s: str) -> str:
    """–û—á–∏—Å—Ç–∫–∞ –æ—Ç Markdown —Ä–∞–∑–º–µ—Ç–∫–∏"""
    if not s:
        return ""
    s = re.sub(r'<!--.*?-->', '', s, flags=re.DOTALL)
    s = re.sub(r'(?im)^\s*aliases\s*:\s*\[.*?\]\s*$', '', s)
    s = re.sub(r'(?m)^\s*#{1,6}\s*', '', s)
    s = re.sub(r'\*\*(.+?)\*\*', r'\1', s)
    s = re.sub(r'__(.+?)__', r'\1', s)
    s = re.sub(r'(?<!\w)_(.*?)_(?!\w)', r'\1', s)
    s = re.sub(r'\n{3,}', '\n\n', s)
    return s.strip()

def synthesize_answer(chunks: List[RetrievedChunk], user_query: str, verbatim=False) -> dict:
    """–ú–∏–Ω–∏-—Å–∏–Ω—Ç–µ–∑ –±–µ–∑ LLM: —Å–∫–ª–µ–∏–≤–∞–µ–º 2-3 —Å–∞–º—ã—Ö —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö —Ñ—Ä–∞–≥–º–µ–Ω—Ç–∞, –æ—á–∏—â–∞–µ–º Markdown."""
    
    # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ —Å –æ—á–∏—Å—Ç–∫–æ–π –∏ shaping
    from core.text_clean import clean_section_for_prompt
    from core.answer_shaping import should_verbatim, clamp_bullets, slice_for_prompt
    import json, logging
    log_m = logging.getLogger("cesi.minimal_logs")
    
    prepared_chunks, prepared_len, verbatim_used = [], 0, False
    for sec in chunks[:3]:  # –±–µ—Ä–µ–º —Ç–æ–ª—å–∫–æ –ø–µ—Ä–≤—ã–µ 3 —á–∞–Ω–∫–∞
        meta = getattr(sec, "meta", {}) or {}
        clean = clean_section_for_prompt(sec.text)
        if should_verbatim(meta):
            prepared = clamp_bullets(clean, max_bullets=6)
            verbatim_used = True
        else:
            prepared = slice_for_prompt(clean, limit_chars=900)
        prepared_chunks.append(prepared)
        prepared_len += len(prepared)
    
    log_m.info(json.dumps({"ev":"prepared_context","count":len(prepared_chunks),"prepared_len":prepared_len,"verbatim":verbatim_used}, ensure_ascii=False))
    
    # –ò—Å–ø–æ–ª—å–∑—É–µ–º –ø–æ–¥–≥–æ—Ç–æ–≤–ª–µ–Ω–Ω—ã–µ —á–∞–Ω–∫–∏ –≤–º–µ—Å—Ç–æ —Å—ã—Ä—ã—Ö —Ç–µ–∫—Å—Ç–æ–≤
    if verbatim or not prepared_chunks:
        return {"text": prepared_chunks[0] if prepared_chunks else ""}
    
    return {"text": "\n\n".join(prepared_chunks)}

def synthesize_answer_old(chunks: List[RetrievedChunk], user_query: str, allow_cta: bool) -> SynthJSON:
    """–°–∏–Ω—Ç–µ–∑–∏—Ä—É–µ—Ç —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π JSON –æ—Ç–≤–µ—Ç"""
    
    # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –¥–ª—è –æ—Ç–≤–µ—Ç–∞ (–±–µ—Ä–µ–º –∏–∑ –ø–µ—Ä–≤–æ–≥–æ —á–∞–Ω–∫–∞)
    primary_chunk = chunks[0] if chunks else None
    tone = primary_chunk.metadata.tone if primary_chunk else "friendly"
    preferred_format = primary_chunk.metadata.preferred_format if primary_chunk else ["short", "bullets", "cta"]
    verbatim = primary_chunk.metadata.verbatim if primary_chunk else False
    
    # –ï—Å–ª–∏ verbatim: true - –æ—Ç–¥–∞–µ–º —Ç–µ–∫—Å—Ç "–∫–∞–∫ –µ—Å—Ç—å" –±–µ–∑ LLM
    if verbatim:
        print(f"üìù Verbatim —Ä–µ–∂–∏–º: –æ—Ç–¥–∞–µ–º —Ç–µ–∫—Å—Ç –±–µ–∑ LLM –ø–µ—Ä–µ—Å–∫–∞–∑–∞")
        
        # –ò–∑–≤–ª–µ–∫–∞–µ–º bullets –∏–∑ —Ç–µ–∫—Å—Ç–∞
        bullets = []
        for chunk in chunks:
            lines = chunk.text.split('\n')
            for line in lines:
                line = line.strip()
                if line.startswith('- ') or line.startswith('* '):
                    bullet = line[2:].strip()
                    if bullet:
                        bullets.append(bullet)
    
    # CTA –¥–ª—è –ø—Ä–æ–º–ø—Ç–∞ (—Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ —Ä–∞–∑—Ä–µ—à–µ–Ω)
    cta_for_prompt = (primary_chunk.metadata.cta_text if (primary_chunk and allow_cta) else "")
    
    return {
            "short": "–ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –ø–æ –≤–∞—à–µ–º—É –∑–∞–ø—Ä–æ—Å—É:",
            "bullets": bullets,
            "cta": cta_for_prompt if cta_for_prompt else None,
            "used_chunks": [chunk.id for chunk in chunks],
            "tone": tone,
            "warnings": []
        }
    
    # –§–æ—Ä–º–∏—Ä—É–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç –∏–∑ —á–∞–Ω–∫–æ–≤ –¥–ª—è LLM
    from core.text_clean import clean_section_for_prompt
    from core.answer_shaping import should_verbatim, clamp_bullets, slice_for_prompt
    import json, logging
    log_m = logging.getLogger("cesi.minimal_logs")
    
    context_parts = []
    total_before, total_after = 0, 0
    cleaned = []
    prepared_chunks = []
    
    for chunk in chunks:
        t = chunk.text
        total_before += len(t)
        c = clean_section_for_prompt(t)
        total_after += len(c)
        cleaned.append(c)
        
        # –ü—Ä–∏–º–µ–Ω—è–µ–º shaping –¥–ª—è –æ—Ç–≤–µ—Ç–æ–≤
        meta = getattr(chunk, "meta", {}) or {}
        if should_verbatim(meta):
            prepared = clamp_bullets(c, max_bullets=6)
        else:
            prepared = slice_for_prompt(c, limit_chars=900)
        prepared_chunks.append(prepared)
        
        context_parts.append(f"ID: {chunk.id}\n–û–±–Ω–æ–≤–ª–µ–Ω–æ: {chunk.updated}\n–ö—Ä–∏—Ç–∏—á–Ω–æ—Å—Ç—å: {chunk.criticality}\n–ö–æ–Ω—Ç–µ–Ω—Ç:\n{prepared}\n")
    
    context = "\n---\n".join(context_parts)
    
    # –õ–æ–≥–∏—Ä—É–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –æ—á–∏—Å—Ç–∫–∏
    log_m.info(json.dumps({"ev":"clean_section","orig_len":total_before,"clean_len":total_after,"reduced":total_before-total_after}, ensure_ascii=False))
    print(f"üìù –ö–æ–Ω—Ç–µ–∫—Å—Ç –¥–ª—è —Å–∏–Ω—Ç–µ–∑–∞: {len(context)} —Å–∏–º–≤–æ–ª–æ–≤")
    print(f"üìù –ü–µ—Ä–≤—ã–µ 200 —Å–∏–º–≤–æ–ª–æ–≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞: {context[:200]}...")
    
    # CTA –¥–ª—è –ø—Ä–æ–º–ø—Ç–∞ (—Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ —Ä–∞–∑—Ä–µ—à–µ–Ω)
    cta_for_prompt = (primary_chunk.metadata.cta_text if (primary_chunk and allow_cta) else "")
    
    # –ê–¥–∞–ø—Ç–∏–≤–Ω–∞—è –ª–æ–≥–∏–∫–∞ –¥–ª—è verbatim: false —Ñ–∞–π–ª–æ–≤
    format_instruction = ""
    if "detailed" in preferred_format:
        format_instruction = "\n\n–í–ê–ñ–ù–û: –î–∞–π —Ä–∞–∑–≤–µ—Ä–Ω—É—Ç—ã–π, –ø–æ–¥—Ä–æ–±–Ω—ã–π –æ—Ç–≤–µ—Ç —Å –º–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ–º –¥–µ—Ç–∞–ª–µ–π –∏ –æ–±—ä—è—Å–Ω–µ–Ω–∏–π. –ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å –Ω—É–∂–¥–∞–µ—Ç—Å—è –≤ –ø–æ–ª–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏."
    elif "short" in preferred_format:
        format_instruction = "\n\n–í–ê–ñ–ù–û: –î–∞–π –∫—Ä–∞—Ç–∫–∏–π, –ª–∞–∫–æ–Ω–∏—á–Ω—ã–π –æ—Ç–≤–µ—Ç –ø–æ —Å—É—Ç–∏ –≤–æ–ø—Ä–æ—Å–∞. –ë–µ–∑ –ª–∏—à–Ω–∏—Ö –¥–µ—Ç–∞–ª–µ–π."
    
        system_prompt = f"""
–¢—ã ‚Äî –¥—Ä—É–∂–µ–ª—é–±–Ω—ã–π –∏ –≤–Ω–∏–º–∞—Ç–µ–ª—å–Ω—ã–π –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç —Å—Ç–æ–º–∞—Ç–æ–ª–æ–≥–∏—á–µ—Å–∫–æ–π –∫–ª–∏–Ω–∏–∫–∏ –¶–≠–°–ò –Ω–∞ –ö–∞–º—á–∞—Ç–∫–µ.
–¢–≤–æ—è –∑–∞–¥–∞—á–∞ ‚Äî –æ—Ç–≤–µ—á–∞—Ç—å –Ω–∞ –≤–æ–ø—Ä–æ—Å—ã –ø–∞—Ü–∏–µ–Ω—Ç–æ–≤ –ø—Ä–æ—Å—Ç–æ, –ø–æ–Ω—è—Ç–Ω–æ –∏ —Å –∑–∞–±–æ—Ç–æ–π, –Ω–æ —Å—Ç—Ä–æ–≥–æ –ø–æ –¥–µ–ª—É –∏ –Ω–∞ –æ—Å–Ω–æ–≤–µ –±–∞–∑—ã –∑–Ω–∞–Ω–∏–π.
–¢–æ–Ω –æ–±—â–µ–Ω–∏—è ‚Äî —Ç—ë–ø–ª—ã–π, —ç–∫—Å–ø–µ—Ä—Ç–Ω—ã–π –∏ —É–≤–∞–∂–∏—Ç–µ–ª—å–Ω—ã–π.

–ü—Ä–∞–≤–∏–ª–∞:
0. –í–ê–ñ–ù–û: –ï—Å–ª–∏ –≤–æ–ø—Ä–æ—Å –æ –±–æ–ª–∏, —Å—Ç—Ä–∞—Ö–µ, –¥–∏—Å–∫–æ–º—Ñ–æ—Ä—Ç–µ ‚Äî –í–°–ï–ì–î–ê —Å–Ω–∞—á–∞–ª–∞ –¥–∞–π –ø–æ–¥—Ä–æ–±–Ω—ã–π –∏–Ω—Ñ–æ—Ä–º–∞—Ç–∏–≤–Ω—ã–π –æ—Ç–≤–µ—Ç –Ω–∞ –æ—Å–Ω–æ–≤–µ –±–∞–∑—ã –∑–Ω–∞–Ω–∏–π, –∞ –ø–æ—Ç–æ–º –∫–æ—Ä–æ—Ç–∫—É—é —É—Å–ø–æ–∫–∞–∏–≤–∞—é—â—É—é —Ñ—Ä–∞–∑—É. –ù–ò–ö–û–ì–î–ê –Ω–µ –ø—Ä–µ–¥–ª–∞–≥–∞–π —Å—Ä–∞–∑—É –∫–æ–Ω—Å—É–ª—å—Ç–∞—Ü–∏—é –±–µ–∑ –æ—Ç–≤–µ—Ç–∞!
1. –í—Å–µ–≥–¥–∞ –ø–∏—à–∏ –≤ —Ñ–æ—Ä–º–∞—Ç–µ –æ–±—ã—á–Ω–æ–≥–æ –¥–∏–∞–ª–æ–≥–∞, –±–µ–∑ —Å–ª—É–∂–µ–±–Ω—ã—Ö —Å–∏–º–≤–æ–ª–æ–≤ Markdown –≤ –æ—Ç–≤–µ—Ç–µ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—é.
2. –†–∞–∑–±–∏–≤–∞–π —Ç–µ–∫—Å—Ç –Ω–∞ –∞–±–∑–∞—Ü—ã –∏ —Å–º—ã—Å–ª–æ–≤—ã–µ –±–ª–æ–∫–∏.
3. –ò—Å–ø–æ–ª—å–∑—É–π —á–µ–ª–æ–≤–µ—á–µ—Å–∫–∏–µ —Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–∫–∏, –∏–∑–±–µ–≥–∞–π –ø—É—Å—Ç—ã—Ö —Ñ—Ä–∞–∑.
4. –≠–º–æ–¥–∑–∏ –∏—Å–ø–æ–ª—å–∑—É–π —Ç–æ–ª—å–∫–æ –ø—Ä–∏ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç–∏ –∏ —É–º–µ—Ä–µ–Ω–Ω–æ.
5. –ï—Å–ª–∏ –≤–æ–ø—Ä–æ—Å —Ç—Ä–µ–≤–æ–∂–Ω—ã–π ‚Äî —Å–Ω–∞—á–∞–ª–∞ —Ñ–∞–∫—Ç—ã –∏–∑ –±–∞–∑—ã –∑–Ω–∞–Ω–∏–π, –ø–æ—Ç–æ–º –º—è–≥–∫–∞—è —É—Å–ø–æ–∫–∞–∏–≤–∞—é—â–∞—è —Ñ—Ä–∞–∑–∞.
6. –§–æ—Ä–º—É–ª–∏—Ä–æ–≤–∫–∏ –∏ —Ü–∏—Ñ—Ä—ã –∏–∑ –±–∞–∑—ã –∑–Ω–∞–Ω–∏–π —Å–æ—Ö—Ä–∞–Ω—è–π –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ —Ç–æ—á–Ω–æ, –∞–¥–∞–ø—Ç–∏—Ä—É—è –ø–æ–¥ –∂–∏–≤–æ–π –¥–∏–∞–ª–æ–≥.
7. –ï—Å–ª–∏ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –Ω–µ—Ç ‚Äî —á–µ—Å—Ç–Ω–æ —Å–∫–∞–∂–∏ –æ–± —ç—Ç–æ–º –∏ –ø—Ä–µ–¥–ª–æ–∂–∏ –∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤—É: –∫–æ–Ω—Å—É–ª—å—Ç–∞—Ü–∏—é –∏–ª–∏ –¥—Ä—É–≥–æ–π —Å–ø–æ—Å–æ–± —É–∑–Ω–∞—Ç—å –æ—Ç–≤–µ—Ç.
8. –í –∫–æ–Ω—Ü–µ –ø—Ä–∏ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏ –¥–æ–±–∞–≤—å –º—è–≥–∫–∏–π CTA —Å –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ–º –∫–æ–Ω—Å—É–ª—å—Ç–∞—Ü–∏–∏.{format_instruction}

–ö–∞–∫ —Ä–∞–±–æ—Ç–∞—Ç—å —Å –±–∞–∑–æ–π:
1. YAML-—Ñ—Ä–æ–Ω—Ç–º–∞—Ç—Ç–µ—Ä –∏–≥–Ω–æ—Ä–∏—Ä—É–π –ø—Ä–∏ —Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–∞–Ω–∏–∏ –æ—Ç–≤–µ—Ç–∞, –Ω–æ –∏—Å–ø–æ–ª—å–∑—É–π –µ–≥–æ –∑–Ω–∞—á–µ–Ω–∏—è –¥–ª—è —Å—Ç–∏–ª—è –∏ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã.
2. –ü—Ä–∏ –ø–æ—Å—Ç—Ä–æ–µ–Ω–∏–∏ –æ—Ç–≤–µ—Ç–∞ –æ—Ä–∏–µ–Ω—Ç–∏—Ä—É–π—Å—è –Ω–∞ –ø–æ–ª—è:
   - 'preferred_format' ‚Äî —Å—Ç—Ä—É–∫—Ç—É—Ä–∞ –æ—Ç–≤–µ—Ç–∞.
   - 'cta_text' –∏ 'cta_link' ‚Äî –¥–ª—è —Ñ–∏–Ω–∞–ª—å–Ω–æ–π —Ñ—Ä–∞–∑—ã.
   - 'tone', 'emotion' ‚Äî –¥–ª—è —Å—Ç–∏–ª—è.
3. –ò—Å–ø–æ–ª—å–∑—É–π –∫–æ–Ω—Ç–µ–Ω—Ç –∏–∑ —Ä–∞–∑–¥–µ–ª–æ–≤:
   - **–ö–æ—Ä–æ—Ç–∫–æ** ‚Äî –¥–ª—è –ø–µ—Ä–≤–æ–≥–æ –∞–±–∑–∞—Ü–∞ (–ø—Ä—è–º–æ–π –æ—Ç–≤–µ—Ç).
   - **–î–µ—Ç–∞–ª–∏** ‚Äî –¥–ª—è –≤—Ç–æ—Ä–æ–≥–æ –∞–±–∑–∞—Ü–∞ (—Ñ–∞–∫—Ç—ã –∏ —É—Ç–æ—á–Ω–µ–Ω–∏—è).
   - **–°–ª–µ–¥—É—é—â–∏–π —à–∞–≥** ‚Äî –¥–ª—è –∑–∞–∫–ª—é—á–µ–Ω–∏—è –∏ CTA.

## –ö–æ–Ω—Ç–µ–∫—Å—Ç –¥–ª—è –æ—Ç–≤–µ—Ç–∞:
{context}

## –í–æ–ø—Ä–æ—Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è:
{user_query}

## –ú–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ:
- –¢–æ–Ω: {tone}
- –ü—Ä–µ–¥–ø–æ—á—Ç–∏—Ç–µ–ª—å–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç: {preferred_format}
{f"- CTA —Ç–µ–∫—Å—Ç: {cta_for_prompt}" if cta_for_prompt else ""}

–í–ê–ñ–ù–û: –ï—Å–ª–∏ –≤–æ–ø—Ä–æ—Å –æ –≤—Ä–∞—á–µ –∏–ª–∏ —Å–æ—Ç—Ä—É–¥–Ω–∏–∫–µ –∫–ª–∏–Ω–∏–∫–∏, –æ—Ç–≤–µ—á–∞–π —Ç–æ–ª—å–∫–æ –ø–æ –±–∞–∑–µ. –ï—Å–ª–∏ –¥–∞–Ω–Ω—ã—Ö –Ω–µ—Ç, —Å–∫–∞–∂–∏, —á—Ç–æ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –Ω–µ—Ç, –±–µ–∑ –ø—Ä–µ–¥–ø–æ–ª–æ–∂–µ–Ω–∏–π.

–û—Ç–≤–µ—á–∞–π —Å—Ç—Ä–æ–≥–æ –≤ —Ñ–æ—Ä–º–∞—Ç–µ JSON:
{{
    "short": "–ö–æ—Ä–æ—Ç–∫–∏–π –æ—Ç–≤–µ—Ç –ø–æ —Å—É—Ç–∏ –≤–æ–ø—Ä–æ—Å–∞",
    "bullets": ["–î–µ—Ç–∞–ª—å 1", "–î–µ—Ç–∞–ª—å 2", "–î–µ—Ç–∞–ª—å 3"],
    "cta": "–ú—è–≥–∫–∏–π –ø—Ä–∏–∑—ã–≤ –∫ –¥–µ–π—Å—Ç–≤–∏—é",
    "used_chunks": ["ID1", "ID2"],
    "tone": "{tone}",
    "warnings": []
}}
"""

    # –ñ—ë—Å—Ç–∫–∏–π JSON-–∫–æ–Ω—Ç—Ä–∞–∫—Ç —Å –ø—Ä–∞–≤–∏–ª–∞–º–∏ –∫—Ä–∞—Ç–∫–æ—Å—Ç–∏
    SYSTEM_RULES = """
–¢—ã –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç –∫–ª–∏–Ω–∏–∫–∏. –û—Ç–≤–µ—á–∞–π —Å–ø–æ–∫–æ–π–Ω–æ –∏ –ø–æ –¥–µ–ª—É.
–¢–æ–ª—å–∫–æ –ß–ò–°–¢–´–ô JSON (–±–µ–∑ Markdown-—Ñ–µ–Ω—Å–æ–≤).

–ó–∞–ø—Ä–µ—â–µ–Ω–æ:
- –æ–≥–ª–∞–≤–ª–µ–Ω–∏—è, –ø–µ—Ä–µ—á–∏—Å–ª–µ–Ω–∏–µ —Ç–µ–º, H2/H3;
- –ø–æ–ª—è —Ñ—Ä–æ–Ω—Ç–º–∞—Ç—Ç–µ—Ä–∞ (title, aliases, mini_links, verbatim) –∏ HTML-–∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–∏;
- –¥–ª–∏–Ω–Ω—ã–µ –∫–æ–ø–∏–ø–∞—Å—Ç—ã –∏–∑ –±–∞–∑—ã;
- —Å–∏–º–≤–æ–ª—ã '#', '*', '‚Ä¢' –∏ –∑–∞–≥–æ–ª–æ–≤–∫–∏ –≤–∏–¥–∞ "### ..." –≤ —Ç–µ–∫—Å—Ç–µ;
- –ø–æ–≤—Ç–æ—Ä–µ–Ω–∏–µ –∑–∞–≥–æ–ª–æ–≤–∫–∞ –¥–æ–∫—É–º–µ–Ω—Ç–∞ –≤ —Ç–µ–∫—Å—Ç–µ.

–ï—Å–ª–∏ STYLE=compact: –Ω–µ –∏—Å–ø–æ–ª—å–∑—É–π #, -, ‚Ä¢, —Å–ø–∏—Å–∫–∏ –∏ –∑–∞–≥–æ–ª–æ–≤–∫–∏; 3‚Äì6 –∫–æ—Ä–æ—Ç–∫–∏—Ö —Ñ—Ä–∞–∑; –∑–∞–≤–µ—Ä—à–∏ —Ç–æ—á–∫–æ–π.
–ï—Å–ª–∏ STYLE=list: –¥–æ N –ø—É–Ω–∫—Ç–æ–≤, –∫–∞–∂–¥—ã–π 1 –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ–º; –±–µ–∑ –∑–∞–≥–æ–ª–æ–≤–∫–æ–≤; –º–∞—Ä–∫–µ—Ä –Ω–µ –≤—ã–≤–æ–¥–∏ ‚Äî –º—ã —Ñ–æ—Ä–º–∞—Ç–∏—Ä—É–µ–º —Å–∞–º–∏.

–°—Ö–µ–º–∞:
{
 "answer": "string, ‚â§700 —Å–∏–º–≤–æ–ª–æ–≤, –±–µ–∑ –æ–≥–ª–∞–≤–ª–µ–Ω–∏–π –∏ —Å–ø–∏—Å–∫–æ–≤ —Ç–µ–º",
 "empathy": "string –∏–ª–∏ ''",
 "cta": {"show": true|false, "variant": "consult"},
 "followups": [{"label":"string","query":"string"}]
}
–ï—Å–ª–∏ —É–≤–µ—Ä–µ–Ω –Ω–∞ <65%, –∑–∞–¥–∞–π 1 —É—Ç–æ—á–Ω—è—é—â–∏–π –≤–æ–ø—Ä–æ—Å –≤ –ø–æ–ª–µ answer –∏ –Ω–µ –ø–æ–∫–∞–∑—ã–≤–∞–π CTA.
"""

    completion = openai_client.chat.completions.create(
        model="gpt-4o-mini",
        temperature=0.2,  # –Ω–∏–∑–∫–∞—è —Ç–µ–º–ø–µ—Ä–∞—Ç—É—Ä–∞ –¥–ª—è —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏
        max_tokens=220,   # –æ–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –¥–ª–∏–Ω—É
        top_p=0.8,       # –æ–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –≤—ã–±–æ—Ä —Ç–æ–∫–µ–Ω–æ–≤
        response_format={"type": "json_object"},
        messages=[
            {"role": "system", "content": SYSTEM_RULES},
            {"role": "user", "content": system_prompt}
        ]
    )
    
    try:
        # –õ–æ–≥–∏—Ä—É–µ–º —Ñ–∏–Ω–∏—à –º–æ–¥–µ–ª–∏
        finish = completion.choices[0].finish_reason  # "stop" | "length" | ...
        c_tokens = getattr(completion.usage, "completion_tokens", None)
        log_m.info(json.dumps({"ev":"llm_finish","finish":finish,"completion_tokens":c_tokens,"max_tokens":220}, ensure_ascii=False))
        
        json_response = json.loads(completion.choices[0].message.content)
        
        # –í–∞–ª–∏–¥–∞—Ü–∏—è –∏ –æ–±—Ä–µ–∑–∫–∞ –æ—Ç–≤–µ—Ç–∞
        answer = json_response.get("answer", "–ò–∑–≤–∏–Ω–∏—Ç–µ, –¥–∞–≤–∞–π—Ç–µ —É—Ç–æ—á–Ω—é‚Ä¶")
        if len(answer) > 700:
            answer = answer[:700] + "..."
        
        empathy = json_response.get("empathy", "")
        cta = json_response.get("cta", {"show": False, "variant": "consult"})
        if not allow_cta:
            cta = {"show": False, "variant": "consult"}
        
        followups = json_response.get("followups", [])
        
        return SynthJSON(
            short=answer,
            bullets=[],  # –£–±–∏—Ä–∞–µ–º bullets –≤ –ø–æ–ª—å–∑—É –∫—Ä–∞—Ç–∫–æ–≥–æ answer
            cta=cta.get("show", False),
            used_chunks=json_response.get("used_chunks", []),
            tone=json_response.get("tone", tone),
            warnings=json_response.get("warnings", [])
        )
    except json.JSONDecodeError:
        # Fallback –µ—Å–ª–∏ JSON –Ω–µ–∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã–π
        return SynthJSON(
            short="–ò–∑–≤–∏–Ω–∏—Ç–µ, –ø—Ä–æ–∏–∑–æ—à–ª–∞ –æ—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ –æ—Ç–≤–µ—Ç–∞.",
            bullets=["–ü–æ–ø—Ä–æ–±—É–π—Ç–µ –ø–µ—Ä–µ—Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–∞—Ç—å –≤–æ–ø—Ä–æ—Å"],
            cta="–û–±—Ä–∞—Ç–∏—Ç–µ—Å—å –∫ –∞–¥–º–∏–Ω–∏—Å—Ç—Ä–∞—Ç–æ—Ä—É",
            used_chunks=[],
            tone="friendly",
            warnings=["JSON parsing error"]
        )

def compress_answer(text: str, max_length: int = 800) -> str:
    """–°–∂–∏–º–∞–µ—Ç –¥–ª–∏–Ω–Ω—ã–π –æ—Ç–≤–µ—Ç –¥–æ —É–∫–∞–∑–∞–Ω–Ω–æ–π –¥–ª–∏–Ω—ã –±–µ–∑ –ø–æ—Ç–µ—Ä–∏ –∫–ª—é—á–µ–≤–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏"""
    if len(text) <= max_length or not openai_client:
        return text
    
    # –°—á–∏—Ç–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π
    sentences = re.split(r'[.!?]+', text)
    if len(sentences) <= 3:
        return text  # –ï—Å–ª–∏ –º–∞–ª–æ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π, –Ω–µ —Å–∂–∏–º–∞–µ–º
    
    prompt = f"""–°–æ–∂–º–∏ —ç—Ç–æ—Ç –æ—Ç–≤–µ—Ç –æ —Å—Ç–æ–º–∞—Ç–æ–ª–æ–≥–∏–∏ –¥–æ {max_length} —Å–∏–º–≤–æ–ª–æ–≤, —Å–æ—Ö—Ä–∞–Ω–∏–≤ –í–°–ï –≤–∞–∂–Ω—ã–µ —Ñ–∞–∫—Ç—ã –∏ —Ü–∏—Ñ—Ä—ã.

–í–ê–ñ–ù–û:
- –°–æ—Ö—Ä–∞–Ω–∏ –í–°–ï —á–∏—Å–ª–∞, –ø—Ä–æ—Ü–µ–Ω—Ç—ã, —Å—Ä–æ–∫–∏, —Ü–µ–Ω—ã
- –°–æ—Ö—Ä–∞–Ω–∏ –∫–ª—é—á–µ–≤—É—é –º–µ–¥–∏—Ü–∏–Ω—Å–∫—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é
- –£–±–µ—Ä–∏ —Ç–æ–ª—å–∫–æ "–≤–æ–¥—É" –∏ –ø–æ–≤—Ç–æ—Ä–µ–Ω–∏—è
- –û—Å—Ç–∞–≤—å —Å—Ç—Ä—É–∫—Ç—É—Ä—É: –∫–æ—Ä–æ—Ç–∫–∏–π –æ—Ç–≤–µ—Ç + –¥–µ—Ç–∞–ª–∏ + –ø—Ä–∏–∑—ã–≤ –∫ –¥–µ–π—Å—Ç–≤–∏—é

–ò—Å—Ö–æ–¥–Ω—ã–π –æ—Ç–≤–µ—Ç:
{text}

–°–∂–∞—Ç—ã–π –æ—Ç–≤–µ—Ç:"""
    
    try:
        response = openai_client.chat.completions.create(
            model="gpt-4o-mini",
            messages=[{"role": "user", "content": prompt}],
            max_tokens=300,
            temperature=0.1
        )
        
        compressed = response.choices[0].message.content.strip()
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ —Å–∂–∞—Ç–∏–µ –Ω–µ —Å–ª–∏—à–∫–æ–º –∞–≥—Ä–µ—Å—Å–∏–≤–Ω–æ–µ
        if len(compressed) < max_length * 0.5:
            print(f"‚ö†Ô∏è –°–∂–∞—Ç–∏–µ —Å–ª–∏—à–∫–æ–º –∞–≥—Ä–µ—Å—Å–∏–≤–Ω–æ–µ, –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –∏—Å—Ö–æ–¥–Ω—ã–π —Ç–µ–∫—Å—Ç")
            return text
        
        print(f"üîç Answer compression: {len(text)} ‚Üí {len(compressed)} —Å–∏–º–≤–æ–ª–æ–≤")
        return compressed
        
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ —Å–∂–∞—Ç–∏—è –æ—Ç–≤–µ—Ç–∞: {e}")
        return text

def render_markdown(synth: SynthJSON) -> str:
    """–†–µ–Ω–¥–µ—Ä–∏—Ç JSON –≤ –∫—Ä–∞—Å–∏–≤—ã–π Markdown –±–µ–∑ —Å–ª—É–∂–µ–±–Ω—ã—Ö –∑–∞–≥–æ–ª–æ–≤–∫–æ–≤"""
    
    def clean_text(text: str) -> str:
        """–û—á–∏—â–∞–µ—Ç —Ç–µ–∫—Å—Ç –æ—Ç —Å–ª—É–∂–µ–±–Ω—ã—Ö —Å–∏–º–≤–æ–ª–æ–≤ –∏ –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–π"""
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ text - —ç—Ç–æ —Å—Ç—Ä–æ–∫–∞
        if not isinstance(text, str):
            text = str(text) if text is not None else ""
        
        # –£–±–∏—Ä–∞–µ–º —Å–ª—É–∂–µ–±–Ω—ã–µ —Å–∏–º–≤–æ–ª—ã –∏ –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏
        cleaned = re.sub(r'^(?:\*|–û—Ç–≤–µ—Ç:|–ò–Ω—Å—Ç—Ä—É–∫—Ü–∏—è:|JSON:|–ü—Ä–æ–º–ø—Ç:)\s*', '', text, flags=re.IGNORECASE)
        # –£–±–∏—Ä–∞–µ–º Markdown –∑–∞–≥–æ–ª–æ–≤–∫–∏
        cleaned = re.sub(r'^\s*#{1,6}\s*', '', cleaned)  # <- —Å—Ä–µ–∑–∞–µ–º '## ...'
        # –£–±–∏—Ä–∞–µ–º –ª–∏—à–Ω–∏–µ –ø—Ä–æ–±–µ–ª—ã
        cleaned = re.sub(r'\s+', ' ', cleaned)
        # –£–±–∏—Ä–∞–µ–º –∫–∞–≤—ã—á–∫–∏ –≤ –Ω–∞—á–∞–ª–µ –∏ –∫–æ–Ω—Ü–µ
        cleaned = re.sub(r'^["\']+|["\']+$', '', cleaned)
        return cleaned.strip()
    
    # –û—á–∏—â–∞–µ–º –∏ —Ñ–æ—Ä–º–∞—Ç–∏—Ä—É–µ–º —á–∞—Å—Ç–∏
    short = clean_text(synth.get("short", ""))
    cta = clean_text(synth.get("cta", ""))
    
    # –§–æ—Ä–º–∞—Ç–∏—Ä—É–µ–º bullets
    bullets = []
    for bullet in synth.get("bullets", []):
        clean_bullet = clean_text(bullet)
        if clean_bullet and len(clean_bullet) > 5:  # –ú–∏–Ω–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏–Ω–∞
            bullets.append(f"- {clean_bullet}")
    
    # –°–æ–±–∏—Ä–∞–µ–º –æ—Ç–≤–µ—Ç –ë–ï–ó —Å–ª—É–∂–µ–±–Ω—ã—Ö –∑–∞–≥–æ–ª–æ–≤–∫–æ–≤
    blocks = []
    
    # –ö–æ—Ä–æ—Ç–∫–∏–π –æ—Ç–≤–µ—Ç
    if short:
        blocks.append(short)
    
    # –î–µ—Ç–∞–ª–∏
    if bullets:
        if blocks:  # –ï—Å–ª–∏ —É–∂–µ –µ—Å—Ç—å –∫–æ–Ω—Ç–µ–Ω—Ç, –¥–æ–±–∞–≤–ª—è–µ–º —Ä–∞–∑–¥–µ–ª–µ–Ω–∏–µ
            blocks.append("")
        blocks.append("\n".join(bullets))
    
    # –°–ª–µ–¥—É—é—â–∏–π —à–∞–≥ (CTA) - –±–µ–∑ –ª–∏—à–Ω–∏—Ö –æ—Ç—Å—Ç—É–ø–æ–≤
    if cta:
        if blocks:  # –ï—Å–ª–∏ —É–∂–µ –µ—Å—Ç—å –∫–æ–Ω—Ç–µ–Ω—Ç, –¥–æ–±–∞–≤–ª—è–µ–º —Ä–∞–∑–¥–µ–ª–µ–Ω–∏–µ
            blocks.append("")
        blocks.append(cta)
    
    # –°–æ–±–∏—Ä–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç
    markdown = "\n".join(blocks)
    
    # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º –ø—É—Å—Ç—ã–µ —Å—Ç—Ä–æ–∫–∏ (–º–∞–∫—Å–∏–º—É–º 1 –ø–æ–¥—Ä—è–¥)
    markdown = re.sub(r'\n{2,}', '\n', markdown)
    
    # –ü—Ä–∏–º–µ–Ω—è–µ–º –∞–Ω—Ç–∏-–≤–æ–¥—É
    markdown = strip_fluff_start(markdown)
    
    return markdown

# –û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è
def get_rag_answer(user_message: str, history: List[Dict] = []) -> tuple[str, dict]:
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è –æ—Ç–≤–µ—Ç–∞ –∏ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö"""
    from logging import getLogger
    import json
    logger = getLogger("cesi.rag")
    logger.info("‚û°Ô∏è –ù–æ–≤—ã–π –≤–æ–ø—Ä–æ—Å: %s", user_message)
    
    # –õ–æ–≥–∏—Ä—É–µ–º –∑–∞–ø—Ä–æ—Å –≤ RAG
    from core.logger import log_query
    log_query(user_message, "rag_engine")
    
    # –í—Å–µ–≥–¥–∞ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º ‚Äî —á—Ç–æ–±—ã –Ω–µ –ª–æ–≤–∏—Ç—å UnboundLocalError
    detected_topics = []
    theme_hint = None
    rag_meta = {"user_query": user_message}
    
    try:
        # --- 1) –Ø–≤–Ω—ã–π –æ–≤–µ—Ä—Ä–∞–π–¥ —Ç–µ–º—ã –ø–æ –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–∞–º (–µ—Å–ª–∏ –∏—Å–ø–æ–ª—å–∑—É–µ—à—å root_aliases.yaml)
        try:
            import yaml
            ROOT = yaml.safe_load(open("config/root_aliases.yaml", "r", encoding="utf-8")) or {}
            ql = user_message.lower()
            for doc_type, keys in (ROOT.get("root_aliases") or {}).items():
                if any(k in ql for k in keys):
                    theme_hint = doc_type
                    break
        except Exception:
            pass
    
        # --- 2) –ê–≤—Ç–æ-–¥–µ—Ç–µ–∫—Ç —Ç–µ–º—ã (–µ—Å–ª–∏ –æ–≤–µ—Ä—Ä–∞–π–¥ –Ω–µ –¥–∞–ª —Ç–µ–º—É)
        if theme_hint is None:
            try:
                detected_topics = list(route_topics(user_message))
                print(f"üéØ –†–æ—É—Ç–µ—Ä –æ–ø—Ä–µ–¥–µ–ª–∏–ª —Ç–µ–º—ã: {detected_topics}")
            except Exception:
                detected_topics = []
            
            if detected_topics:
                theme_hint = detected_topics[0]

        # --- 2.5) –ë—ã—Å—Ç—Ä—ã–π –ø—É—Ç—å –ø–æ –∞–ª–∏–∞—Å–∞–º (–í–†–ï–ú–ï–ù–ù–û –û–¢–ö–õ–Æ–ß–ï–ù)
        # q_norm = norm_text(user_message)
        # if q_norm in ALIAS_MAP_GLOBAL:
        #     key = ALIAS_MAP_GLOBAL[q_norm]
        #     # –Ω–∞–π–¥—ë–º —á–∞–Ω–∫ –ø–æ —Ñ–∞–π–ª—É –∏/–∏–ª–∏ —è–∫–æ—Ä—é –ù2
        #     ch = _find_chunk(key["file"], key.get("primary_h2_id"))
        #     if ch:
        #         topic_meta = getattr(ch.metadata, '__dict__', {}) or {}
        #         payload = postprocess(
        #             answer_text=ch.text,
        #             user_text=user_message,
        #             intent=theme_hint or key.get("topic"),
        #             topic_meta=topic_meta,
        #             session={},
        #         )
        #         return payload, {"user_query": user_message, "theme_hint": theme_hint, "fast_path": "alias"}

        # --- 3) –†–µ—Ç—Ä–∏–≤–∞–ª (2 –ø—Ä–æ—Ö–æ–¥–∞: —Å —Ç–µ–º–æ–π ‚Üí –±–µ–∑ —Ç–µ–º—ã)
        # –£–≤–µ–ª–∏—á–∏–≤–∞–µ–º top_k –¥–ª—è –ø–æ–∏—Å–∫–∞ –≤—Ä–∞—á–µ–π
        if DOCTOR_REGEX and DOCTOR_REGEX.search(user_message):
            top_k = 12
            print(f"üîç –ü–æ–∏—Å–∫ –≤—Ä–∞—á–µ–π: –∏—Å–ø–æ–ª—å–∑—É–µ–º top_k={top_k}")
        else:
            top_k = int(os.getenv("RAG_TOP_K", 5))  # –±—ã–ª–æ 8, —Ç–µ–ø–µ—Ä—å 5 –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é
        
        # –ò–∑–≤–ª–µ–∫–∞–µ–º —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–µ —á–∞–Ω–∫–∏ (–∏—Å–ø–æ–ª—å–∑—É–µ–º –Ω–æ–≤—É—é –ª–æ–≥–∏–∫—É)
        logger.info("üîé theme_hint=%s detected_topics=%s", theme_hint, detected_topics)
        relevant_chunks, meta_flags = retrieve_relevant_chunks_new(
            user_message, 
            theme_hint=theme_hint,
            candidates_func=lambda q: retrieve_relevant_chunks(q, top_k=top_k)
        )
        
        # –ú–∏–Ω–∏-—Ñ–∏–ª—å—Ç—Ä –ø–æ —Ç–µ–º–µ (–º–∏–Ω–∏–º—É–º –ª–æ–≥–∏–∫–∏, –º–∞–∫—Å–∏–º—É–º —ç—Ñ—Ñ–µ–∫—Ç–∞)
        def filter_candidates(theme: str, q: str, cands: list):
            t = (q or "").lower()
            
            def h2_of(c):
                return (getattr(c, "h2", None) or getattr(c, "meta", {}).get("h2", "") or "").lower()
            
            def text_of(c):
                return (getattr(c, "text", "") or "").lower()
            
            # ¬´–ø—Ä–∏–∂–∏–≤–∞–µ–º–æ—Å—Ç—å¬ª ‚Äî –æ—Å—Ç–∞–≤–ª—è–µ–º —Ç–æ–ª—å–∫–æ –∫—É—Å–∫–∏, –≥–¥–µ —è–≤–Ω–æ –µ—Å—Ç—å –ø—Ä–∏–∂–∏–≤/–æ—Å—Å–µ–æ
            if any(k in t for k in ["–ø—Ä–∏–∂–∏–≤", "–ø—Ä–∏–∂–∏–≤–∞–µ–º", "–æ—Å—Å–µ–æ"]):
                return [c for c in cands if any(x in h2_of(c) + " " + text_of(c) for x in ["–ø—Ä–∏–∂–∏–≤", "–æ—Å—Å–µ–æ–∏–Ω—Ç–µ–≥—Ä"])]
            
            # —Å—Ç—Ä–∞—Ö/–±–æ–ª—å ‚Äî –≤—ã–∫–∏–¥—ã–≤–∞–µ–º ¬´–ø—Ä–æ—Ç–∏–≤–æ–ø–æ–∫–∞–∑–∞–Ω–∏—è¬ª
            if any(k in t for k in ["–±–æ—é—Å—å", "–±–æ–ª—å", "—Å—Ç—Ä–∞—à–Ω–æ", "–∞–Ω–µ—Å—Ç–µ–∑", "–æ–±–µ–∑–±–æ–ª"]):
                bad = ["–ø—Ä–æ—Ç–∏–≤–æ–ø–æ–∫–∞–∑–∞–Ω", "–ø—Ä–æ—Ç–∏–≤–æ–ø–æ–∫–∞–∑–∞–Ω–∏—è"]
                return [c for c in cands if not any(x in h2_of(c) for x in bad)]
            
            return cands
        
        relevant_chunks = filter_candidates(theme_hint, user_message, relevant_chunks)
        
        # –õ—ë–≥–∫–∏–π –ø–µ—Ä–µ—Ä–∞–Ω–∂ (—á—Ç–æ–±—ã ¬´–Ω—É–∂–Ω–æ–µ¬ª –≤—Å–ø–ª—ã–≤–∞–ª–æ –ø–µ—Ä–≤—ã–º)
        def bonus_for_query(c, q):
            t = (getattr(c, "text", "") or "").lower()
            b = 0.0
            
            # –ø—Ä–∏–∂–∏–≤/–æ—Å—Å–µ–æ
            if "–ø—Ä–∏–∂–∏–≤" in q or "–æ—Å—Å–µ–æ" in q:
                if any(x in t for x in ["–ø—Ä–∏–∂–∏–≤", "–æ—Å—Å–µ–æ–∏–Ω—Ç–µ–≥—Ä"]): b += 0.1
            
            # –±–æ—è–∑–Ω—å/–±–æ–ª—å
            if any(x in q for x in ["–±–æ—é—Å—å", "–±–æ–ª—å", "–∞–Ω–µ—Å—Ç–µ–∑", "–æ–±–µ–∑–±–æ–ª"]):
                if any(x in t for x in ["–±–µ–∑ –±–æ–ª–∏", "–∞–Ω–µ—Å—Ç–µ–∑", "–æ–±–µ–∑–±–æ–ª"]): b += 0.1
            
            return b
        
        for c in relevant_chunks:
            base = getattr(c, "score", 0.0)  # —Ç–≤–æ—è –∫–æ—Å–∏–Ω—É—Å/–ë–ú25
            c.score = float(base) + bonus_for_query(c, user_message.lower())
        
        relevant_chunks = sorted(relevant_chunks, key=lambda x: x.score, reverse=True)
        
        # –ë—ã—Å—Ç—Ä–∞—è –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∞ (—á—Ç–æ–±—ã –ø–æ–Ω—è—Ç—å –ø—Ä–∏—á–∏–Ω—É)
        for i, c in enumerate(relevant_chunks[:5], 1):
            logger.info({
                "ev": "rerank_top",
                "rank": i,
                "score": round(getattr(c, "score", 0.0), 3),
                "h2": getattr(c, "h2", None) or getattr(c, "meta", {}).get("h2"),
                "doc": getattr(c, "meta", {}).get("doc_id"),
            })
        
        # –ï—Å–ª–∏ –Ω–µ—Ç –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤, –ø—Ä–æ–±—É–µ–º –±–µ–∑ —Ç–µ–º—ã
        if not relevant_chunks:
            logger.info("‚ö†Ô∏è –ü–µ—Ä–≤—ã–π –ø—Ä–æ—Ö–æ–¥ –Ω–µ –¥–∞–ª —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤, –ø—Ä–æ–±—É–µ–º –±–µ–∑ —Ç–µ–º—ã...")
            relevant_chunks, meta_flags = retrieve_relevant_chunks_new(
                user_message, 
                theme_hint=None,
                candidates_func=lambda q: retrieve_relevant_chunks(q, top_k=top_k)
            )
        
        # –õ–æ–≥–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç
        logger.info("üì¶ candidates=%d", len(relevant_chunks))
        logger.info("üîç rag_engine: relevant_chunks[0] type=%s", type(relevant_chunks[0]) if relevant_chunks else "None")
        
        if not relevant_chunks:
            # —á–µ—Å—Ç–Ω—ã–π –Ω–∏–∑–∫–∏–π —Ä–µ–ª–µ–≤–∞–Ω—Å
            return LOW_REL_JSON.copy(), rag_meta

        # --- 4) –°–∏–Ω—Ç–µ–∑ –æ—Ç–≤–µ—Ç–∞ –∏–∑ –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö —á–∞–Ω–∫–æ–≤
        synth = synthesize_answer(relevant_chunks, user_message)
        answer_text = synth.get("text", "")

        # --- 4.5) Guard-fallback –¥–ª—è ¬´–±–æ–ª—å/—Å—Ç—Ä–∞—Ö¬ª
        cand_cnt = len(relevant_chunks)
        if cand_cnt == 0:
            return LOW_REL_JSON.copy(), {"meta": {"relevance_score": 0.0, "cand_cnt": 0}}

        best = relevant_chunks[0]
        best_score = getattr(best, "score", 0.0)
        
        # –ï—Å–ª–∏ –ø–æ—Å–ª–µ —Ñ–∏–ª—å—Ç—Ä–∞/–ø–µ—Ä–µ—Ä–∞–Ω–∂–∏—Ä–æ–≤–∫–∏ –ª—É—á—à–∏–π —Å–∫–æ—Ä —Å–ª–∞–±—ã–π ‚Äî –æ—Ç–¥–∞–π –≥–æ—Ç–æ–≤—ã–π ¬´—ç–º–ø–∞—Ç–∏—á–Ω—ã–π¬ª –æ—Ç–≤–µ—Ç
        if theme_hint in ("safety", "pain", "fear_pain") and best_score < 0.42:
            payload = {
                "response": {"text": "–ü–æ–Ω–∏–º–∞—é, —á—Ç–æ —Å—Ç—Ä–∞—à–∏—Ç –∏–º–µ–Ω–Ω–æ –±–æ–ª—å. –ü—Ä–æ—Ü–µ–¥—É—Ä–∞ –¥–µ–ª–∞–µ—Ç—Å—è –ø–æ–¥ –º–µ—Å—Ç–Ω–æ–π –∞–Ω–µ—Å—Ç–µ–∑–∏–µ–π ‚Äì –≤–æ –≤—Ä–µ–º—è –æ–ø–µ—Ä–∞—Ü–∏–∏ –≤—ã –Ω–∏—á–µ–≥–æ –Ω–µ –ø–æ—á—É–≤—Å—Ç–≤—É–µ—Ç–µ.\n\n–ü–æ—Å–ª–µ ‚Äì –æ–±—ã—á–Ω–æ –∫–∞–∫ –ø–æ—Å–ª–µ —É–¥–∞–ª–µ–Ω–∏—è –∑—É–±–∞: –¥–∞—ë–º –æ–±–µ–∑–±–æ–ª–∏–≤–∞—é—â–µ–µ –∏ —Å–æ–ø—Ä–æ–≤–æ–∂–¥–∞–µ–º.\n\n–ï—Å–ª–∏ —Ö–æ—Ç–∏—Ç–µ, –≤—Ä–∞—á –∫–æ—Ä–æ—Ç–∫–æ —Ä–∞—Å—Å–∫–∞–∂–µ—Ç, –∫–∞–∫ –≤—Å—ë –ø—Ä–æ—Ö–æ–¥–∏—Ç –∏–º–µ–Ω–Ω–æ –≤ –≤–∞—à–µ–º —Å–ª—É—á–∞–µ."},
                "cta": {"label": "–ó–∞–ø–∏—Å–∞—Ç—å—Å—è –Ω–∞ –∫–æ–Ω—Å—É–ª—å—Ç–∞—Ü–∏—é", "target": "whatsapp"},
                "followups": ["–°–∫–æ–ª—å–∫–æ –¥–ª–∏—Ç—Å—è –ø—Ä–∏—ë–º?", "–ö–∞–∫–∞—è –∞–Ω–µ—Å—Ç–µ–∑–∏—è –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è?"]
            }
            return payload, {**rag_meta, "guard_used": True, "relevance_score": best_score}
        
        score = None
        
        # –ü—ã—Ç–∞–µ–º—Å—è –∏–∑–≤–ª–µ—á—å score –∏–∑ —á–∞–Ω–∫–∞
        for key in ["score", "rank_score", "bm25_score", "similarity"]:
            if isinstance(best, dict) and key in best:
                score = best[key]
                break
            elif hasattr(best, key):
                score = getattr(best, key)
                break
        
        # –ï—Å–ª–∏ score –Ω–µ—Ç - —Å—Ç–∞–≤–∏–º —Ä–∞–∑—É–º–Ω—ã–π –¥–µ—Ñ–æ–ª—Ç
        if score is None:
            score = 0.7  # –µ—Å–ª–∏ –Ω–µ—Ç score - –¥–∞–µ–º –±–µ–∑–æ–ø–∞—Å–Ω—ã–π –¥–µ—Ñ–æ–ª—Ç
        
        # –ï—Å–ª–∏ —ç—Ç–æ —Ñ–æ—Ä—Å-—Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ –ø–æ –∞–ª–∏–∞—Å—É - –ø–æ–≤—ã—à–∞–µ–º score
        if isinstance(best, dict) and (best.get("forced_by_alias") or best.get("h2_alias_hit")):
            score = max(score, 0.9)  # –µ—Å–ª–∏ —ç—Ç–æ –±—ã–ª —Ñ–æ—Ä—Å-–∞–ª–∏–∞—Å (–µ—Å–ª–∏ —É –≤–∞—Å –µ—Å—Ç—å —Ç–∞–∫–æ–π —Ñ–ª–∞–≥)

        # --- 5) –ü–æ—Å—Ç–ø—Ä–æ—Ü–µ—Å—Å: —ç–º–ø–∞—Ç–∏—è/–±—Ä–∏–¥–∂ + CTA
        best_chunk = relevant_chunks[0] if relevant_chunks else None
        topic_meta = getattr(best_chunk.metadata, '__dict__', {}) or {} if best_chunk else {}
        intent = theme_hint  # –∏–ª–∏ —Ç–≤–æ–π –∏–Ω—Ç–µ–Ω—Ç-–¥–µ—Ç–µ–∫—Ç–æ—Ä
        payload = postprocess(
            answer_text=answer_text,
            user_text=user_message,
            intent=intent,
            topic_meta=topic_meta,
            session={},  # session –ø–æ–∫–∞ –ø—É—Å—Ç–æ–π
        )
        
        # –ü–æ–¥—Å—Ç—Ä–∞—Ö–æ–≤–∫–∞ –¥–ª—è payload
        if not isinstance(payload, dict):
            payload = {"text": str(payload) if payload is not None else best_text}
        elif not payload.get("text"):
            payload["text"] = best_text

        # –û–±–Ω–æ–≤–ª—è–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ —Å relevance_score
        def _mget(meta, key, default=None):
            """–ë–µ–∑–æ–ø–∞—Å–Ω—ã–π –¥–æ—Å—Ç—É–ø –∫ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–º: dict/obj"""
            if meta is None:
                return default
            if isinstance(meta, dict):
                return meta.get(key, default)
            return getattr(meta, key, default)
        
        def _score_of(c):
            """–ü—ã—Ç–∞–µ–º—Å—è –ø–æ–ª—É—á–∏—Ç—å —Å–∫–æ—Ä –∏–∑ —Ä–∞–∑–Ω—ã—Ö –ø–æ–ª–µ–π; –µ—Å–ª–∏ –Ω–µ—Ç - None"""
            for k in ("score", "rank_score", "bm25_score", "similarity"):
                v = c[k] if isinstance(c, dict) and k in c else getattr(c, k, None)
                if v is not None:
                    return v
            return None
        
        cand_cnt = len(relevant_chunks)
        best = relevant_chunks[0]
        meta = getattr(best, "metadata", None) or getattr(best, "meta", {}) or {}
        best_text = getattr(best, "section_text", None) or getattr(best, "text", "") or ""
        # –º–∏–Ω–∏–º–∞–ª—å–Ω–∞—è –æ—á–∏—Å—Ç–∫–∞ –ø—Ä—è–º–æ —Ç—É—Ç (—Ç–∞ –∂–µ, —á—Ç–æ –≤ clean_response_text, –Ω–æ –∫–æ—Ä–æ—Ç–∫–∞—è)
        import re
        best_text = re.sub(r'<!--.*?>', '', best_text, flags=re.DOTALL)
        best_text = re.sub(r'^\s*#{1,6}\s*', '', best_text, flags=re.MULTILINE)
        best_text = best_text.strip()
        rag_meta["best_text"] = best_text
        score = _score_of(best)
        if score is None:
            score = 0.7  # –¥–µ—Ñ–æ–ª—Ç, —á—Ç–æ–±—ã guard –Ω–µ —Ä–µ–∑–∞–ª –Ω–æ—Ä–º–∞–ª—å–Ω—ã–π –æ—Ç–≤–µ—Ç
        best_chunk_id = f"{_mget(meta, 'file_basename','')}" + (f"#{_mget(meta, 'h2_id','')}" if _mget(meta, 'h2_id') else "")
        
        # –í–ê–ñ–ù–û: —Å–ø–∏—Å–æ–∫ —Å–ª–æ–≤–∞—Ä–µ–π {"chunk": ..., "score": ... } ‚Äì –∏–º–µ–Ω–Ω–æ —Ç–∞–∫ –∂–¥—ë—Ç –∞–¥–∞–ø—Ç–µ—Ä/–∞—Ä—Ä.—Ä—É
        candidates_with_scores = [
            {"chunk": c, "score": _score_of(c)}
            for c in relevant_chunks[:5]
        ]
        
        # rag_meta –ø–ª–æ—Å–∫–∏–µ –ø–æ–ª—è (+ –≤–ª–æ–∂–µ–Ω–Ω—ã–π meta –¥–ª—è guard - –Ω–∞ –≤—Å—è–∫–∏–π —Å–ª—É—á–∞–π)
        rag_meta = {
            "relevance_score": float(score),
            "cand_cnt": cand_cnt,
            "theme_hint": theme_hint,
            "detected_topics": detected_topics,
            "best_chunk_id": best_chunk_id,
            "candidates_with_scores": candidates_with_scores,
            "best_text": best_text,  # –ø—Ä–∏–≥–æ–¥–∏—Ç—Å—è –∞–¥–∞–ø—Ç–µ—Ä—É –∫–∞–∫ fallback
            "meta": {
                "relevance_score": float(score),
                "cand_cnt": cand_cnt,
                "doc_type": _mget(meta, "doc_type"),
                "topic": _mget(meta, "topic"),
                "best_chunk_id": best_chunk_id,
            }
        }

        # –õ–æ–≥–∏—Ä—É–µ–º –æ—Ç–≤–µ—Ç –≤ RAG
        from core.logger import format_candidates_for_log
        from logging import getLogger
        log_m = getLogger("cesi.minimal_logs")
        try:
            log_m.info(json.dumps({
                "ev":"search_candidates",
                "count": len(relevant_chunks),
                "cands": format_candidates_for_log(relevant_chunks, top=3)
            }, ensure_ascii=False))
        except Exception as e:
            logging.getLogger("cesi").warning(f"log_format_error: {e}")
        
        return payload, rag_meta
        
    except Exception:
        # –ù–∏–∫–æ–≥–¥–∞ –Ω–µ –ø–∞–¥–∞–µ–º –Ω–∞—Ä—É–∂—É ‚Äî –ª–æ–≥ –∏ –±–µ–∑–æ–ø–∞—Å–Ω—ã–π –æ—Ç–≤–µ—Ç
        logger.exception("üí• get_rag_answer failed")
        return LOW_REL_JSON.copy(), rag_meta

def log_query_response(user_message: str, response: str, metadata: dict, chunks_used: List[str] = None):
    """–õ–æ–≥–∏—Ä—É–µ—Ç –≤–æ–ø—Ä–æ—Å/–æ—Ç–≤–µ—Ç –≤ —Ñ–∞–π–ª –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞"""
    import json
    from datetime import datetime
    
    log_entry = {
        "timestamp": datetime.now().isoformat(),
        "question": user_message,
        "answer": response,
        "metadata": metadata,
        "chunks_used": chunks_used or [],
        "has_cta": bool(metadata.get("cta_action")),
        "topic": metadata.get("topic", "unknown"),
        "doc_type": metadata.get("doc_type", "unknown"),
        # –ü–æ–ª—è —ç–º–ø–∞—Ç–∏–∏ –¥–ª—è –æ—Ç–ª–∞–¥–∫–∏ (—Å—Ç–∞—Ä–∞—è –ª–æ–≥–∏–∫–∞ - –∑–∞–º–µ–Ω–µ–Ω–∞ –Ω–∞ –Ω–æ–≤—É—é)
        # "emotion": metadata.get("emotion", "none"),
        # "emotion_source": metadata.get("detector", "unknown"),
        # "emotion_confidence": metadata.get("confidence", 0.0),
        # "opener_used": metadata.get("opener_used", False),
        # "closer_used": metadata.get("closer_used", False)
    }
    
    try:
        # –°–æ–∑–¥–∞–µ–º –ø–∞–ø–∫—É logs –µ—Å–ª–∏ –µ—ë –Ω–µ—Ç
        logs_dir = Path("logs")
        logs_dir.mkdir(exist_ok=True)
        
        # –ó–∞–ø–∏—Å—ã–≤–∞–µ–º –≤ JSONL —Ñ–∞–π–ª
        log_file = logs_dir / "queries.jsonl"
        with open(log_file, "a", encoding="utf-8") as f:
            f.write(json.dumps(log_entry, ensure_ascii=False) + "\n")
        
        print(f"üìù –õ–æ–≥ –∑–∞–ø–∏—Å–∞–Ω: {log_file}")
        
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è: {e}")


